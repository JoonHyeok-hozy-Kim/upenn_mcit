[Back to Main](../../main.md)

# 1. History
### Representation Learning
- Throw away unnecessary info
  - cf.) compression

### Perceptrons


### SVM

<br><br>

# 2. Models
- Problem Setting)
  - Nature provides data $`(X,Y)`$
  - We want to learn from them (i.e. Generalization!)
    - identify patterns between $`X`$ and $`Y`$
    - make prediction on $`y'`$ given $`x'`$
  - Assume there is a prob. dist. $`P`$ s.t.
    - $`D_{\text{train}} =\{(x^i,y^i)\sim P\}_{i=1}^n`$

### Linear Regression
- Model)
  - $`f(x; w,b) = w^\top x + b`$
- Objective Function)
  - $`\ell(w,b) = \displaystyle\frac{1}{2n}\sum_i(y-\hat{y})^2`$ : Mean Squared Error
- Optimization : Minimize the MSE
  - $`w^* ,b^* = \arg\min \ell(w,b)`$
    - closed form sol.) 
      - $`w^* = (\tilde{\mathbf{X}}^\top \tilde{\mathbf{X}})^{-1} \tilde{\mathbf{X}}^\top\mathbf{Y}`$
        - where $`\tilde{\mathbf{X}} = \begin{bmatrix} \mathbf{X} & \mathbf{1}  \end{bmatrix}`$ considering the bias term.
    - No sol. Case)
      - $`(\tilde{\mathbf{X}}^\top \tilde{\mathbf{X}})^{-1}`$ is not invertible
        - Not enough data : $`n\lt d+1`$
          - Why?)
            - Consider that $`\tilde{\mathbf{X}}^\top \tilde{\mathbf{X}} \in \mathbb{R}^{(d+1)\times (d+1)}`$.
            - However, if $`n\lt d+1`$, then $`\text{rank}(\tilde{\mathbf{X}}^\top \tilde{\mathbf{X}}) \le n \lt d+1`$
            - Thus, there are at least $`(d+1-n)`$ eigenvalues that are equal to zero.
            - Hence, $`(\tilde{\mathbf{X}}^\top \tilde{\mathbf{X}})^{-1}`$ is not PD and not invertible.
              - Still, satisfies PSD!
        - Feature multi-colinearity
    - Difficult case) 
      - If one eigenvalue is huge while others are very small $`\displaystyle\frac{\lambda_{\max}}{\lambda_{\min}}\gg 1`$
        - Alternative) GD

### MLE
- Why doing this?)
  - We want to know the true probability distribution $`p^*(x)`$.
  - However, we don't know what $`p^*(x)`$ is, so we set up a likelihood model $`p_\theta(x)`$.
    - i.e.) $`p_\theta(x)=p(x\mid\theta)`$
  - We may approximate the model by solving
    - $`\displaystyle \theta^* = \arg\min_{\theta} D_{KL}(p^*(x)\Vert p_\theta(x)) = \mathbb{E}\left[ \log\frac{p^*(x)}{p_\theta(x)} \right]`$
  - Since $`p^*(x)`$ is independent of $`\theta`$, the problem goes
    - $`\displaystyle\arg\min_{\theta} \mathbb{E}\left[ -\log p_\theta(x) \right] = \arg\max_{\theta} \mathbb{E}\left[ \log p_\theta(x) \right]`$
      - i.e.) Maximizing the log-likelihood
- Model
  - Consider the supervise model for the paired data $`(x,y)`$.
  - Then, we may set the likelihood model as
    - $`y = w^\top x + b + \epsilon`$
      - where $`\epsilon\sim\mathcal{N}(0,\sigma_\epsilon^2)`$
  - Assuming that our model's probability distribution is Gaussian and satisfies i.i.d., we have
    - $`p(y\mid x;w,b)\stackrel{\text{i.i.d.}}{\sim}\mathcal{N}(w^\top x + b,\sigma_\epsilon^2) \quad `$
      - cf.) $`\displaystyle p(D_{\text{dataset}};w,b)=\prod_{i=1}^{n} p(y^i\mid x^i;w,b)`$
- Optimization : Maximize the log-likelihood
  - $`\displaystyle\arg\max_{\theta} \mathbb{E}\left[ \log p(y\mid x;w,b) \right]`$
- Prop.)
  - Identical to MSE problem of [Linear Regression](#linear-regression)
    - pf.)
      - Recall that the Gaussian pdf goes $`p(x) = \displaystyle\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\displaystyle\frac{(x-\mu)^2}{2\sigma^2}}`$
      - Thus, the problem goes   
        $`\begin{aligned}
          \arg\max_{\theta} \mathbb{E}\left[ \log p(y\mid x;w,b) \right]
          &= \arg\max_{\theta} \frac{1}{n}\sum_{i=1}^{n} \log p(y^i\mid x^i;w,b)\\
          &= \arg\max_{\theta} \frac{1}{n}\sum_{i=1}^{n} \log \left( \frac{1}{\sqrt{2\pi\sigma_\epsilon^2}}e^{-\displaystyle\frac{(y^i-w^\top x^i - b)^2}{2\sigma_\epsilon^2}} \right)  \\
          &= \arg\max_{\theta} \frac{1}{n}\sum_{i=1}^{n} \left( \log \frac{1}{\sqrt{2\pi\sigma_\epsilon^2}} -\displaystyle\frac{(y^i-w^\top x^i - b)^2}{2\sigma_\epsilon^2}\right)  \\
          &= \arg\max_{\theta} - \frac{\log 2\pi\sigma_\epsilon^2}{2} - \frac{1}{2n\sigma_\epsilon^2}\sum_{i=1}^{n} (y^i-w^\top x^i - b)^2  \\
          &= \arg\min_{\theta} \frac{1}{\sigma_\epsilon^2} \cdot \underbrace{\frac{1}{2n}\sum_{i=1}^{n} (y^i-w^\top x^i - b)^2}_{\text{MSE}}   \\
        \end{aligned}`$


[Back to Main](../../main.md)