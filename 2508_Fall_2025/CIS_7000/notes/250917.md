[Back to Main](../main.md)

# Normalizing Flow (NF)	
- Preliminaries)
  - Variational Inference with Normalizing Flows (Rezende & Mohamed)
  - NICE (Dinh et al., ICLR 2015)
- Density Estimation Using Real NVP (Dinh et al., 2017),
- Glow: Generative Flow with Invertible 1×1 Convolutions (Kingma & Dhariwal, 2018),
- Flow++: Improving Flow-Based Generative Models with Variational Dequantization (Ho et al., 2019),
- STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis (Gu et al., 2025)

<br>

## Normalizing Flow
### Concept) Normalizing Flow
- Key Idea)
  - A Normalizing Flow is a sequence of **invertible**, **differentiable** transformations applied to a simple base distribution (e.g., Gaussian) to model a complex target distribution.
    - Forward Flow (Generation)
      - $`\mathbf{x} = f(\mathbf{z})`$
    - Inverse Flow (Inference, Likelihood Evaluation)
      - $`\mathbf{z} = f^{-1}(\mathbf{x})`$
  - $`\mathbf{x} = f_K \circ f_{K-1} \circ\cdots\circ f_1(\mathbf{z}_0)`$
    - where
      - $`\mathbf{z}_0\sim p_0(\mathbf{z})`$
      - $`f_i`$ are bijective mappings

#### Concept) Change of Variables Formula
- For an invertible transformation $`f: \mathbb{R}^d \to \mathbb{R}^d`$, we may get   
  $`\begin{aligned}
    p_X(\mathbf{x}) 
    &= p_Z\left( f^{-1}(\mathbf{x}) \right) \cdot \left\vert \det\displaystyle\frac{\partial f^{-1}}{\partial\mathbf{x}} \right\vert \\
    &= p_Z(\mathbf{z}) \cdot \left\vert \det\displaystyle\frac{\partial f}{\partial\mathbf{z}} \right\vert^{-1}
  \end{aligned}`$
    - Why?
      - Consider that
        - $`J_{f^{-1}}(\mathbf{x}) = \displaystyle\frac{\partial f^{-1}(\mathbf{x})}{\partial\mathbf{x}} = \left(\frac{\partial f(\mathbf{z})}{\partial \mathbf{z}}\right)^{-1}`$ where $`\mathbf{z} = f^{-1}(\mathbf{x})`$
      - Thus,
        - $`\det J_{f^{-1}}(\mathbf{x}) = \displaystyle\frac{1}{\det J_{f}(\mathbf{z})}`$
  - Also, taking the log, we have   
    $`\begin{aligned}
        \log p_X(\mathbf{x}) 
        &= \log p_Z(f^{-1}(\mathbf{x})) + \log\left\vert \det\displaystyle\frac{\partial f^{-1}}{\partial\mathbf{x}} \right\vert \\
        &= \log p_Z(\mathbf{z}) - \log\left\vert \det\displaystyle\frac{\partial f}{\partial\mathbf{z}} \right\vert \\
    \end{aligned}`$

<br>

#### Props.) Normalizing Flow
- Advantages)
  - Exact (Tractable) likelihood evaluation possible.
  - Sampling is trivial
    - Why?)
      - Just push a base sample $`\mathbf{z}_0 \sim p_0`$ through the forward transformations.
  - Training objective
    - maximize log-likelihood of data under the flow.
- Drawbacks)
  - Designing transformations with efficient Jacobians is challenging.
  - Tradeoff between expressiveness and computational cost.
  - Training may suffer from optimization difficulties in high dimensions.


<br>


## Paper 1) Density Estimation Using Real NVP (Dinh et al., 2017)
### Problem Setting)
- NICE had limited expressiveness.

### Concept) Affine Coupling Layers
- Desc.)
  - Split variables into two parts
  - Transform one part conditioned on the other
- Def.)
  - Let
    - $`x\in\mathbb{R}^D`$ : the $`D`$ dimensional input
  - For $`d\lt D`$
    - The output $`y`$ of an affine coupling layer follows
      - $`y_{1:d} = x_{1:d}`$
      - $`y_{d+1:D} = x_{d+1:D} \odot\exp(s(x_{1:d})) + t(x_{1:d})`$
        - where
          - $`s:\mathbb{R}^d\rightarrow\mathbb{R}^{D-d}`$ : the scale function
          - $`t:\mathbb{R}^d\rightarrow\mathbb{R}^{D-d}`$ : the translation function

<br>


## Paper 2) Glow: Generative Flow with Invertible 1×1 Convolutions (Kingma & Dhariwal, 2018)
#### Motivation
- Real NVP worked, but limited in expressivity and architecture flexibility.
- Goal: make flows more scalable and expressive for high-res image synthesis.

#### Contributions
- Invertible 1×1 Convolutions
- Generalizes channel permutation → learnable invertible linear transform across channels.
- Determinant easy to compute via LU decomposition.
- Simplified architecture
- Replace complicated coupling functions with deep conv nets.
- Demonstrated large-scale flow training
- High-quality images on CelebA-HQ, LSUN, ImageNet.

#### Position
- Glow = the ImageNet-scale normalizing flow.
- First to show flows can compete with GANs in visual fidelity.


<br>


## Paper 3) Flow++: Improving Flow-Based Generative Models with Variational Dequantization (Ho et al., 2019)
#### Motivation
- Glow strong, but lagged behind autoregressive models in log-likelihood.
- Dequantization (discrete → continuous) was too naive (uniform noise).

#### Contributions
- Variational Dequantization
- Instead of adding uniform noise, learn a flow model for dequantization distribution.
- Improves likelihood estimation significantly.
- Mixture of Logistic CDF Coupling Transforms
- More expressive than affine coupling (better local flexibility).
- Deep networks for $`s,t`$
- Attention layers, residual networks.

#### Results
- State-of-the-art likelihoods on CIFAR-10 (beating Glow).
- Flows can close gap with autoregressive models if carefully improved.

#### Position
- Flow++ = proof that flows can be competitive with likelihood-based models, not just GANs.
- Key technique: variational dequantization is now standard.


<br>


## Paper 4) STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis (Gu et al., 2025)
#### Motivation
- Even Glow/Flow++ scale poorly on very high-res (e.g., >1K×1K).
- Need better scaling strategies for latent flows.

#### Contributions
- STAR = Spatially-aware Transformations for flows
- Exploit hierarchical latent spaces (similar to VAE-like pyramids).
- Each latent level has flow blocks that model local+global dependencies.
- Efficient invertible architectures
- Optimize memory & compute for high-res training.
- Scalable synthesis
- Show flows can generate megapixel images with competitive fidelity.

#### Results
- High-res synthesis (1024²+) previously dominated by diffusion/GANs, now feasible with flows.
- Improved FID/IS over prior NF baselines at large scale.

#### Position
- STARFlow = modern push to make flows competitive in the “megapixel regime”.
- Positions flows as a viable alternative to diffusion for controllable, invertible generation.









<br>

[Back to Main](../main.md)