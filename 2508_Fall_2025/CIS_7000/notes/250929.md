[Back to Main](../main.md)

# Energy Based Models (EBM)
2025-09-29

### Papers
- Your Classifier Is Secretly an Energy Based Model (Grathwohl et al., 2020),
- Implicit Generation and Generalization in Energy-Based Models (Du & Mordatch, 2020),
- Energy-Based Transformers are Scalable Learners and Thinkers (Gladstone et al., 2025)
- Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and - Probabilistic Interpretations (Xu et al., 2024)

<br>

## Concept) Energy Based Model
### Core Idea: The Energy Landscape 

Energy-Based Models (EBMs) are a class of generative models that model a probability distribution indirectly through a scalar **energy** function. The core intuition is to imagine an "energy landscape" for all possible data.

-   **Low Energy (Valleys)** = **High Probability (Likely Data)**
    -   Stable regions where real, plausible data points reside.
        - e.g., a clear photo of a dog
-   **High Energy (Mountains)** = **Low Probability (Unlikely Data)**
    -   Unstable regions where nonsensical data reside.
        -   e.g., a photo of pure static noise

The primary goal of an EBM is to learn an energy function that accurately represents this landscape.

---

### How Do EBMs Work?

EBMs are defined by two key components:

1.  **The Energy Function ($E_{\theta}(x)$)**
    -   A neural network that takes an input $x$ and outputs a single scalar value representing its energy. 
    -   The architecture of this network can be arbitrary (e.g., CNN, Transformer), which provides great flexibility.

2.  **The Probability Distribution ($p_{\theta}(x)$)**
    -   The energy value is converted into a probability using the Boltzmann distribution:

    $$p_{\theta}(x) = \frac{e^{-E_{\theta}(x)}}{Z(\theta)}$$

    -   $e^{-E_{\theta}(x)}$: The negative exponent ensures that a *low* energy value results in a *high* score (and thus, higher probability).
    -   $Z(\theta)$: The **Partition Function**. This is a normalizing constant, calculated by integrating or summing $e^{-E_{\theta}(x)}$ over all possible data points $x$ in the entire data space. Its purpose is to ensure that the total probability adds up to 1.
        - **The Biggest Technical Hurdle**: The partition function $Z(\theta)$ is notoriously difficult, and usually impossible, to compute directly. Much of the research in EBMs focuses on clever ways to train and sample from the model *without* computing $Z(\theta)$.

---

### Training and Inference

-   **Training (Learning the Landscape)**
    -   The model is trained using a principle called **contrastive divergence**.
        - Def.)
          $$`\mathcal{L}(\theta) = \mathbb{E}_{x_{\text{real}}}\left[\mathbb{E}_\theta[x_{\text{real}}]\right] + \mathbb{E}_{x_{\text{fake}}}\left[ \max(0, -\mathbb{E}_\theta(x_{\text{fake}})) \right]`$$
          - where
            - $`x_{\text{real}}`$ : the real data (Positive Sample)
            - $`x_{\text{fake}}`$ : the fake data generated with MCMC (Negtive Sample)
            - $`m`$ : the margin
        - Derivation)
          - Start with the MLE where $`p_{\theta}(x) = \frac{e^{-E_{\theta}(x)}}{Z(\theta)}`$
          - Then, we may get the Loss function of   
            $$`\begin{array}{lll}
                \mathcal{L}(\theta) &= -\mathbb{E}_{x\sim p_{\text{data}}}\left[ \log p_\theta(x) \right] \\
                &= -\mathbb{E}_{x\sim p_{\text{data}}}\left[ -E_{\theta}(x) - \log Z(\theta) \right] & \because p_{\theta}(x) = \frac{e^{-E_{\theta}(x)}}{Z(\theta)} \\
                &= \mathbb{E}_{x\sim p_{\text{data}}}\left[ E_{\theta}(x) \right] + \log Z(\theta) & \because \log Z(\theta) \text{ is normalized.}
            \end{array}`$$
    -   **For real data ($x_{real}$)**: The model's parameters are adjusted to *decrease* its energy output, $E(x_{real})$. This is like digging the valleys deeper.
    -   **For "fake" data ($x_{fake}$)**: The model is fed some form of negative or corrupted samples, and its parameters are adjusted to *increase* their energy output, $E(x_{fake})$. This is like raising the mountains higher.

-   **Inference / Sampling (Generating New Data)**
    -   To generate a new sample, you can imagine dropping a ball anywhere on the energy landscape and letting it roll downhill to find a low-energy valley.
    -   This process is typically done algorithmically. It starts with random noise and iteratively modifies the data in the direction that most rapidly decreases the energy (i.e., gradient descent on the energy function). This procedure is often performed using methods like **Langevin Dynamics**.

---

### Pros and Cons of EBMs

| Pros | Cons |
|-|-|
| **Flexibility**: The energy function can be any arbitrary neural network.       | **Intractable Partition Function**: Makes direct likelihood calculation impossible. |
| **Stable Training**: Avoids the unstable training dynamics of models like GANs. | **Slow Sampling**: Generating data requires a slow, iterative optimization process. |
| **Compositionality**: Different EBMs can be easily combined to model complex data. |                                                                               |

---

### Connection to the Research Papers

-   **`Your Classifier Is Secretly an Energy Based Model`**: This seminal paper shows that the logits of a standard softmax classifier can be interpreted as the negative energy of an EBM. This links classification and energy-based modeling.
-   **`Implicit Generation and Generalization in EBMs`**: Focuses on the ability of EBMs to act as powerful generative models by learning the underlying data distribution.
-   **`Energy-Based Transformers are Scalable Learners and Thinkers`**: Demonstrates the scalability of the EBM framework by using a powerful Transformer architecture as the energy function for complex reasoning tasks.
-   **`Energy-Based Concept Bottleneck Models`**: Uses the EBM framework to improve model interpretability. By assigning energies to human-understandable concepts, it allows for intervention and probabilistic explanations of the model's predictions.

---



<br>


## Paper 1



<br>


## Paper 2



<br>


## Paper 3



<br>


## Paper 4










<br>

[Back to Main](../main.md)