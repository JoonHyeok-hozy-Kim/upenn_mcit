[Back to Main](../main.md)

# T2I T2V
2025-10-20

## Talk) Towards Understanding and Building Intuition for LM
- Yizhe Zhang

### Concept) Text Diffusion
- Parallel decoding
  - fast sampling
  - global planning
- Types)
  - Discrete Diffusion
  - Cont Diffusion
  - Latent Diffusion

### Discrete Diffusion
- Can it take advantage of parallel pre-training?
  - Score matching between AR -> bidirectional
- Use RL to improve
  - Coupled-GRPO
    - Partial Mask + Complementary Mask

### Cont Diffusion

### CADD
- Discrete + Cont
  - Sharing the same backbone Transformer

### Latent Diffusion
- Diffusion handles latent continuous semantics
  - global planning
- AR does local 
- Compared to AR
  - not good at summarization 

<br>

### Papers
- Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (Esser et al., 2024),
- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (Saharia et al., 2022),
- Wan: Open and Advanced Large-Scale Video Generative Models (Wan et al., 2025)

<br>



<br>


## Paper 1) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
[@esserScalingRectifiedFlow2024]
- [SD3](../project/references/basics/sd3.md)



<br>


## Paper 2) Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding
[@sahariaPhotorealisticTexttoImageDiffusion2022]
### Model) Imagen
- Key Takeaways)
  - Followings two are effective for T2I synthesis
    - Text embeddings from large LMs
    - Pretrained on text-only corpora
- Architecture)
  - Forward Process)
    - Frozen T5-XXL encoder to map input text into a sequence of embeddings
      - T5 and CLIP both works.
      - But, human evaluators prefer T5-XXL encoders over CLIP text encoders
    - $`64\times64`$ image diffusion model
    - Super-resolution diffusion model
      - Scales)
        - $`256^2`$
        - $`1024^2`$
      - Props.)
        - Conditioned on the text embedding sequence
        - Use [CFG](../project/references/basics/classifier_free_guidance.md)
  - Sampling)
    - 

<br>


## Paper 3) Wan: Open and Advanced Large-Scale Video Generative Models
[@wanWanOpenAdvanced2025]



<br>


## Paper 4

<br>

$\displaystyle q(\mathbf{x}_{t-1}\mid\mathbf{x}_{t}) =\frac{q(\mathbf{x}_{t}\mid\mathbf{x}_{t-1})q(\mathbf{x}_{t-1})}{q(\mathbf{x}_{t})} \;\Leftrightarrow\; q(\mathbf{x}_{t}\mid\mathbf{x}_{t-1}) = \frac{q(\mathbf{x}_{t-1}\mid\mathbf{x}_{t}) \cdot q(\mathbf{x}_{t})}{q(\mathbf{x}_{t-1})}$

<br>
<br>

$\displaystyle\arg\min_\theta \text{Distance} \left( \underbrace{q(\mathbf{x}_{t}\mid\mathbf{x}_{t-1})}_{\text{What we set}} \;,\; \underbrace{p_\theta(\mathbf{x}_{t}\mid\mathbf{x}_{t-1})}_{\text{What NN learns!}} \right)$

<br>
<br>
<br>
<br>

<br>

[Back to Main](../main.md)