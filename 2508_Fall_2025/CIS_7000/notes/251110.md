[Back to Main](../main.md)

# LLM2
2025-11-10

### Papers
- Training language models to follow instructions with human feedback (Ouyang et al., 2022)
- Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al., 2023),
- DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Shao et al., 2024),
- Demystifying Long Chain-of-Thought Reasoning in LLMs (Yeo et al., 2025)



<br>


## Paper 1) Training language models to follow instructions with human feedback (Ouyang et al., 2022)
- Goal)
  - Aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback
- How?)
  - a set of labeler-written prompts
  - collect a dataset of labeler demonstrations of the desired model behavior
  - collect a dataset of rankings of model outputs
  - fine-tune this supervised model using reinforcement learning from human feedback
- Result)
  - Instruct GPT : Fine-tuned 1.3B parameter model 
    - More preferred to 175B parameter GPT-3.
    - Improvement in truthfulness and reduction in toxic output generation.
    - having minimal performance regressions on public NLP datasets



<br>


## Paper 2) Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al., 2023)
- Goal)
  - introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss
    - where 
      - RLHF : reinforcement learning from human feedback (just like the [InstructGPT above](#paper-1-training-language-models-to-follow-instructions-with-human-feedback-ouyang-et-al-2022))
- Result)
  - Direct Preference Optimization (DPO)
    - is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning
    - can fine-tune LMs to align with human preferences as well as or better than existing methods


<br>


## Paper 3) DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Shao et al., 2024)
- Goal)
  - a domain-specific language model that significantly outperforms the mathematical capabilities
- How?)
  - the publicly accessible Common Crawl data contains valuable information for mathematical purposes


<br>


## Paper 4) Demystifying Long Chain-of-Thought Reasoning in LLMs (Yeo et al., 2025)










<br>

[Back to Main](../main.md)