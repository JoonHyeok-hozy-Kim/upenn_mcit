[Back to Main](../main.md)

# LLM1
2025-11-

### Papers
- Training Compute-Optimal Large Language Models (Hoffmann et al., 2022),
- LLaMA: Open and Efficient Foundation Language Models (Touvron et al., 2023),
- Mistral 7B (Jiang et al., 2023),
- DeepSeek-V3 Technical Report (DeepSeek- AI et al., 2024)

<br>

## Auto Regressive Model



<br>


## Paper 1) Training Compute-Optimal Large Language Models
- More data


<br>


## Paper 2) LLaMA: Open and Efficient Foundation Language Models
- trained on public data
- less parameter thanks to
  - SWiGLU
  - RoPE


<br>


## Paper 3) Mistral 7B
- Even lighter model than Llama with
  - grouped query attention
  - sliding window attention


<br>


## Paper 4) DeepSeek-V3 Technical Report










<br>

[Back to Main](../main.md)