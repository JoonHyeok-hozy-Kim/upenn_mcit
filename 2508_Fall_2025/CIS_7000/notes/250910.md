[Back to Main](../main.md)

# Auto Regressive Models 1
2025-09-10


- [Auto Regressive Models 1](#auto-regressive-models-1)
  - [Auto Regressive Model](#auto-regressive-model)
      - [1. Core Idea](#1-core-idea)
      - [2. Origins](#2-origins)
      - [3. Why AR Matters](#3-why-ar-matters)
      - [4. Typical Implementations](#4-typical-implementations)
      - [5. Limitations](#5-limitations)
  - [Paper 1 Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation](#paper-1-autoregressive-model-beats-diffusion-llama-for-scalable-image-generation)
      - [Concept 1-1) Image Tokenizer](#concept-1-1-image-tokenizer)
      - [Concept 1-2) Learned Codebook](#concept-1-2-learned-codebook)
  - [Paper 2 Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](#paper-2-visual-autoregressive-modeling-scalable-image-generation-via-next-scale-prediction)
  - [Paper 3 Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Gu \& Dao, 2023)](#paper-3-mamba-linear-time-sequence-modeling-with-selective-state-spaces-gu--dao-2023)
  - [Paper 4 Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Sun et al., 2024)](#paper-4-learning-to-learn-at-test-time-rnns-with-expressive-hidden-states-sun-et-al-2024)
      - [Concept 4-1) Self-Attention](#concept-4-1-self-attention)

<br>

## Auto Regressive Model
#### 1. Core Idea
- Def.)
  - $`\displaystyle p(x) = \prod_{t=1}^T p(x_t\mid x_{\lt t})`$
- Desc.)
  - Each new element (word, pixel, token, etc.) is predicted **given all previous ones**.

#### 2. Origins
- Classical Statistics: $`AR(p)`$ models in time series (Yule, 1927)
  - $`\displaystyle x_t = \sum_{i=1}^p \phi_i x_{t-i} + \epsilon_t`$
- Deep Learning
  - Bengio et al. (2003): Neural probabilistic language model.
  - Larochelle & Murray (2011): NADE (Neural Autoregressive Density Estimator).
  - Germain et al. (2015): MADE (Masked Autoencoder for Distribution Estimation).

#### 3. Why AR Matters
- Provides a tractable likelihood
  - Thus, easy to train via maximum likelihood
- Naturally sequential
  - Aligns with 
    - language
    - audio
    - image raster scan 
      - Desc.)
        - Generate an image pixel by pixel, left-to-right, top-to-bottom
        - An image can be described as
          - $`\displaystyle p(\mathbf{x}) = \prod_{i=1}^{H\times W} p(x_i\mid x_{\lt i})`$
        - Thus, the 2D grid is illustrated as the 1D sequence with tokens.
          - cf.) CNN deals with feature map without sequence in 2D of $`H\times W\times C`$

#### 4. Typical Implementations
- RNN-based AR models
  - Predict next token step by step
- CNN-based AR models
  - Masked convolutions to enforce causality
  - e.g.) PixelCNN
- Transformer AR models
  - Masked self-attention
  - e.g.) GPT, Llama

#### 5. Limitations
- Slow sampling
  - Why?) Must generate one step at a time
- Long-range dependencies
  - Hard to capture without large models
    - cf.) Solved partial by attention/state space models
- Exposure bias
  - Training vs generation mismatch

<br>


## Paper 1 Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation
Sut et al. 2024

- Summary
  - One Sentence
    - Vanilla AR models like Llama can achieve state-of-art image generation performance without inductive biases on visual signals
  - Contributions
    - Image Tokenizer

#### Concept 1-1) Image Tokenizer
- Architecture
  - Same as VQGAN
- Purpose
  - Convert a continuous image $`(H\times W\times C)`$ into a sequence of discrete tokens.
    - cf.) Text tokenizers split sentences into discrete tokens (words, subwords)
  - This makes images compatible with AR models that work on token sequences
- How)
  - Vector Quantization (VQ)
    - Train an encoder-decoder (e.g. VQ-VAE, VQGAN)
    - The encoder maps image patches into latent vectors.
    - Each latent vector is quantized to the nearest entry in a [learned codebook](#concept-1-2-learned-codebook).
    - The decoder reconstructs the image from these discrete codes
  - Result)
    - An image is represented as a sequence of codebook indices (tokens).
- Advantage)
  - Enables scalability like other AR models
    - Used the massive LAION-COCO dataset to scale up!

#### Concept 1-2) Learned Codebook
- Def.)
  - A vector dictionary that is learned in VQ-VAE or VQGAN
- How to learn)
  - Codebook entries are randomly initialized on the embedding space.
    - where $`\dim(\text{Latent Space})=\dim(\text{Embedding Space})`$
  - Encoder maps the latent vector $`z_e(x)`$
  - Map $`z_e(x)`$ into the nearest neighboring coding entry $`e_k`$ in the embedding space
  - During training the codebook is being updated by the backprop, and gradually evolves to be the best vector set that covers the latent space


<br>


## Paper 2 Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction
- Summary)
  - Redefine AR learning on images as coarse-to-fine "next-scale(resolution) prediction"
    - cf.) Standard: "next-token prediction"
- Motivation)
  - Existing AR image models generate images pixel by pixel or patch by patch
  - They have drawbacks of
    - computationally slow
    - struggles with global coherence at high resolution.
      - i.e.) AR models are good at generating sharp local textures, but lose global structure!
- Key Concepts) 
  - Next-Scale Prediction
    - Instead of predicting the next pixel, which is too fine-grained,
    - VAR predicts the next scale of the image conditioned on the previous lower scale.
      - i.e.) Coarse $`\rightarrow`$ fine image generation.
  - Hierarchical Autoregression
    - Factorization of the joint distribution is redefined as
      - $`p(x) = \displaystyle\prod_{s=1}^S p(x_s \mid x_{\lt s})`$
        - where $`x_s`$ is the image at scale $`s`$
    - Here, each image scale provides a context for generating the next finer scale.
- Advantage)
  - High FID scores, competitive with or better than diffusion models.
  - Scaling Laws
  - Zero-shot generalization
    - i.e.) Since the model captures multi-scale dependencies ARly, it can generalize to different image resolutions, unseen combinations of structures, or new prompts without retraining.


<br>


## Paper 3 Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Gu & Dao, 2023)
Gu et al. 2023

- Background)
  - Transformers dominate sequence modeling but [self-attention](#concept-4-1-self-attention) is expensive. $`O(T^2)`$
  - State Space Models (SSMs) are an alternative
    - Inspired by control theory
    - Capture long-range dependencies with linear recurrence equations
    - Already explored in S4.
- What Mamba is)
  - Improved Selective SSM
  - Core Idea
    - Use state space recurrence for linear-time sequence proceessing
    - Add selectivity mechanism so the model can decide which part of the sequence to keep/update
    - This bridges the gap
      - More expressive than plain SSMs
      - Cheaper than transformers $`O(T)`$


<br>


## Paper 4 Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Sun et al., 2024)
Sut et al. 2024

- Summary)
  - Test Time Training (TTT)
    - keeps the RNN's linear computational cost
    - makes the hidden state much more expressive by updating it in a richer way
      - How?) hidden layers adapt at test time!

#### Concept 4-1) Self-Attention
- Desc.)
  - Mechanism to compute how much each token in a sequence should attend to every other token
  - Unlike RNNs, self-attention allows direct connections across all positions in the sequence
    - cf.) RNN is sequential
      - Thus, it struggles with long-term dependencies.
      - But, it has linear computational cost!
- How?)
  - Input 
    - $`X = \left[ x_1,x_2,\cdots,x_T \right]`$ : a series of token embeddings
  - For each token, compute three projections:
    - Query : $`Q=XW^Q`$
    - Key : $`K=XW^K`$
    - Value : $`V = XW^V`$
  - Let attention score be
    - $`\text{score}(i,j) = \displaystyle\frac{Q_i\cdot K_j}{\sqrt{d}}`$
  - Softmax normalization
    - Goal)
      - Derive probability distribution over all tokens
  - Output
    - $`z_i = \displaystyle\sum_j \text{softmax}(\text{score}(i,j))\cdot V_j`$
- Advantage)
  - Captures all pairwise interactions in parallel
  - Handles long-range dependencies much better than RNNs.
  - Basis for all modern LLMs.
- Drawback)
  - $`O(T^2)`$ computation where $`T`$ is the length of the sequence.
    - cf.) RNN has linear computational cost of $`O(T\cdot d^2)`$ where $`d`$ is the hidden dimension.







<br>

[Back to Main](../main.md)