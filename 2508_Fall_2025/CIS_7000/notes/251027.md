[Back to Main](../main.md)

# Controllable
2025-10-27

### Papers
- Adding Conditional Control to Text-to-Image Diffusion Models (Zhang & Agrawala, 2023),
- Classifier-Free Diffusion Guidance (Ho & Salimans, 2022)
- Prompt-to-Prompt Image Editing with Cross-Attention Control (Hertz et al., 2022),
- T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion - Models (Mou et al., 2023)

<br>

## Auto Regressive Model



<br>


## Paper 1) Adding Conditional Control to Text-to-Image Diffusion Models
- ControlNet

- we propose ControlNet, a neural network architecture that can add spatially localized input conditions to a pretrained text-to-image diffusion model via efficient finetuning, 
- we present pretrained ControlNets to control Stable Diffusion, conditioned on Canny edges, Hough lines, user scribbles, human key points, segmentation maps, shape normals, depths, and cartoon line drawings


<br>


## Paper 2) Classifier-Free Diffusion Guidance



<br>


## Paper 3) Prompt-to-Prompt Image Editing with Cross-Attention Control
- Edit images by injecting the cross-attention maps during the diffusion process



<br>


## Paper 4) T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion - Models
- Pre-trained T2I diffusion model (e.g. Stable Diffusion) with extra guidance
- Various adapters according to different conditions
  - Do not affect the original network topology and generation abilities
  - Low training cost









<br>

[Back to Main](../main.md)