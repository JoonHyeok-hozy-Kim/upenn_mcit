[Back to Main](../main.md)

# 5. Decision Theory for Optimization

### Concept) Sequential Optimization
![](../images/05/001.png)

<br>

### Concept) Acquisition Function (Infill Function, Figure of Merit)
- Def.)
  - A function that provides a score to each potential observation location commensurate with its propensity for aiding the optimization task.
- Notation)
  - $`\alpha: \mathcal{X}\rightarrow\mathbb{R}`$
    - Assigning a score to each point in the domain
  - $`\alpha(x;\mathcal{D})`$
    - Explicitly showing its dependence on data
      - cf.) Various $`\alpha`$s are defined by deriving the posterior belief of the objective function given the data, i.e. the [model posterior](./04.md#cf-log-posterior) $`p(f\mid\mathcal{D})`$
- Prop.)
  - $`\alpha`$ encodes preferences over potential observation locations.
    - i.e.) $`\alpha(x;\mathcal{D}) \gt \alpha(x';\mathcal{D})`$
  - How to find the next observation point)
    - $`\displaystyle x\in \arg\max_{x'\in\mathcal{X}} \alpha(x';\mathcal{D})`$
      - Question) Why should we solve the above global opt. problem to solve the [global problem of estimating the objective function](#concept-sequential-optimization)?
        - A) $`\alpha`$ is more tractable than the original problem.

<br>

### Concept) Bayesian Decision Theory
- Def.)
  - A framework for decision making under uncertainty that is flexible enough to handle effectively any scenario
    - cf.)
      - Degroot (1970), Optimal Statistical Decisions
      - Berger (1985), Statistical Decision Theory and Bayesian Analysis



<br>

### Concept) Isolated Decision
- Settings)
  - $`\mathcal{A}`$ : the action space
    - Props.)
      - We will select an action $`a\in\mathcal{A}`$
      - For the [sequential optimization above](#concept-sequential-optimization), $`\mathcal{X} = \mathcal{A}`$
        - Why?) We are choosing $`\displaystyle x\in \arg\max_{x'\in\mathcal{X}} \alpha(x';\mathcal{D})`$
  - $`\psi`$ : a random variable encompassing any relevant uncertain elements when making and evaluating decisions
  - $`p(\psi\mid\mathcal{D})`$ : posterior belief about $`\psi`$
    - i.e.) Our belief about $`\psi`$ in light of data $`\mathcal{D}`$ 
  - $`u(a,\psi,\mathcal{D})`$ : utility function
    - where $`a\in\mathcal{A}`$ : an action
      - Prop.)
        - It measures the quality of selecting $`a`$.
        - Higher $`u`$ means more favorable outcome.
  - $`\mathbb{E}\left[ u(a,\psi,\mathcal{D}) \mid a,\mathcal{D} \right] = \displaystyle\int u(a,\psi,\mathcal{D}) p(\psi\mid\mathcal{D}) d\psi`$ : expected utility
    - cf.)
      - What we know : $`a,\mathcal{D}`$
      - What we don't know : $`\psi`$
- Expected Utility Optimization
  - $`a\in \displaystyle\arg\max_{a'\in\mathcal{A}} \mathbb{E}\left[ u(a',\psi,\mathcal{D}) \mid a',\mathcal{D} \right]`$

<br>

### Concept) Sequential Decisions with Fixed Budget
- Settings)
  - The action space $`\mathcal{A}`$ of each decision $`a`$ is the domain $`\mathcal{X}`$.
  - We must act under uncertainty $`(\psi)`$ about the objective function $`f`$.
    - How?)
      - Reason about the result of making an observation at some point $`x\in\mathcal{X}`$ via the posterior predictive distribution $`p(y\mid x,\mathcal{D})`$
  - Ultimate purpose of optimization is to collect and return the dataset $`\mathcal{D}`$.
    - How?)
      - We will maximize $`u(\mathcal{D})`$
      - Make observations that, in expectation, promise the biggest improvement in utility.
    - Prop.)
      - Each observation will update the dataset and impact on the entire remainder of optimization.
  - Fixed observation budget
    - Concept) Decision Horizon $`\tau`$
      - the number of remaining observations
  - Concept) Terminal Utility
    - Def.)
      - $`u(\mathcal{D}_\tau) = u\left( \underbrace{\mathcal{D}}_{\text{known}},\; \underbrace{x}_{\text{action}},\; \underbrace{y,\; x_2,\;y_2,\;\cdots,\;x_\tau,\;y_\tau}_{\text{unknown}}  \right)`$
  - Concept) Expected Terminal Utility
    - Def.)
      - $`\mathbb{E}\left[ u(\mathcal{D}_\tau) \mid x,\mathcal{D} \right]`$
        - cf.) Refer to the [isolated decision](#concept-isolated-decision).
    - Prop.)
      - The expectation over the future data may be written as   
        $`\displaystyle \int\cdots\int u(\mathcal{D}_\tau) p(y\mid x,\mathcal{D}) \prod_{i=2}^\tau p(x_i, y_i \mid \mathcal{D}_{i-1}) dy \; d\left\{(x_i,y_i)\right\}`$
        - which is intractable.
    - Analysis) [Bellman Optimality](#concept-bellman-optimality)
      - Inductively setup the optimization problem.
        1. Start with the [base case $`(\tau=1)`$](#1-one-observation-remaining-case).
        1. Get the relation between the base case and the [next step $`(\tau=2)`$](#2-two-observation-remaining-case)
        1. [Generalize](#3-inductive-case) the relation with arbitrary decision horizon $`\tau`$.
- Optimization Problem)
  - When there are $`\tau`$ evaluations are remaining, we may choose $`x`$ s.t.  
    $`x\in\displaystyle\arg\max_{x'\in\mathcal{X}} \mathbb{E} \left[ u(\mathcal{D}_\tau) \mid x',\mathcal{D} \right]`$
  - Concept) the expected increase in utility making an observation at $`x`$
    - $`\alpha_\tau (x;\mathcal{D}) = \mathbb{E}\left[ u(\mathcal{D}_\tau) \mid x,\mathcal{D} \right] - u(\mathcal{D})`$ 

#### 1. One observation remaining case
- Settings)
  - $`\tau = 1`$
    - i.e.)
      - This is the final decision.
- Marginal Gain)
  - $`\alpha_1(x;\mathcal{D}) = \displaystyle\int u(\mathcal{D}_1) \; p(y\mid x,\mathcal{D}) dy - u(\mathcal{D})`$
- Optimal Observation)
  - $`x\in\displaystyle\arg\max_{x'\in\mathcal{X}} \alpha_1 (x';\mathcal{D})`$
- Result)
  - $`u(\mathcal{D}) + \alpha_1^*(\mathcal{D})`$ : Expected Utility
    - where $`\alpha_1^*(\mathcal{D}) = \displaystyle\max_{x'\in\mathcal{X}} \alpha_1(x';\mathcal{D})`$ is... 
      - the expected increase in utility when starting with $`\mathcal{D}`$ and continuing optimally for $`\tau=1`$ additional observations.
      - the value of the dataset $`\mathcal{D}`$ with a horizon of $`\tau=1`$
- e.g.)
  - Settings)
    - Objective Function
      - The belief over the objective function $`p(f\mid\mathcal{D})`$ is a GP.
      - Our observations reveal exact values of the objective
    - Utility Function
      - $`u(\mathcal{D}) = f(\mathbf{x})`$ : the maximal objective value contained in the data
      - Then the marginal utility gain can be denoted as
        - $`u(\mathcal{D}_1) - u(\mathcal{D}) = \max\{ y - u(\mathcal{D}),\; 0 \}`$
  - Graphics)   
    ![](../images/05/002.png)

#### 2. Two observation remaining case
- Settings)
  - $`\tau = 2`$
    - i.e.) Making penultimate (second last) observation $`x`$.
- Marginal Gain)    
  $`\begin{aligned}
    \alpha_2(x;\mathcal{D}) &= \mathbb{E}\left[ u(\mathcal{D}_\tau) \mid x,\mathcal{D} \right] - u(\mathcal{D}) \\ 
    &= \int u(\mathcal{D}_2) \; \underbrace{p(y\mid x,\mathcal{D})}_{\text{current (penultimate)}} \; \underbrace{p(x_2, y_2 \mid \mathcal{D}_1)}_{\text{final}} dy \; d(x_2, y_2) - u(\mathcal{D})
  \end{aligned}`$
  - However, this is too complicated.
  - Instead, assuming that $`(x_2,y_2)`$ will be optimal, we may denote with [the $`\tau=1`$ case above](#1-one-observation-remaining-case) as...   
    $`\begin{array}{ccc}
      \alpha_2(x;\mathcal{D}) = u(\mathcal{D}_2) - u(\mathcal{D})  
      &=& \overbrace{\left[ u(\mathcal{D}_1) - u(\mathcal{D}) \right]}^{\text{marginal gain from choosing } x} 
      &+& \overbrace{\left[ u(\mathcal{D}_2) - u(\mathcal{D}_1) \right]}^{\text{marginal gain from next (final) choice}} \\
      &=& \alpha_1(x;\mathcal{D}) &+& \underbrace{\mathbb{E}_{y \sim p(y \mid x, \mathcal{D})}
        \left[
          \underbrace{\max_{x_2} \alpha_1(x_2;\mathcal{D}_1)}_{\text{final choice based on updated data}}
        \right]}_{\text{expected gain, estimated now when selecting } x} \\
      &=& \alpha_1(x;\mathcal{D}) &+& \mathbb{E}_{y \sim p(y \mid x, \mathcal{D})} \left[ \alpha_1^*(\mathcal{D}_1) \mid x,\mathcal{D} \right]
    \end{array}`$
    - Desc.)
      - The expected increase in utility after two observations can be decompsed as...
        - $`\alpha_1(x;\mathcal{D})`$
          - the expected increase after our first observation $`x`$
        - $`\mathbb{E}_{y \sim p(y \mid x, \mathcal{D})} \left[ \alpha_1^*(\mathcal{D}_1) \mid x,\mathcal{D} \right]`$
          - the expected additional increase from the final observation $`x_2`$
- Optimal Observation)
  - $`x\in\displaystyle\arg\max_{x'\in\mathcal{X}} \alpha_2 (x';\mathcal{D})`$
- Result)
  - $`u(\mathcal{D}) + \alpha_2^*(\mathcal{D})`$ : Expected Utility
    - where $`\alpha_2^*(\mathcal{D}) = \displaystyle\max_{x'\in\mathcal{X}} \alpha_2(x';\mathcal{D})`$ is... 
      - the expected increase in utility when starting with $`\mathcal{D}`$ and continuing optimally for $`\tau=2`$ additional observations.
      - the value of the dataset $`\mathcal{D}`$ with a horizon of $`\tau=2`$
- Graphics)   
  ![](../images/05/003.png)   
  ![](../images/05/004.png)   

#### 3. Inductive Case
- Settings)
  - An arbitrary $`\tau`$
  - Assume we can compute the value of any dataset with horizon of $`\tau-1`$
- Marginal Gain)   
  $`\begin{array}{cl}
    \alpha_\tau(x;\mathcal{D}) &= \mathbb{E}\left[ u(\mathcal{D}_\tau) \mid x,\mathcal{D} \right]- u(\mathcal{D}) \\
    &= u(\mathcal{D}_\tau) - u(\mathcal{D}) 
    &=& \left[ u(\mathcal{D}_1) - u(\mathcal{D}) \right] &+& \left[ u(\mathcal{D}_\tau) - u(\mathcal{D}) \right] \\
    &&=& \alpha_1(x;\mathcal{D}) &+& \mathbb{E}\left[ \alpha_{\tau-1}(x_2;\mathcal{D}_1) \mid x,\mathcal{D} \right] \\
    &&=& \alpha_1(x;\mathcal{D}) &+& \mathbb{E}\left[ \alpha_{\tau-1}^*(\mathcal{D}_1) \mid x,\mathcal{D} \right] \\
  \end{array}`$
- Optimal Decision)
  - $`x\in\displaystyle\arg\max_{x'\in\mathcal{X}} \alpha_\tau(x';\mathcal{D})`$ : Optimal observation point
  - $`\alpha_\tau^*(\mathcal{D}) = \displaystyle\max_{x'\in\mathcal{X}} \alpha_\tau(x';\mathcal{D})`$ : Optimal marginal gain


### Concept) Bellman Optimality
- Def.)
  - $`\alpha_\tau^*(\mathcal{D}) = \displaystyle\max_{x'\in\mathcal{X}} \left\{ \alpha_1(x';\mathcal{D}) + \mathbb{E}\left[ \alpha_{\tau-1}^*(\mathcal{D}_1) \mid x',\mathcal{D} \right] \right\}`$
- Props.)
  - Recursive definition
  - Assumes that we will always act to maximize expected terminal utility given available data.
- Limit)
  - Exponential time complexity w.r.t. decision horizon $`\tau`$
    - $`O(n^\tau q^\tau)`$
      - where
        - $`n`$ : the number of allowed evaluations of the objective for each recursive call to the optimizer
        - $`q`$ : the number of observations of the integrand for each call to the quadrature routine
        - $`\tau`$ : decision horizon
    - Why?) $`x\in\arg\max \alpha_\tau`$ can be decomposed as...   
      $`\begin{array}{cclll}
        \alpha_\tau &=& \alpha_1 + \mathbb{E}\left[ \alpha_{\tau-1}^* \right]
        &=& \alpha_1 + \mathbb{E}\left[ \max\alpha_{\tau-1} \right]  \\
        &=& \alpha_1 + \mathbb{E}\left[ \max \left\{ \alpha_1 + \mathbb{E}\left[ \alpha_{\tau-2}^* \right] \right\} \right]
        &=& \alpha_1 + \mathbb{E}\left[ \max \left\{ \alpha_1 + \mathbb{E}\left[ \max \alpha_{\tau-2} \right] \right\} \right]  \\
        &=& \alpha_1 + \mathbb{E}\left[ \max \left\{ \alpha_1 + \mathbb{E}\left[ \max \left\{ \alpha_1 + \mathbb{E}\left[ \alpha_{\tau-3}^* \right] \right\} \right] \right\} \right] &=& \cdots
      \end{array}`$
      - Desc.)
        - Until the horizon ($`\tau`$) is reached, each optimal decision requires repeated...
          1. maximization over the domain $`\left(\displaystyle\max_{x'\in\mathcal{X}}\right) \Rightarrow n`$ 
          2. expectation over unknown observations $`\left( \mathbb{E}_{y\sim p(y\mid x,\mathcal{D})}[\cdot] \right) \Rightarrow q`$    
          ![](../images/05/005.png)
- Sol.)
  - Approximate Dynamic Programming
    - [Limited Lookahead](#tech-limited-lookahead)
    - [Rollout](#tech-rollout)


#### Tech.) Limited Lookahead
- Idea)
  - Limit how many future observations we consider in each decision
- Desc.)
  - Artificially limit the horizon as $`\alpha_\tau(x;\mathcal{D}) \approx \alpha_\ell(x;\mathcal{D})`$
    - where $`\ell`$ is the feasible maximum horizon
  - Then we may have $`u(\mathcal{D}_\tau) \approx u(\mathcal{D}_\ell)`$
- Prop.)
  - Myopic approximation
  - $`\alpha_{\min\{\ell, \tau\}}`$ : $`\ell`$-step lookahead policy or rolling horizon strategy
  - $`O(n^\ell q^\ell)`$ time complexity
- e.g.)
  - One step lookahead : $`\alpha_1`$
    - Limit)
      - Focus too much on exploitation
        - Why?) It is oblivious to anything that might happen beyond the present observation.
    - Sol.)
      - Extend the lookahead horizon
        - e.g.) [$`\alpha_2`$ above](#2-two-observation-remaining-case)
      - [Rollout](#tech-rollout)

#### Tech.) Rollout
- Idea)
  - Use a tractable suboptimal policy to simulate future decisions
    - i.e.) Utilize an inexpensive base or heuristic policy to decide $`x_2`$
- e.g.)
  - Lam et al. (2016) *Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach*
  - González et al (2016b) *Glasses: Relieving the Myopia of Bayesian Optimisation*
  - Jiang et al (2020a) *Binoculars for Efficient, Nonmyopic Sequential Experimental Design*

<br>

### Concept) Termination
- Def.)
  - $`\mathcal{A} = \mathcal{X} \cup \{\varnothing\}`$ : new action space
    - where $`\varnothing`$ is an immediate termination action
  - $`\tau_\text{max}`$ : a fixed and known upper bound on the total number of observations we may make
    - e.g.) $`\tau_\text{max}=1,000,000`$ suffices majority of plausible scenarios
  - $`\alpha_\tau(\varnothing;\mathcal{D}) = 0`$
    - i.e.) After termination no other action will ever again be allowed and no data added.

### Concept) Cost-Aware Optimization
- Def.)
  - $`c(x)`$ : a known cost function
  - $`u'(\mathcal{D})`$ : data utility
    - Desc.)
      - Utility ignoring any costs incurred to acquire it.
      - Thus, $`u(\mathcal{D}) = u'(\mathcal{D}) - c(\mathcal{D})`$
        - Assuming that 
          - $`u'(\mathcal{D}) = \displaystyle\sum_{x\in\mathcal{D}}u'(x)`$
          - $`c(\mathcal{D}) = \displaystyle\sum_{x\in\mathcal{D}}c(x)`$



<br><br>

[Back to Main](../main.md)