[Back to Main](../main.md)

# 7. Common Bayesian Optimization Policies

### Concept) Multi-Armed Bandit
- Def.)
  - A finite dimensional model of sequential optimization with noisy optimizations
- Desc.)
  - An agent is faced with a finite set of alternatives (arms)
  - This agent should select a sequence of items from this set.
  - Choices yield stochastic reward drawn from unknown distribution associated with that arm.
  - We seek a sequential policy that maximizes the expected [cumulative reward](./06.md#concept-cumulative-reward).
- Notation)
  - Each point $`x\in\mathcal{X}`$ represents an arm 
  - $`\phi = f(x)`$ is the objective function that determines the stochastic reward with each arm $`x`$ 

<br>

### e.g.) Example GP Model
- Settings)
  - One-dimensional objective function $`f`$ observed without noise
    - Ground Truth of $`f`$   
      ![](../images/07/002.png)
      - Desc.)
        - Numerous local maxima
        - Global maximum on the LHS of the domain
  - GP prior belief about this function
    - Mean : [Constant Function](./03.md#concept-constant-mean-function)
    - Covariance : [Matern with $`v=5/2`$](./03.md#concept-the-mat√©rn-family-covariance-functions)   
      ![](../images/07/001.png)
    - Three observation points
      - Two of which are local maxima
- What we need to choose
  - [Utility Function $`u(\mathcal{D})`$](./05.md#concept-isolated-decision)
  - Model : GP chosen above!
  - Policy
    - e.g.) [One-step look ahead](#concept-one-step-lookahead) below
      - Recall that the [limited lookahead](./05.md#tech-limited-lookahead) was myopic, but computationally tractable compared to the [optimal](./05.md#concept-bellman-optimality) case

<br>

### Concept) One-Step Lookahead
- Def.)
  - Settings)
    - $`\mathcal{D} = (\mathbf{x,y})`$ : the dataset we already have
    - $`u(\mathcal{D})`$ : an arbitrary [utility function](./05.md#concept-isolated-decision) that evaluates the returned dataset $`\mathcal{D}`$
  - Goal)
    - We want to choose a point $`x\in\mathcal{X}`$. 
    - Update our dataset into $`\mathcal{D}' = (\mathbf{x',y'}) = \mathcal{D}\cup\left\{(x,y)\right\}`$
      - where $`y`$ is the corresponding observed value of $`x`$.
      - cf.) Following the notation of the [Bayesian Decision theory](./05.md#concept-bayesian-decision-theory) in the previous chapter we should use $`\mathcal{D}_1`$ to denote the new data set.
        - But, for simplicity, we use $`\mathcal{D}'`$.
  - How?)
    - Consider the [expected marginal gain](./05.md#1-one-observation-remaining-case) in utility after incorporating $`x`$:
      - $`\alpha(x;\mathcal{D}) = \mathbb{E}\left[ u(\mathcal{D}') \mid x,\mathcal{D} \right] - u(\mathcal{D})`$
    - We choose $`x`$ that maximizes this score:
      - $`\displaystyle x\in\arg\max_{x'\in\mathcal{X}} \alpha(x';\mathcal{D})`$

<br>

### Concept) Expected Improvement (EI)
- Desc.)
  - An [acquisition function](./05.md#concept-acquisition-function-infill-function-figure-of-merit) that adopts
    - Utility Function : [Simple Reward](./06.md#concept-simple-reward)
    - Policy : [One-Step Lookahead](#concept-one-step-lookahead)
- Def.)
  - Adopting the [Simple Reward](./06.md#concept-simple-reward), we have
    - $`u(\mathcal{D}) = \max \mu_\mathcal{D}(\mathbf{x})`$
      - cf.) Recall that $`\mathcal{A} = \mathbf{x}`$
        - i.e.) The action space limited to locations evaluated during optimization!
  - Then, the EI can be defined as
    - $`\alpha_{\text{EI}}(x;\mathcal{D}) = \displaystyle \underbrace{\int \left[ \max \mu_{\mathcal{D}'}(\mathbf{x}') \right] \; p(y\mid x,\mathcal{D}) \text{d}y}_{\text{new utility by choosing } x} - \underbrace{\max \mu_\mathcal{D}(\mathbf{x})}_{\text{previous utility}}`$
      - where $`\mathbf{x}' = \mathbf{x}\cup\{x\}`$
  - Further assuming the exact observation $`y = \phi = f(x)`$, we may get more simple form as below.
    - Let
      - $`\mathcal{D} = (\mathbf{x},\boldsymbol{\phi})`$ : the dataset
      - $`\phi^* = \max\boldsymbol{\phi}`$ : the incumbent
        - i.e.) the maximal objective value yet seen.
    - Then, we may rewrite as
      - $`u(\mathcal{D}) = \phi^*`$
      - $`u(\mathcal{D}') = \max(\phi^*, \phi)`$ 
        - where $`\phi`$ is a new observation
      - $`u(\mathcal{D}')-u(\mathcal{D}) = \max(\phi-\phi^*, 0)`$
    - Thus, the Expected Improvement can be denoted as
      - $`\alpha_{\text{EI}}(x;\mathcal{D}) = \displaystyle\int\max(\phi-\phi^*,0)\; p(\phi\mid x,\mathcal{D})\;\text{d}\phi`$
- Props.)   
  ![](../images/07/003.png)
  - The EI vanishes near regions where we have existing observations
  - EI automatically considers the dilemma between exploration and exploitation.
- Application)
  - Sequentially maximizing EI to gather 20 additional observations.   
    ![](../images/07/004.png)
    - Desc.)
      - The first ten observations focused on the exploitation.
        - This may due to the [one-step lookahead](#concept-one-step-lookahead).

<br>

### Concept) Knowledge Gradient (KG)
- Desc.)
  - An [acquisition function](./05.md#concept-acquisition-function-infill-function-figure-of-merit) that adopts
    - Utility Function : [Global Reward](./06.md#concept-global-reward)
      - cf.) In this context, the global reward is interpreted as the amount of "knowledge" about the global maximum offered by a dataset $`\mathcal{D}`$.
      - Then the knowledge gradient can be interpreted as the expected change in knowledge offered by a measurement at $`x`$.
    - Policy : [One-Step Lookahead](#concept-one-step-lookahead)
- Def.)
  - Adopting the [Global Reward](./06.md#concept-global-reward), we have
    - $`u(\mathcal{D}) = \displaystyle\max_{x\in\mathcal{X}} \mu_\mathcal{D}(x)`$
  - Then, the Knowledge Gradient can be defined as
    - $`\alpha_{\text{KG}}(x;\mathcal{D}) = \displaystyle\int\left[ \max_{x'\in\mathcal{X}} \mu_{\mathcal{D}'}(x') \right] \;p(y\mid x,\mathcal{D}) \text{d}y - \max_{x'\in\mathcal{X}} \mu_\mathcal{D}(x')`$
      - Desc.)
        - Knowledge : the global reward
- Props.)   
  ![](../images/07/005.png)
  - The knowledge gradient seeks to maximize the global maximum of the posterior mean, regardless of its location.
    - Why?) It utilizes the [global reward](./06.md#concept-global-reward).
  - Still, KG may seek points in the neighborhood of the best seen point as the relevant local maximum is probably not located precisely at this point.
    - The choice can be the both side of the best seen point.
      - e.g.)   
        ![](../images/07/006.png)
        - Desc.)
          - The dark blue sample corresponds with the next observation point in the image above.
- Application)
  - Sequentially maximizing EI to gather 20 additional observations.   
    ![](../images/07/007.png)
    - Desc.)
      - Somewhat more even exploration of the domain compared to [EI](#concept-expected-improvement-ei).
- Question)
  - Is global reward computationally tractable?


<br>

### Concept) Probability of Improvement (PI)
- Desc.)
  - PI computes the probability of an observed value to improve upon some chosen threshold, regardless of the magnitude of this improvement.
- Def.)
  - Consider a simple reward of the dataset $`\mathcal{D} = (\mathbf{x,y})`$:
    - $`u(\mathcal{D}) = \max \mu_\mathcal{D}(\mathbf{x})`$
  - Let
    - $`\epsilon`$ : the minimum amount of utility improvement we want
    - $`\tau = u(\mathcal{D}) + \epsilon`$ : the desired utility
  - Then, the PI can be defined as
    - $`\alpha_{\text{PI}}(x;\mathcal{D},\tau) = \text{Pr}\left( u(\mathcal{D}') \gt \tau \mid x,\mathcal{D} \right)`$ 
      - where $`u(\mathcal{D}')`$ is the updated utility. 
  - Further assuming the exact observation $`y = \phi = f(x)`$, we may get more simple form as below.
    - Let
      - $`\phi^* = \max f(\mathbf{x}) = u(\mathcal{D})`$
    - Then, the updated utility can be rewritten as
      - $`u(\mathcal{D}') = \max(\phi^*, \phi)`$
        - where $`\phi`$ is a new observation
    - Then, the PI can be defined as
      - $`\alpha_{\text{PI}}(x;\mathcal{D},\tau) = \text{Pr}\left( \phi \gt \tau \mid x,\mathcal{D} \right)`$ 
- Props.)   
  - In general, PI is more risk-averse than [EI](#concept-expected-improvement-ei).   
    ![](../images/07/009.png)
    - Why?)
      - PI prefers a certain improvement of modest magnitude to an uncertain improvement of potentially large magnitude.
      - In the diagram above, the PI will prefer $`x`$ over $`x'`$.


![](../images/07/008.png)


<br>

### Concept) 


<br>

### Concept) 






<br><br>

[Back to Main](../main.md)