[Back to Main](../main.md)

# 1. Introduction
## 1.1 Formalization of Optimization
### Concept) Objective Function 
- Def.)
  - $`f : \mathcal{X} \rightarrow \mathbb{R}`$ : a real-valued objective function
    - Don't require a known functional form or even be computable directly.
    - Only require access to a mechanism revealing some information about the objective function at identified points on demand

<br>

### Concept) Optimization
- Settings
  - $`\mathcal{X}`$ : some domain
  - $`f : \mathcal{X} \rightarrow \mathbb{R}`$ : a real-valued [objective function](#concept-objective-function)
- The goal of optimization)
  - Search a domain for a point $`x^*`$ attaining the globally maximal value $`f^*`$ as
    - $`\displaystyle x^*\in\arg\max_{x\in\mathcal{X}} f(x)`$
    - $`\displaystyle f^* = \max_{x\in\mathcal{X}} f(x) = f(x^*)`$
- How to solve?)
  - [Iterative Optimization Algorithm](#concept-iterative-optimization-algorithm)

<br>

### Concept) Iterative Optimization Algorithm
- Motivation)
  - Directly solving for the location of global optima is infeasible except in exceptional circumstances.
  - Recall that we don't know much about our [objective function](#concept-objective-function).
  - Instead, we are taking an indirect approach.
- How?)
  - Design a sequence of experiments to probe the [objective function](#concept-objective-function) for information
- Goal)
  - Reveal the solution to the [optimization problem](#concept-optimization)
    - $`\displaystyle f^* = \max_{x\in\mathcal{X}} f(x) = f(x^*)`$
- The Algorithm)
  - Input)
    - Initial dataset $`\mathcal{D}`$
  - Main)
    - Repeat until the **termination condition** is reached
      - $`x^* \leftarrow \text{policy}(\mathcal{D})`$ : [Optimization Policy](#concept-optimization-policy)
      - $`y^* \leftarrow \text{observe}(x^*)`$ : [Observation Model](#concept-observation-model)
      - $`\mathcal{D} \leftarrow \mathcal{D} \cup \{(x^*,y^*)\}`$
  - Return)
    - $`\mathcal{D}`$

<br>

### Concept) Optimization Policy
- Goal)
  - Inspects the available data and selects a point $`x\in\mathcal{X}`$
    - We will make our next [observation](#concept-observation-model).
- Prop.)
  - No restriction on the policy
    - May be deterministic or stochastic
- Challenge)
  - Designing policies that can rapidly optimize a broad class of objective functions

<br>

### Concept) Observation Model
- Def.)
  - $`\phi = f(x)`$ : the underlying objective function value (deterministic, actual)
  - $`y`$ : our observation at point $`x`$ (stochastic)
    - with the probability distribution of $`p(y\vert x, \phi)`$ 
- Desc.)
  - We want to know $`\phi`$, but there is no perfect observation in reality due to noise.
  - Thus, we assume that observations are realized by a stochastic mechanism depending on the objective function
- Assumptions) 
  - $`p(\mathbf{y}\vert \mathbf{x}, \mathbf{\phi}) = \prod_{i} p(y_i \vert x_i, \phi_i)`$ (conditional independence)
    - i.e.) multiple measurements $`\mathbf{y} = \begin{bmatrix} y_1 \\\vdots\\y_n \end{bmatrix}`$ are conditionally independent given the corresponding observation locations $`\mathbf{x} = \begin{bmatrix} x_1 \\\vdots\\x_n \end{bmatrix}`$
    - cf.) Not strictly necessary but practically convenient
- e.g.)
  - Gaussian noise
    - $`\mathbf{y} = \phi + \epsilon`$ where $`\epsilon \sim N(0, \sigma_n^2 I)`$
      - Then
        - $`p(\mathbf{y} \vert \mathbf{x}, \phi, \sigma_n) = N(y; \phi, \sigma_n^2 I)`$

<br><br>


find a simulator
in parameters

## 1.2 Bayesian Approach

### Concept) 

<br>

### Concept) 

<br>

### Concept) 

<br>

### Concept) 

<br>

### Concept) 
















<br><br>

[Back to Main](../main.md)