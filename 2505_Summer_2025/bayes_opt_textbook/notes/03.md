[Back to Main](../main.md)

# 3. Modeling with Gaussian Process

### Concept) Prior Mean Function
- Notation) $`\mu(x)`$
  - $`\mu(x) = \mathbb{E}[\phi\mid x]`$
    - where $`\phi = f(x)`$ is the value of $`f`$ at $`x`$
- Props.)
  - The prior mean function only affects the **marginal distribution** of function values.
    - cf.) The [covariance function](#concept-prior-covariance-function) can modify the **joint distribution** of function values.
      - i.e.) [Sample path](./02.md#concept-sample-path) is determined solely by the [covariance function](#concept-prior-covariance-function).
  - Prior mean influences the posterior process only through the [posterior](./02.md#concept-gp-posterior) mean.
    - Why?)
      - $`\underbrace{\mu_\mathcal{D}(x)}_{\text{posterior}} = \underbrace{\mu(x)}_{\text{prior}} + \kappa(x)^\top \mathbf{C^{-1}(y-m)}`$
        - where 
          - $`\mathbf{y}`$ is an observation vector
          - $`p(\mathbf{y}) = \mathcal{N}(\mathbf{y;m,C})`$
          - $`\kappa(x) = \text{cov}[\mathbf{y},\phi\mid x]`$
      - $`K_\mathcal{D}(x,x') = K(x,x')-\kappa(x)^\top \mathbf{C}^{-1}\kappa(x')`$
        - Not determined by $`\mu(x)`$
    - Interpolation vs Extrapolation   
      ![](../images/03/001.png)
      - Interpolatory Region (Data Dominant)
        - Strong correlation between a given function values and observations
        - $`\mu_\mathcal{D}`$ is determined by $`\mathbf{y}`$, not $`\mu(x)  \quad (\because \Vert\kappa(x)\Vert \gggtr 0)`$
      - Extrapolatory Region (Prior Mean Dominant)
        - Weak correlation between a given function values and observations
        - $`\mu_\mathcal{D} \approx \mu(x) \quad (\because \kappa(x) \approx 0)`$


<br>

### Concept) Constant Mean Function
- Def.)
  - $`\mu(x;c)\equiv c`$ where $`c\in\mathbb{R}`$
- Prop.)
  - $`c`$ is usually treated as a parameter to be estimated or marginalized.
- Types)
  - [Constant Mean + Gaussian Prior](#concept-constant-mean--gaussian-prior)
  - [Linear Combination of Basis Functions](#concept-linear-combination-of-basis-functions)
  - Custom Prior Knowledge
    - e.q.) [Quadratic Mean using Mahalanobis Distance](#eg-quadratic-mean-using-mahalanobis-distance)

#### Concept) Constant Mean + Gaussian Prior
- Idea)
  - Give the constant mean the Gaussian prior knowledge.
- How?
  - Suppose we have prior on $`c`$ of $`p(c) = \mathcal{N}(c;a,b^2)`$.
  - Then, we can marginalize $`f`$ as   
    $`\begin{aligned}
        p(f) &= \displaystyle\int p(f\mid c) p(c) dc \\
        &= \mathcal{GP}(f;\mu\equiv a, K+b^2)
    \end{aligned}`$
- Advantage)
  - We can avoid estimating $`c`$ and directly use the prior covariance function of $`b^2`$


#### Concept) Linear Combination of Basis Functions
- Idea)
  - Use basis function $`\psi(x)`$ to synthesize more complex structure of the prior mean
    - e.g.) $`\Psi(x) = \begin{bmatrix} x^2 & x & 1 \end{bmatrix} `$
- How?)
  - Let 
    - $`\mu(x;\boldsymbol{\beta}) = \boldsymbol{\beta^\top\psi}(x)`$ : the prior mean function
      - where
        - $`\boldsymbol{\psi}:\mathcal{X}\rightarrow\mathbb{R}^n`$ : the basis function
        - $`\boldsymbol{\beta}\in\mathbb{R}^n`$
          - cf.) $`\boldsymbol{\beta^\top\psi}(x) \in\mathbb{R}`$
    - $`K`$ : an arbitrary prior covariance function
  - Further assume the multivariate normal prior on $`\boldsymbol{\beta}`$ as $`p(\boldsymbol{\beta}) = \mathcal{N}(\boldsymbol{\beta}; \mathbf{a,B})`$
  - Then we can marginalize $`f`$ as $`p(f) = \mathcal{GP}(f;m,C)`$
    - where $`\begin{cases} m(x) = \mathbf{a}^\top\boldsymbol{\psi}(x) \\ C(x,x') = K(x,x') + \boldsymbol{\psi}(x)^\top\mathbf{B}\boldsymbol{\psi}(x') \end{cases}`$
- cf.)
  - $`\boldsymbol{\psi} = 1`$ is the [Constant Mean + Gaussian Prior](#concept-constant-mean--gaussian-prior) case above.


#### e.g.) Quadratic Mean using Mahalanobis Distance
- Desc.)
  - Assume a concave quadratic mean of $`\mu(\mathbf{x; A,b},c) = \mathbf{(x-b)^\top A^{-1} (x-b)} + c`$
    - where $`\mathbf{A\prec0} `$
- Prop.)
  - $`\mu`$ encodes that values near $`\mathbf{b}`$ are expected to be higher than those farther away   
    ![](../images/03/002.png)


<br>

### Concept) Prior Covariance Function
- Notation)
  - $`K(x,x') = \text{cov}[\phi, \phi' \mid x,x']`$
    - where $`\begin{cases} \phi = f(x) \\ \phi' = f(x')\end{cases}`$
- Props.)
  - The prior covariance function determines the fundamental properties of [sample path](./02.md#concept-sample-path) behavior, including
      - [continuity](./02.md#concept-sample-path-continuity)
      - [differentiability](./02.md#concept-sample-path-differentiability)
      - [aspects of global optima](./02.md#concept-global-maximum-of-gp)
  - $`K`$ must be 
    - symmetric
    - positive semi definite (PSD).


#### Concept) Correlation between function values  
- Def.)    
  - $`\rho = \text{corr}[\phi, \phi' \mid x,x']  = \displaystyle\frac{K(x,x')}{\sqrt{K(x,x)K(x',x')}}`$
    - where $`K`$ is the [prior covariance function](#concept-prior-covariance-function)
- Props.)
  - $`\rho`$ can be interpreted as the strength of the dependence between $`\phi,\phi'`$


#### Concept) Stationary Covariance Function
- Def.)
  - A covariance function $`K(x,x')`$ is stationary iff $`K`$ only depends on the differnce $`x-x'`$
- Notation)
  - $`K(x-x') \Leftrightarrow K(x,x') = K(x-x',0)`$
- Props.)
  - If a GP is stationary, iff it has a **stationary covariance function** and a [constant mean function](#concept-constant-mean-function)
  - The distribution of any set of functions $`\phi, \phi'`$ is invariant under **translation**.
    - i.e.) The function acts the same everywhere!
    - Thus, the local behavior around a single point suffices to specify the global behavior of the entire function!
      - cf.) Not always valid.
        - e.g.) An objective function may exhibit different behavior near the optimum.

#### Concept) Isotropic Covariance Function
- Def.)
  - A covariance function $`K(x,x')`$ is isotropic iff $`K`$ only depends on the Euclidean distance of $`d = \vert x-x \vert'`$
- Notation)
  - $`K(d)`$
- Props.)
  - Isotropy is more restrictive assumption than [stationarity](#concept-stationary-covariance-function).
    - cf.) Isotropy implies stationarity.
  - Covariance is invariant to **translation** and **rotation**.
    - Thus, the function has identical behavior in every direction from every point.


<br>

### Theorem) Bochner, 1993
![](../images/03/003.png)
- Meaning)   
  - The Fourier transform of any [stationary covariance function](#concept-stationary-covariance-function) on $`\mathbb{R}^n`$ is proportional to a probability measure and vice versa.
  - The constant of proportionality is $`K(0)`$
  - $`\nu`$ : the spectral mesure of $`K`$.
  - When a corresponding density function $`\kappa`$ exists, $`\kappa(\boldsymbol{\xi})`$ is called the **spectral density** of $`K`$ and forms a Fourier pair with $`K`$:   
    $`\begin{cases}
        K(\mathbf{x}) &= \displaystyle\int\exp(2\pi i\mathbf{x}^\top \boldsymbol{\xi}) \kappa(\boldsymbol{\xi}) d \boldsymbol{\xi} \\
        \kappa(\boldsymbol{\xi}) &= \displaystyle\int\exp(2\pi i\mathbf{x}^\top \boldsymbol{\xi}) K(\mathbf{x}) d\mathbf{x}
    \end{cases}`$
- Usage)
  - Suppose we found spectral density from a dataset.
  - Then, the Bochner Theorem shows that we can Fourier transform the **spectral density** into [stationary covariance function](#concept-stationary-covariance-function).
    - Recall that [stationary covariance function](#concept-stationary-covariance-function) had nice properties!


<br>

### Concept) 


<br><br>

[Back to Main](../main.md)