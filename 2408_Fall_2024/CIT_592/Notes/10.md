- [Back to Main](../main.md)

# 10. Expectations
### Concept) Random Variable
- Def.)
  - A **random variable** $`X`$ is a **function** from the sample space $`S`$ of an experiment to the real numbers.
    - $`X(S)`$ : the **range** of function $`X`$
    - $`X=r`$ : an event that consists of all outcomes $`s\in S`$ s.t. $`X(s)=r`$
    - $`p(X=r)`$ : the sum of probability that $`X=r`$
      - i.e.) $`\displaystyle \sum_{s; X=r} p(s)`$
  - The distribution of a random variable is the set of all pairs $`(r, p(X=r))`$ s.t. $`r\in X(S)`$

<br>

### Concept) Expectation
- Def.)
  - $`E[X] = \sum_{s\in S} X(s)p(s)`$

#### Prop.) Linearity of Expectation
- For
  - $`X,Y`$ : random variables
  - $`a,b \in \mathbb{R}`$
- $`E[aX + bY] = aE[X] + bE[Y]`$

#### e.g.) N Coin Flips
- Q) What is the expected number of heads when flipping a coin $`n`$ times?
  - Sol.) $`0.5n`$

<br><br>

### Concept) Indicator Random Variable
- Def.)
  - A random variable that takes the value 1 if the event happens and 0 if the event does not happen.
- Prop.)
  - $`E(X_i) = P(X_i = 1)`$
    - i.e.) The expectation is equal to the probability of the event happening.

#### e.g.) Picking 5 Hearts.
- Let $`H`$ be the event that you pick 5 hearts from 52 card suit.
- Sol.)
  - Put $`H = \sum_{i=1}^5 H_i`$.
    - where $`H_i`$ is the probability that $`i`$-th card is heart.
  - Then $`E[H] = E[\sum_{i=1}^5 H_i] = \sum_{i=1}^5 E[H_i]`$.
  - $`\forall i, E[H_i] = \frac{13}{52} = \frac{1}{4}`$.
    - Why?)
      - $`E[H_1] = \frac{13}{52} = \frac{1}{4}`$
      - $`E[H_2] = \frac{13}{52}\cdot\frac{12}{51} + \frac{39}{52}\cdot\frac{13}{51} = \frac{1}{4}`$
      - and so on... 
  - Thus, $`E[H]=\sum_{i=1}^5 E[H_i] = \frac{5}{4}`$


#### e.g.) Picking Hat
- Q) 
  - A bunch of people go to a bar. 
  - Each one of them has to check their hats in. 
  - When they leave the bar they ask for their hats back. However, they are too drunk to be careful about picking up the correct hat. 
  - Compute the expected number of people who get their (own) hats back?
- Sol.)
  - 1
    - Why?)
      - Let $`X_i = \begin{cases}
        1 &, \textrm{his or her hat} \\
        0 &, \textrm{other person's hat}
      \end{cases}`$.
      - Then $`\displaystyle E[X] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \frac{1}{n} = 1`$
    - i.e.)
      - The answer is independent of $`n`$.

#### e.g.) Min Value in Array
- Q)
  - Consider the following algorithm.
    ```python
    def findMin(arr):
      minimum = arr[0]
      for i in range(0,len(arr)):
        if arr[i] < minimum :
          minimum = arr[i]
      return minimum
    ```
  - What is the expected number of times the variable minimum gets reassigned (number of updates)?
- Sol.)
  - Put $`X_i`$ be the event that `arr[i]` is the minimum value in the set `{arr[0], arr[1], ..., arr[i]}`
  - Then $`P(X_i = 1) = \frac{1}{i+1}`$.
  - Thus, $`\displaystyle E[X_i] = \sum_{i=0}^{n-1} 1 \cdot P(X_i = 1) =  \sum_{i=0}^{n-1} \frac{1}{i+1} \approx \ln n `$

#### e.g.) Bernoulli Trial
- The probability of exactly $`k`$ successes in a sequence of n independent Bernoulli trials, with probability of success $`p`$ and probability of failure $`q = 1 âˆ’ p`$ is
  - $`\begin{pmatrix} n\\k \end{pmatrix} p^k q^{n-k}`$



<br><br>

- [Back to Main](../main.md)