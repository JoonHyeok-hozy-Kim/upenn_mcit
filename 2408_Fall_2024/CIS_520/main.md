[Back to Main](../../README.md)

# CIS 520 Machine Learning

|No|Subject|Keywords|
|:-:|:-|:-|
| 1|[Supervised Learning and Error](notes/01.md)|- Model, Training <br> - Supervised Learning : Classification, Regression <br> - Training Error, Loss, Mean Squared Error, Empirical Risk Minimization <br> - Generalization, True Risk, Generalization Gap|
| 2|[k Nearest Neighbors (k-NN)](notes/02.md)|- Minkowski Distance, Normalized Compression Distance (NCD) <br> - Curse of Dimensionality|
| 3|[Linear Models and Perceptron](notes/03.md)|- Linear Predictors <br> - Perceptron : Separability, Convergence Theorem|
| 4|[Gradient Descent](notes/04.md)|- Taylor Approximation, Learning Rate <br> - ADAGRAD <br> - Newton's Method <br> - Convexity, Smoothness <br> - Convergence, Convergence Rate|
| 5|[Gradient Descent on Smooth <br> and Strongly Convex Functions](notes/05.md)|- $`\mu`$-Strongly Convex|
| 6|[Linear Regression](notes/06.md)|- Steps of Learning <br> - Linear Regression <br> - Overfitting <br> - Regularization : $`L_2`$, $`L_1`$, Elastic Net <br> - Ridge Regression, Lasso|
| 7|[Logistic Regression](notes/07.md)|- Squashing Function : Sigmoid Function|
| 8|[Support Vector Machine (SVM)](notes/08.md)|- Margin <br> - Hard Margin SVM <br> - Soft Margin SVM|
|9-10|[Kernels Part 1 & 2](notes/09.md)|- Feature Map $`\phi(\mathbf{x})`$ <br> - Kernel Trick <br> - The Radial Basis Function (RBF) Kernel (Gaussian Kernel) <br> - Kernel Constructions|
|11|[Gaussian Process](notes/11.md)|- Multivariate Gaussian Distribution, Covariance, Correlation <br> - Periodic Kernel|




<br>

- Exam Prep
  - [Mid Term](exam_prep/mid_term.md)



[Back to Main](../../README.md)