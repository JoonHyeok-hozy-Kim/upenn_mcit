[Back to Main](../../README.md)

# CIS 520 Machine Learning

|No|Subject|Keywords|
|:-:|:-|:-|
| 1|[Supervised Learning and Error](notes/01.md)|- Model, Training <br> - Supervised Learning : Classification, Regression <br> - Training Error, Loss, Mean Squared Error, Empirical Risk Minimization <br> - Generalization, True Risk, Generalization Gap|
| 2|[k Nearest Neighbors (k-NN)](notes/02.md)|- Minkowski Distance, Normalized Compression Distance (NCD) <br> - Curse of Dimensionality|
| 3|[Linear Models and Perceptron](notes/03.md)|- Linear Predictors <br> - Perceptron : Separability, Convergence Theorem|
| 4|[Gradient Descent](notes/04.md)|- Taylor Approximation, Learning Rate <br> - ADAGRAD <br> - Newton's Method <br> - Convexity, Smoothness <br> - Convergence, Convergence Rate|
| 5|[Gradient Descent on Smooth and Strongly Convex Functions](notes/05.md)|- $`\mu`$-Strongly Convex|








[Back to Main](../../README.md)