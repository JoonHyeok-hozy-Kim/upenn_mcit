{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import packages."
      ],
      "metadata": {
        "id": "C_F-T_BI-lb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eNX6cprp5s-X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Decision Trees and Bagging\n",
        "\n",
        "In this problem, we'll implement a simplified version of random forests. We'll be using the `iris` dataset, which has 4 features that are discretized to $0.1$ steps between $0$ and $8$ (i.e. the set of all possible features is $\\{0.1, 0.2, \\dots, 7.8, 7.9\\}$. Thus, all thresholds that we'll need to consider are $\\{0.15, 0.25, \\dots, 7.75, 7.85\\}$.\n",
        "\n",
        "Your task in this first part is to finish the implementation of decision trees. We've provided a template for some of the decision tree code, following which you'll finish the bagging algorithm to get a random forest.\n",
        "\n",
        "1. Entropy (2pts): calculate the entropy of a given vector of labels in the `entropy` function. Note that the generalization of entropy to 3 classes is $H = -\\sum_{i=1}^3 p_i\\log(p_i)$ where $p_i$ is the proportion of examples with label $i$.\n",
        "2. Find the best split (1pt): finish the `find_split` function by finding the feature index and value that results in the split minimizing entropy.\n",
        "3. Build the tree (2pts): finish the `expand_node` function by completing the recursive call for building the decision tree.\n",
        "4. Predict with tree (2pts): implement the `predict_one` function, which makes a prediction for a single example.\n",
        "\n",
        "Throughout these problems, the way we represent the decision tree is by using python dicts. In particular, a node is a `dict` that can have the following keys:\n",
        "\n",
        "1. `node['label']` Return the label that we should predict upon reaching this node. node should **only** have a `label` entry if it is a leaf node.\n",
        "2. `node['left']` points to another node dict that represents this node's left child.\n",
        "3. `node['right']` points to another node dict that represents this node's right child.\n",
        "4. `node['split']` is a tuple containing the feature index and value that this node splits left versus right on.\n",
        "\n",
        "In our implementation, all comparisons will be **greater than** comparisons, and \"yes\" answers go **left**. In other words, if `node['split'] = (2, 1.25)`, then we expect to find all data remaining at this node with feature 2 value **greater** than 1.25 in `node['left']`, and feature value **less** than 1.25 in `node['right']`.\n",
        "\n",
        "Tips:\n",
        "+ If you have NaNs, you may be dividing by zero.\n"
      ],
      "metadata": {
        "id": "hlUg32j7-9xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "  def entropy(self, y):\n",
        "      # Calculate the entropy of a given vector of labels in the `entropy` function.\n",
        "      #\n",
        "      # y := Tensor(int) of size (m) -- the given vector of labels\n",
        "      # Return a float that is your calculated entropy\n",
        "\n",
        "      # Fill in the rest\n",
        "      m = y.size(0)\n",
        "      if m == 0:\n",
        "        return 0\n",
        "      frequency = torch.bincount(y).float() / m\n",
        "      # print(frequency)\n",
        "      return -torch.sum(frequency * torch.log(frequency))\n",
        "\n",
        "\n",
        "  def find_split(self, node, X, y, k=4):\n",
        "      # Find the best split over all possible splits that minimizes entropy.\n",
        "      #\n",
        "      # node := Map(string: value) -- the tree represented as a Map, the key will take four\n",
        "      #   different string: 'label', 'split','left','right' (See implementation below)\n",
        "      #   'label': a label node with value as the mode of the labels\n",
        "      #   'split': the best split node with value as a tuple of feature id and threshold\n",
        "      #   'left','right': the left and right branch with value as the label node\n",
        "      # X := Tensor of size (m,d) -- Batch of m examples of demension d\n",
        "      # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
        "      # k := int -- the number of classes, with default value as 4\n",
        "      # Return := tuple of (int, float) -- the feature id and threshold of the best split\n",
        "      m, d = X.size()\n",
        "      result = None\n",
        "      min_loss = torch.inf\n",
        "\n",
        "      features_all = [i for i in range(0, d)]\n",
        "      features_random = torch.randint(0, 4, (k,))\n",
        "\n",
        "      for f_idx in features_all:\n",
        "        for threshold in torch.arange(0.15, 7.9, 0.1):\n",
        "          # print('threshold : {}'.format(threshold))\n",
        "          mask = X[:, f_idx] > threshold\n",
        "          if mask.sum() == 0 or mask.sum() == mask.size(0):\n",
        "            continue\n",
        "\n",
        "          # print('mask : {}'.format(mask))\n",
        "          m_left, y_left = mask.sum(), y[mask]\n",
        "          m_right, y_right = (~mask).sum(), y[~mask]\n",
        "          # print('left : {} \\n right : {}'.format(left, right))\n",
        "          curr_loss = m_left / (m_left + m_right) * self.entropy(y_left)\n",
        "          curr_loss += m_right / (m_left + m_right) * self.entropy(y_right)\n",
        "          min_loss, result = curr_loss, [f_idx, threshold] if min_loss > curr_loss or result is None else result\n",
        "\n",
        "      return result\n",
        "\n",
        "\n",
        "  def expand_node(self, node, X, y, max_depth=1, k=4):\n",
        "      # Completing the recursive call for building the decision tree\n",
        "      # node := Map(string: value) -- the tree represented as a Map, the key will take four\n",
        "      #   different string: 'label', 'split','left','right' (See implementation below)\n",
        "      #   'label': a label node with value as the mode of the labels\n",
        "      #   'split': the best split node with value as a tuple of feature id and threshold\n",
        "      #   'left','right': the left and right branch with value as the label node\n",
        "      # X := Tensor of size (m,d) -- Batch of m examples of demension d\n",
        "      # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
        "      # max_depth := int == the deepest level of the the decision tree\n",
        "      # k := int -- the number of classes, with default value as 4\n",
        "      # Return := tuple of (int, float) -- the feature id and threshold of the best split\n",
        "\n",
        "      H = self.entropy(y)\n",
        "      if H == 0 or max_depth == 0:\n",
        "        return\n",
        "\n",
        "      best_split = self.find_split(node, X, y, k=k)\n",
        "\n",
        "      if best_split is None:\n",
        "        return\n",
        "\n",
        "      mask = X[:, best_split[0]] > best_split[1]\n",
        "      X_left, y_left = X[mask], y[mask]\n",
        "      X_right, y_right = X[~mask], y[~mask]\n",
        "\n",
        "      del node['label']\n",
        "      node['split'] = best_split\n",
        "      node['left'] = {'label' : y_left.mode().values}\n",
        "      node['right'] = {'label' : y_right.mode().values}\n",
        "\n",
        "      self.expand_node(node['left'], X_left, y_left, max_depth-1, k)\n",
        "      self.expand_node(node['right'], X_right, y_right, max_depth-1, k)\n",
        "\n",
        "      return\n",
        "\n",
        "\n",
        "  def predict_one(self, node, x):\n",
        "      # Makes a prediction for a single example.\n",
        "      # node := Map(string: value) -- the tree represented as a Map, the key will take four\n",
        "      #   different string: 'label', 'split','left','right' (See implementation below)\n",
        "      #   'label': a label node with value as the mode of the labels\n",
        "      #   'split': the best split node with value as a tuple of feature id and threshold\n",
        "      #   'left','right': the left and right branch with value as the label node\n",
        "      # x := Tensor(float) of size(d,) -- the single example in a batch\n",
        "      # Fill in the rest\n",
        "      # print(node)\n",
        "      if 'label' in node:\n",
        "        return node['label']\n",
        "\n",
        "      f_idx, threshold = node['split']\n",
        "      if x[f_idx] > threshold:\n",
        "        return self.predict_one(node['left'], x)\n",
        "      else:\n",
        "        return self.predict_one(node['right'], x)"
      ],
      "metadata": {
        "id": "UbPKu7-U--wR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_decision_tree(X,y, k=4):\n",
        "    # The function will fit data with decision tree with the expand_node method implemented above\n",
        "\n",
        "    root = { 'label': y.mode().values }\n",
        "    dt = DecisionTree()\n",
        "    dt.expand_node(root, X, y, max_depth=10, k=k)\n",
        "    return root\n",
        "\n",
        "def predict(node, X):\n",
        "    # return the predict result of the entire batch of examples using the predict_one function above.\n",
        "    dt = DecisionTree()\n",
        "    return torch.stack([dt.predict_one(node, x) for x in X])"
      ],
      "metadata": {
        "id": "JvzsvucYAFup"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with the Iris Dataset!"
      ],
      "metadata": {
        "id": "HtWd-QBSMAiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "data = train_test_split(\n",
        "    iris.data,\n",
        "    iris.target,\n",
        "    test_size=0.5,\n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "X, X_te, y, y_te = [torch.from_numpy(A) for A in data]\n",
        "X, X_te, y, y_te = X.float(), X_te.float(), y.long(), y_te.float()\n",
        "\n",
        "DT = fit_decision_tree(X, y, k=4)\n",
        "print('Train Accuracy : {}'.format((predict(DT, X) == y).float().mean().item()))\n",
        "print('Test Accuracy : {}'.format((predict(DT, X_te) == y_te).float().mean().item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ3N46elz_1l",
        "outputId": "fe6bd890-5f29-4209-8be7-377a977f91ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy : 0.800000011920929\n",
            "Test Accuracy : 0.7733333110809326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging Decision Trees for Random forests\n",
        "\n",
        "Note that our `find_split` implementation can use a random subset of the features when searching for the right split via the argument $k$. For the vanilla decision tree, we defaulted to $k=4$. Since there were 4 features, this meant that the decision tree could always considered all 4 features. By reducing the value of $k$ to $\\sqrt(k)=2$, we can introduce variance into the decision trees for the bagging algorithm.\n",
        "\n",
        "You'll now implement the bagging algorithm. Note that if you use the `clf` and `predict` functions given as keyword arguments, you can pass this section in the autograder without needing a correct implementation for decision trees from the previous section.\n",
        "\n",
        "1. Bootstrap (1pt): Implement `bootstrap` to draw a random bootstrap dataset from the given dataset.\n",
        "2. Fitting a random forest (1pt): Implement `random_forest_fit` to train a random forest that fits the data.\n",
        "3. Predicting with a random forest (1pt): Implement `predict_forest_fit` to make predictions given a random forest.\n",
        "\n",
        "Tip:\n",
        "+ If you're not sure whether your bootstrap is working or not, remember that on average, there will be $1-1/e\\approx 0.632$ unique samples in a bootstrapped dataset."
      ],
      "metadata": {
        "id": "XLJHIJH_NDaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap(X,y):\n",
        "    # Draw a random bootstrap dataset from the given dataset.\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
        "    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
        "    #\n",
        "    # Return := Tuple of (Tensor(float) of size (m,d),Tensor(int) of size(m,)) -- the random bootstrap\n",
        "    #       dataset of X and its correcting lable Y\n",
        "    # Fill in the rest\n",
        "    m = X.size(0)\n",
        "    random_idx = torch.randint(0, m, (m,))\n",
        "    return X[random_idx], y[random_idx]\n",
        "\n",
        "\n",
        "def random_forest_fit(X, y, m, k, clf, bootstrap):\n",
        "    # Train a random forest that fits the data.\n",
        "    # X := Tensor(float) of size (n,d) -- Batch of n examples of demension d\n",
        "    # y := Tensor(int) of size (n) -- the given vectors of labels of the examples\n",
        "    # m := int -- number of trees in the random forest\n",
        "    # k := int -- number of classes of the features\n",
        "    # clf := function -- the decision tree model that the data will be trained on\n",
        "    # bootstrap := function -- the function to use for bootstrapping (pass in \"bootstrap\")\n",
        "    #\n",
        "    # Return := the random forest generated from the training datasets\n",
        "    # Fill in the rest\n",
        "\n",
        "    # DT = fit_decision_tree(X,y,k=4)\n",
        "    # clf is the root of the decision tree\n",
        "    random_forest = [None for _ in range(m)]\n",
        "    sqrt_k = int(k**0.5)\n",
        "    for i in range(m):\n",
        "      X_bag, y_bag = bootstrap(X, y)\n",
        "      curr_tree = clf(X_bag, y_bag, sqrt_k)\n",
        "      random_forest[i] = curr_tree\n",
        "\n",
        "    return random_forest\n",
        "\n",
        "def random_forest_predict(X, clfs, predict):\n",
        "    # Implement `predict_forest_fit` to make predictions given a random forest.\n",
        "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
        "    # clfs := list of functions -- the random forest\n",
        "    # predict := function that predicts (will default to your \"predict\" function)\n",
        "    # Return := Tensor(int) of size (m,) -- the predicted label from the random forest\n",
        "    # Fill in the rest\n",
        "    predictions = [predict(tree, X) for tree in clfs]\n",
        "    stacked = torch.stack(predictions)\n",
        "    mode_predictions, _ = torch.mode(stacked, dim=0)\n",
        "    return mode_predictions"
      ],
      "metadata": {
        "id": "FZ1opSzVZLVo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with the Iris Dataset!"
      ],
      "metadata": {
        "id": "IciPAs72bLoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "RF = random_forest_fit(X, y, 50, 2, clf=fit_decision_tree, bootstrap=bootstrap)\n",
        "\n",
        "print('Train accuracy: ', (random_forest_predict(X, RF, predict) == y).float().mean().item())\n",
        "print('Test accuracy: ', (random_forest_predict(X_te, RF, predict) == y_te).float().mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFXWsJvwbMCn",
        "outputId": "677e44dd-08df-4831-8d9b-69c480f68069"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy:  0.8666666746139526\n",
            "Test accuracy:  0.7733333110809326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Boosting\n",
        "\n",
        "In this problem, you'll implement a basic boosting algorithm on the binary classification breast cancer dataset. Here, we've provided the following weak learner for you: an $\\ell_2$ regularized logistic classifier trained with gradient descent."
      ],
      "metadata": {
        "id": "U-EudO8TECKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Logistic, self).__init__()\n",
        "        self.linear = nn.Linear(30, 1)    # A linear layer with 30 in feature, 1 out feature (i.e. Xw+b where X is m x 30)\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.linear(X)\n",
        "        return out.squeeze()\n",
        "\n",
        "\n",
        "def fit_logistic_clf(X, y):\n",
        "    clf = Logistic()\n",
        "    opt = torch.optim.Adam(clf.parameters(), lr=0.1, weight_decay=1e2)  # Adam optimization function, ðœ† = 1e2\n",
        "    loss = torch.nn.BCEWithLogitsLoss() # Binary Cross Entropy Loss with Logit!\n",
        "\n",
        "    # 200 epoch for optimization!\n",
        "    for t in range(200):\n",
        "        out = clf(X)     # Predict for X\n",
        "        opt.zero_grad()  # Intialize gradient into 0 in every iteration.\n",
        "        loss(out, (y>0).float()).backward() # Calculate the gradient of the difference between the prediction (out) and the actual value (y)\n",
        "        opt.step()       # Update the parameters with the optimzed value.\n",
        "\n",
        "    return clf\n",
        "\n",
        "\n",
        "def predict_logistic_clf(X, clf):\n",
        "    return torch.sign(clf(X)).squeeze()"
      ],
      "metadata": {
        "id": "Jd7nby7sEiWl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to boost this logistic classifier to reduce its bias. Implement the following two functions:\n",
        "\n",
        "+ Finish the boosting algorithm: we've provided a template for the boosting algorithm in `boosting_fit`, however it is missing several components. Fill in the missing snippets of code.\n",
        "+ Prediction after boosting (2pts): implement `boosting_predict` to make predictions with a given boosted model.\n",
        "\n",
        "### Adaboost\n",
        "- Algorithm)\n",
        "  - Initialize $H_1 = 0$.\n",
        "    - i.e.) Classify every example to zero\n",
        "  - `for` $i=1,2,\\cdots,T$\n",
        "    - $\\text{Calculate } h_{t+1}^* \\text{ from } D_{\\mathbf{w}}\\sim\\text{Multinomial}(\\mathbf{w})$\n",
        "    - $\\text{Calculate } \\epsilon_{t}, \\alpha^* \\text{ using } h_{t+1}^*$\n",
        "      - cf.) Refer to [this](https://github.com/JoonHyeok-hozy-Kim/upenn_mcit/blob/main/2408_Fall_2024/CIS_520/notes/15.md#tech-how-to-get-optimal-learning-rate-%CE%B1) for how to get $\\alpha^*$\n",
        "    - $\\text{Update } H_{t+1}\\leftarrow H_t + \\alpha^* h_{t+1}$\n",
        "    - $\\text{Update } w_i = \\exp(-y_i H_{t+1}(\\mathbf{x}_i)) / Z \\text{ for next } D_\\mathbf{w}$\n",
        "- Parameter Optimizations\n",
        "  - $\\displaystyle h_{t+1}^*  = \\arg\\min_{h_{t+1}\\in\\mathcal{H}}  \\sum_{i : h_{t+1}(\\mathbf{x}_i) \\ne y_i} w_i$\n",
        "    - where\n",
        "      - $\\displaystyle w_i = \\frac{1}{Z} \\exp(-y_i H_t(\\mathbf{x}_i))$\n",
        "      - $\\displaystyle Z = \\sum_{i=1}^n \\exp(-y_i H_t(\\mathbf{x}_i))$\n",
        "  - $\\alpha^* = 0.5\\ln\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$"
      ],
      "metadata": {
        "id": "qjyI4sczIKrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boosting_fit(X, y, T, fit_logistic_clf, predict_logistic_clf):\n",
        "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
        "    # y := Tensor(int) of size (m) -- the given vectors of labels of the examples\n",
        "    # T := Maximum number of models to be implemented\n",
        "\n",
        "    m = X.size(0)\n",
        "    clfs = []\n",
        "    while len(clfs) < T:\n",
        "        # Calculate the weights for each sample mu. You may need to\n",
        "        # divide this into the base case and the inductive case.\n",
        "\n",
        "        if len(clfs) == 0:\n",
        "            mu = torch.full((m,), 1/m)  # Initialize weights uniformly\n",
        "            H_prev = torch.zeros((m,))  # Initialize ensemble prediction H_1 = 0\n",
        "\n",
        "        else:\n",
        "            prev_alpha, prev_clf = clfs[-1]\n",
        "            # H_prev += prev_alpha * predict_logistic_clf(X, prev_clf)\n",
        "            H_prev += (prev_alpha * predict_logistic_clf(X, prev_clf)).sign()\n",
        "\n",
        "            mu = torch.exp(-y * H_prev)\n",
        "            mu /= mu.sum()\n",
        "\n",
        "        # Here, we draw samples according to mu and fit a weak classifier\n",
        "        mask = torch.multinomial(mu, m, replacement=True)\n",
        "        X0, y0 = X[mask], y[mask]\n",
        "        clf = fit_logistic_clf(X0, y0)\n",
        "\n",
        "        # Calculate the epsilon error term\n",
        "        predictions = predict_logistic_clf(X, clf).sign()\n",
        "        wrongs = (predictions != y).float()\n",
        "        eps = (mu * wrongs).sum()\n",
        "\n",
        "        if eps > 0.5:\n",
        "            # In the unlikely even that gradient descent fails to\n",
        "            # find a good classifier, we'll skip this one and try again\n",
        "            continue\n",
        "\n",
        "        # Calculate the alpha term\n",
        "        alpha = 0.5 * torch.log2((1-eps) / eps)\n",
        "\n",
        "        clfs.append((alpha, clf))\n",
        "\n",
        "    return clfs\n",
        "\n",
        "\n",
        "def boosting_predict(X, clfs, predict_logistic_clf):\n",
        "    # X := Tensor(float) of size (m,d) -- Batch of m examples of demension d\n",
        "    # clfs := list of tuples of (float, logistic classifier) -- the list of boosted classifiers\n",
        "    # Return := Tnesor(int) of size (m) -- the predicted labels of the dataset\n",
        "\n",
        "    m = X.size(0)\n",
        "    prediction = torch.zeros((m,))\n",
        "    for alpha, clf in clfs:\n",
        "        # prediction = prediction + alpha * predict_logistic_clf(X, clf)\n",
        "        prediction = (prediction + alpha * predict_logistic_clf(X, clf)).sign()\n",
        "\n",
        "    return prediction.sign()"
      ],
      "metadata": {
        "id": "dXwILQqqIL5p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out your code on the breast cancer dataset. As a sanity check, your statndard logistic classifier will get a train/test accuracy of around 80% while the boosted logistic classifier will get a train/test accuracy of around 90%."
      ],
      "metadata": {
        "id": "VEwypndsP-01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer = datasets.load_breast_cancer()\n",
        "data=train_test_split(cancer.data,cancer.target,test_size=0.2,random_state=123)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "X,X_te,y,y_te = [torch.from_numpy(A) for A in data]\n",
        "X,X_te,y,y_te = X.float(), X_te.float(), torch.sign(y.long()-0.5), torch.sign(y_te.long()-0.5)\n",
        "\n",
        "\n",
        "logistic_clf = fit_logistic_clf(X,y)\n",
        "print(\"Logistic classifier accuracy:\")\n",
        "print('Train accuracy: ', (predict_logistic_clf(X, logistic_clf) == y).float().mean().item())\n",
        "print('Test accuracy: ', (predict_logistic_clf(X_te, logistic_clf) == y_te).float().mean().item())\n",
        "\n",
        "boosting_clfs = boosting_fit(X,y, 10, fit_logistic_clf, predict_logistic_clf)\n",
        "print(\"\\nBoosted logistic classifier accuracy:\")\n",
        "print('Train accuracy: ', (boosting_predict(X, boosting_clfs, predict_logistic_clf) == y).float().mean().item())\n",
        "print('Test accuracy: ', (boosting_predict(X_te, boosting_clfs, predict_logistic_clf) == y_te).float().mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDBi0Uo6P_oa",
        "outputId": "e10b4a12-2d38-49d0-9d6d-209e9ad9413d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic classifier accuracy:\n",
            "Train accuracy:  0.795604407787323\n",
            "Test accuracy:  0.7982456088066101\n",
            "\n",
            "Boosted logistic classifier accuracy:\n",
            "Train accuracy:  0.8813186883926392\n",
            "Test accuracy:  0.9298245906829834\n"
          ]
        }
      ]
    }
  ]
}