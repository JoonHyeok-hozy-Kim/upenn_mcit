[Back to Main](../main.md)

# 15. Boosting
### Concept) Weak Learner
- Desc.)
  - A model with large bias and high training error
- e.g.)
  - Decision Stump
    - Def.)
      - Depth 1 [Decision Tree](13.md#13-decision-trees).
    - e.g.)
      - $`h(\mathbf{x}) = \begin{cases} +1 & [\mathbf{x}_1] \lt 5 \\ -1 & [\mathbf{x}_1] \ge 5 \end{cases}`$
        - i.e.) Split on the first feature $`[\mathbf{x}_1]`$ with the value of 5.
- Why needed for Boosting?)
  - The intuition of the [Boosting](#concept-general-boosting-algorithm) is to create an ensemble model with weak learners.
  - By adding **weak learners** that learn information that was not learned by the previous ensemble, the boosting algorithm improves its performance.
    - cf.) It's the opposite of [Bagging](14.md#concept-bagging-bootstrap-aggregating).
      - Why?) Bagging's motive was to prevent the overfit of a powerful model such as [Decision Tree](13.md#13-decision-trees) by averaging out the noise that cause the overfitting. 
- Requirement to be used in Boosting)
  - Classification Settings)
    - It is possible for $`h(\mathbf{x})`$ to achieve greater than 50% training accuracy.
  - Regression Settings)
    - $`\exists \alpha_t`$ s.t. $`H_{t-1}(\mathbf{x})`$ improves by $`H_{t-1}(\mathbf{x}) + \alpha_t h_t(\mathbf{x})`$
      - where $`H_{t-1}(\mathbf{x})`$ is the ensemble of $`h(\mathbf{x})`$s at $`t`$. 

<br>

### Concept) General Boosting Algorithm
- Settings)
  - Let
    - $`\mathcal{D} = \{(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_n, y_n)\}`$ : the labeled dataset
    - $`h_t(\mathbf{x})`$ : a new [weak learner](#concept-weak-learner) at $`t`$.
    - $`\displaystyle H_t(\mathbf{x}) = \sum_{i=1}^t \alpha_i h_i(\mathbf{x})`$ : an ensemble of weak learners until $`t`$.
      - where $`\alpha_i`$ is the learning rate at the time $`i`$
- Update Rule : Boosting
  - $`\displaystyle H_t(\mathbf{x}) = H_{t-1}(\mathbf{x}) + \alpha_t h_{t}(\mathbf{x})`$
    - i.e.) Add new weak learner to the existing ensemble!
    - Graphical Intuition
      ||
      |:-|
      |<img src="../images/15/001.png" width="500px">|
- Solution)
  - $`\displaystyle h_{t+1}^* \approx \arg\min_{h_{t+1}\in\mathcal{H}} \sum_{i=1}^n \frac{\partial \ell}{\partial H_t(\mathbf{x}_i)} h_{t+1}(\mathbf{x}_i)`$ 
    - Or find an algorithm s.t. $`\mathcal{A}_\ell(\mathcal{D}, H_t) = h_{t+1}^*`$
- Derivation)
  - Questions)
    - Among many candidates for $`h_{t}(\mathbf{x})`$, which one should we pick?
      - Find $`h_{t}(\mathbf{x})`$ that minimizes the loss at $`t`$.
    - Then what is loss?
      - Recall that our dataset is labeled.
      - Thus, we can calculate the loss of an ensemble at $`t`$ as
        - $`\displaystyle \ell(H_t) = \frac{1}{n}\sum_{i=1}^n \ell(H_t(\mathbf{x}_i), y_i)`$
  - Optimization)
    - $`\displaystyle h_{t+1} = \arg\min_{h_{t+1}\in\mathcal{H}} \ell (H_t + \alpha_{t+1} h_{t+1})`$.
      - cf.) Why is the $`y_i`$ term dropped? (Asked on Ed)
    - We now have to solve this problem iteratively.
    - Recalled that the [Gradient Descent](04.md#4-gradient-descent) used [the Taylor Approximation](04.md#concept-taylor-approximation) to form a loss function.
    - Applying [the Taylor Approximation](04.md#concept-taylor-approximation) to this loss around $`H_t`$, we get
      - $`\ell (H_t + \alpha_{t+1} h_{t+1}) \approx \ell(H_t) + \langle \nabla_{H_t}\ell(H_t), \alpha_{t+1} h_{t+1} \rangle`$
        - where $`\langle \cdot, \cdot \rangle`$ is the inner product.
    - Consider that $`\nabla_{H_t}\ell(H_t) = \begin{bmatrix} \frac{\partial \ell}{\partial H_t(\mathbf{x}_1)} & \cdots & \frac{\partial \ell}{\partial H_t(\mathbf{x}_n)} \end{bmatrix}^\top`$
      - Thus,    
        $`\begin{aligned}
          \langle \nabla_{H_t}\ell(H_t), \alpha_{t+1} h_{t+1} \rangle 
          &= \begin{bmatrix} \frac{\partial \ell}{\partial H_t(\mathbf{x}_1)} & \cdots & \frac{\partial \ell}{\partial H_t(\mathbf{x}_n)} \end{bmatrix}^\top \cdot \begin{bmatrix} h_{t+1}(\mathbf{x}_1) & \cdots & h_{t+1}(\mathbf{x}_n) \end{bmatrix} \\
          &= \sum_{i=1}^n \left(\frac{\partial \ell}{\partial H_t(\mathbf{x}_i)}\right) h_{t+1}(\mathbf{x}_i) \\
        \end{aligned}`$.
    - Also, consider that $`\frac{\partial \ell(H_t)}{\partial h_{t+1}} = 0`$.
    - Therefore, the optimization problem can be rewritten as   
      $`\begin{aligned}
          h_{t+1}^* &\approx \arg\min_{h_{t+1}\in\mathcal{H}} \langle \nabla_{H_t}\ell(H_t), \alpha_{t+1} h_{t+1} \rangle \\
          &= \arg\min_{h_{t+1}\in\mathcal{H}} \sum_{i=1}^n \frac{\partial \ell}{\partial H_t(\mathbf{x}_i)} h_{t+1}(\mathbf{x}_i)
      \end{aligned}`$
      - i.e) Find an algorithm $`\mathcal{A}_\ell(\mathcal{D}, H_t) = h_{t+1}^*`$
- Prop.)
  - However, we cannot derive the general solution for the above optimization problem.
  - Special case that we can get the solution : [Gradient Boosted Regression Trees (GBRT)](#concept-gradient-boosted-regression-trees-gbrt)

<br><br>

### Concept) Gradient Boosted Regression Trees (GBRT)
- Model)
  - $`\mathcal{A}_{\ell_\text{sq}} (\mathcal{D}, H_t) = \mathcal{A}_{\text{Regression-Tree}} (\{(\mathbf{x}_1, r_1), (\mathbf{x}_2, r_2), \cdots, (\mathbf{x}_n, r_n)\})`$
    - where $`r_{it} = H_t(\mathbf{x}_i) - y_i`$ : the residual of the $`i`$-th data at $`t`$.
- Derivation)
  - Recall from the [General Boosting Algorithm](#concept-general-boosting-algorithm) that the loss of an ensemble at $`t`$ was $`\displaystyle \ell(H_t) = \frac{1}{n}\sum_{i=1}^n \ell(H_t(\mathbf{x}_i), y_i)`$.
  - Applying the squared loss, we may get the loss as
    - $`\displaystyle \ell_{\text{sq}}(H) = \frac{1}{2}\sum_{i=1}^n (H(\mathbf{x}_i) - y_i)^2 \Rightarrow \frac{\partial \ell_{\text{sq}}}{\partial H_t(\mathbf{x}_i)} = \underbrace{H_t(\mathbf{x}_i) - y_i}_{r_i : \text{residual}}`$.
  - Plugging this back into the [General Boosting Algorithm](#concept-general-boosting-algorithm)'s solution, we have   
    $`\begin{aligned}
        h_{t+1}^* &\approx \arg\min_{h_{t+1}\in\mathcal{H}} \sum_{i=1}^n \frac{\partial \ell}{\partial H_t(\mathbf{x}_i)} h_{t+1}(\mathbf{x}_i) \\
        &= \arg\min_{h_{t+1}\in\mathcal{H}} \sum_{i=1}^n r_{it} h_{t+1}(\mathbf{x}_i)
    \end{aligned}`$
  - Still, not easy to solve, so we will apply some algebraic manipulations.
    1. Multiply the objective function by 2.   
       $`\begin{aligned}
        h_{t+1}^* &\approx \arg\min_{h_{t+1}\in\mathcal{H}} \sum_{i=1}^n r_{it} h_{t+1}(\mathbf{x}_i) \\
        &= \arg\min_{h_{t+1}\in\mathcal{H}} 2\sum_{i=1}^n r_{it} h_{t+1}(\mathbf{x}_i)
       \end{aligned}`$
       - This does not affect the optimization problem.
    2. Assume that $`\displaystyle\sum_{i=1}^n h_{t+1}(\mathbf{x}_i)^2`$ is constant.
       - Desc.)
         - In general, it cannot be true.
         - However, recall that in our model, we update by $`\displaystyle H_{t+1}(\mathbf{x}) = H_{t}(\mathbf{x}) + \alpha_{t+1} h_{t+1}(\mathbf{x})`$.
         - Thus, we can adjust $`\alpha_{t+1}`$ to satisfy $`\displaystyle\sum_{i=1}^n h_{t+1}(\mathbf{x}_i)^2 = C (\exists C\in\mathbb{R})`$
       - Why doing this?)
         - If $`\displaystyle\sum_{i=1}^n h_{t+1}(\mathbf{x}_i)^2`$ is a constant, adding this to the objective function problem does not affect the solution.
    3. Add $`\displaystyle \sum_{i=1}^n r_i^2 = \sum_{i=1}^n (H_t(\mathbf{x}_i)-y_i)^2`$ to the objective function.
       - Since $`r_i^2`$ does not depend on $`h_{t+1}`$, it does not affect the optimization problem.
  - Then, the optimization problem goes   
    $`\begin{aligned}
        h_{t+1}^* &\approx \arg\min_{h_{t+1}\in\mathcal{H}} \sum_{i=1}^n r_{it} h_{t+1}(\mathbf{x}_i) \\
        &= \arg\min_{h_{t+1}\in\mathcal{H}} \left( \sum_{i=1}^n r_i^2 + 2\sum_{i=1}^n r_{it} h_{t+1}(\mathbf{x}_i) + \sum_{i=1}^n h_{t+1}(\mathbf{x}_i)^2 \right) \\
        &= \arg\min_{h_{t+1}\in\mathcal{H}} \left( r_i + h_{t+1}(\mathbf{x}_i) \right)^2 \\
        &= \arg\min_{h_{t+1}\in\mathcal{H}} \left( h_{t+1}(\mathbf{x}_i) + (H_t(\mathbf{x}_i) - y_i) \right)^2 \\
        &= \arg\min_{h_{t+1}\in\mathcal{H}} \left( h_{t+1}(\mathbf{x}_i) - \underbrace{(y_i - H_t(\mathbf{x}_i))}_{\text{Assume this to be a label}} \right)^2 \\
    \end{aligned}`$
    - We know how to solve this problem!
      - Find a weak learner that minimizes the squared loss between the predictions $`h(\mathbf{x}_i)`$ and the negative residual $`(y_i - H_t(\mathbf{x}_i))`$.
      - We can do this by passing the value $`y_i - H_t(\mathbf{x}_i)`$ instead of the label $`y_i`$.
        - i.e.) $`\mathcal{A}_{\ell_\text{sq}} (\mathcal{D}, H_t) = \mathcal{A}_{\text{Regression-Tree}} (\{(\mathbf{x}_1, r_1), (\mathbf{x}_2, r_2), \cdots, (\mathbf{x}_n, r_n)\})`$








<br><br>

[Back to Main](../main.md)