[Back to Main](../main.md)

# 20. Dimensionality Reduction
### Concept) Dimensionality Reduction
- Settings)
  - Unsupervised Learning
    - Dataset without labels : $`\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\}`$
      - where $`\mathbf{x}_i \in\mathbb{R}^D`$
- Advantages)
  - Computational Efficiency
    - Many algorithms are much faster on **lower dimensional** data.
  - Improved Learning
    - Many algorithmsâ€™ assumptions are better satisfied in **low dimensions**.
  - Pretraining
    - **Pretraining** is the process by which good features for supervised learning can be learned from very large amounts of unsupervised data.
      - Why this matters?)
        - Unsupervised data is often much easier to come by.
        - Thus, with pretraining we can make the most out of the valuable labeled data.
  - Visualization
    - Low dimensional data is more easy to visualize compared to the higher dimensional ones.
- Model)
  - $`f:\mathbb{R}^D\rightarrow\mathbb{R}^d`$ : encoder
    - where $`d\lt D`$.
  - $`g:\mathbb{R}^d\rightarrow\mathbb{R}^D`$ : decoder
  - Then we get
    - $`\underbrace{\mathbf{x}_i}_{\in\mathbb{R}^D} \rightarrow \underbrace{f(\mathbf{x}_i) \rightarrow \mathbf{z}_i}_{\in\mathbb{R}^d} \rightarrow \underbrace{g(\mathbf{z}_i) \rightarrow \hat{\mathbf{x}}_i}_{\in\mathbb{R}^D}`$
- Optimization Problem)
  - Minimize $`\Vert \mathbf{x}_i - \hat{\mathbf{x}}_i\Vert`$
- Implementation Algorithms)
  - [Principal Component Analysis (PCA)](#concept-principal-component-analysis-pca)
  - Autoencoder

<br><br>

### Concept) Principal Component Analysis (PCA)
- Settings)
  - $`\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\} \subset \mathbb{R}^D`$ : dataset
  - $`d\lt D`$ : the target dimensionality.
- Goal)
  - Find a projection $`f: \mathbb{R}^D \rightarrow \mathbb{R}^d`$ that 
    - maps $`\mathbf{x}\in\mathbb{R}^D`$ to $`\mathbf{z}\in\mathbb{R}^d`$
    - while preserving the structure of the data.
  - Especially in PCA, we will find an orthogonal projection of the form
    - $`\mathbf{z} := f(\mathbf{x}) = Q \mathbf{x}`$ 
      - where $`Q\in\mathbb{R}^{d\times D}`$
  - Using the previous notation in [dimensionality reduction](#20-dimensionality-reduction) we can put
    - $`\hat{\mathbf{x}} := g(\mathbf{z}) = Q^\top \mathbf{z}`$
- Assumption)
  - Zero mean assumption on $`\mathbf{x}`$.
    - $`\displaystyle\bar{\mathbf{x}} = \frac{1}{n}\sum_{i=1}^n x_i = 0`$
      - for the calculation of PCA.
    - Even though $`\bar{\mathbf{x}} \ne 0`$, one can simply modify the input as
      - $`\mathbf{x}_i \leftarrow \mathbf{x}_i - \bar{\mathbf{x}}_i`$
    - And this assumption does not impact the dimensionality reduction.
      - Why?)
        - Suppose we achieved the encoder function $`f`$ with the zero mean treatment on $`\mathbf{x}\in\mathbb{R}^D`$.
        - And, $`g:\mathbb{R}^d \rightarrow \mathbb{R}^D`$ is the reconstruction function.
        - Then, we can achieve the real reconstruction function as
          - $`g^+(\mathbf{x}) = g(\mathbf{x}) + \bar{\mathbf{x}}`$
- Two views of PCA
  - [View 1)](#view-1-pca-as-finding-maximum-variance-detection) PCA finds the directions $`Q`$ of **maximum variance** when the data $`\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n`$ are projected onto $`Q`$.
  - [View 2)](#view-2-pca-as-minimizing-reconstruction-error) PCA finds the directions $`Q`$ so that the reconstruction error $`\Vert g(f(\mathbf{x})) - \mathbf{x}\Vert_2^2 = \Vert Q^\top Q\mathbf{x} - \mathbf{x}\Vert_2^2`$ is **minimized** across the data points.

<br>

### View 1) PCA as Finding Maximum Variance Detection
- Settings)
  - $`\mathbf{X} = \begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \\ \vdots \\ \mathbf{x}_n \end{bmatrix} \in \mathbb{R}^{n\times D}`$ : $`n`$ number of $`D`$-dimensional training dataset
  - $`\displaystyle S = \frac{1}{n-1}\mathbf{X}^\top\mathbf{X}`$ : Sample covariance matrix
    - Recall that $`\bar{\mathbf{x}} = 0`$!
  - $`\mathbf{q}_i \in\mathbb{R}^D`$ : the $`i`$-th unit vector
    - i.e. $`\Vert \mathbf{q}_i \Vert = 1, \forall i`$
  - $`Q = \begin{bmatrix} \mathbf{q}_1 \\ \mathbf{q}_2 \\ \vdots \\ \mathbf{q}_d \end{bmatrix} \in \mathbb{R}^{d\times D}`$
- Derivation)
  - Consider the projection of $`\mathbf{x}_i`$ onto $`\mathbf{q}_j`$ is given by
    - $`(\mathbf{x}_i^\top \mathbf{q}_j) \mathbf{q}_j`$
      - cf.) $`\mathbf{x}_i^\top \mathbf{q}_j`$ can be interpreted as the shadow that $`\mathbf{x}_i`$ casts on $`\mathbf{q}_j`$ assuming $`\mathbf{q}_j`$ is a unit vector.
  - Now consider the case that the $`n`$ data points $`\mathbf{x}_i`$s are projected on the unit vector $`\mathbf{q}_j`$.
    - $`\left[ \mathbf{z}_1, \cdots, \mathbf{z}_n \right] = \left[ \mathbf{x}_1^\top \mathbf{q}_j, \cdots, \mathbf{x}_n^\top \mathbf{q}_j \right]`$

<br>

### View 2) PCA as Minimizing Reconstruction Error



<br><br>

[Back to Main](../main.md)