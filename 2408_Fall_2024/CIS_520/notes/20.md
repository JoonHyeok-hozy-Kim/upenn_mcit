[Back to Main](../main.md)

# 20. Dimensionality Reduction
### Concept) Dimensionality Reduction
- Settings)
  - Unsupervised Learning
    - Dataset without labels : $`\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\}`$
      - where $`\mathbf{x}_i \in\mathbb{R}^D`$
- Advantages)
  - Computational Efficiency
    - Many algorithms are much faster on **lower dimensional** data.
  - Improved Learning
    - Many algorithmsâ€™ assumptions are better satisfied in **low dimensions**.
  - Pretraining
    - **Pretraining** is the process by which good features for supervised learning can be learned from very large amounts of unsupervised data.
      - Why this matters?)
        - Unsupervised data is often much easier to come by.
        - Thus, with pretraining we can make the most out of the valuable labeled data.
  - Visualization
    - Low dimensional data is more easy to visualize compared to the higher dimensional ones.
- Model)
  - $`f:\mathbb{R}^D\rightarrow\mathbb{R}^d`$ : encoder
    - where $`d\lt D`$.
  - $`g:\mathbb{R}^d\rightarrow\mathbb{R}^D`$ : decoder
  - Then we get
    - $`\underbrace{\mathbf{x}_i}_{\in\mathbb{R}^D} \rightarrow \underbrace{f(\mathbf{x}_i) \rightarrow \mathbf{z}_i}_{\in\mathbb{R}^d} \rightarrow \underbrace{g(\mathbf{z}_i) \rightarrow \hat{\mathbf{x}}_i}_{\in\mathbb{R}^D}`$
- Optimization Problem)
  - Minimize $`\Vert \mathbf{x}_i - \hat{\mathbf{x}}_i\Vert`$
- Implementation Algorithms)
  - [Principal Component Analysis (PCA)](#concept-principal-component-analysis-pca)
  - Autoencoder

<br><br>

### Concept) Principal Component Analysis (PCA)
- Settings)
  - $`\mathcal{D} = \{\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\} \subset \mathbb{R}^D`$ : dataset
  - $`d\lt D`$ : the target dimensionality.
- Goal)
  - Find a projection $`f: \mathbb{R}^D \rightarrow \mathbb{R}^d`$ that 
    - maps $`\mathbf{x}\in\mathbb{R}^D`$ to $`\mathbf{z}\in\mathbb{R}^d`$
    - while preserving the structure of the data.
  - Especially in PCA, we will find an orthogonal projection of the form
    - $`\mathbf{z} := f(\mathbf{x}) = Q \mathbf{x}`$ 
      - where $`Q\in\mathbb{R}^{d\times D}`$
  - Using the previous notation in [dimensionality reduction](#20-dimensionality-reduction) we can put
    - $`\hat{\mathbf{x}} := g(\mathbf{z}) = Q^\top \mathbf{z}`$
- Assumption)
  - Zero mean assumption on $`\mathbf{x}`$.
    - $`\displaystyle\bar{\mathbf{x}} = \frac{1}{n}\sum_{i=1}^n x_i = 0`$
      - for the calculation of PCA.
    - Even though $`\bar{\mathbf{x}} \ne 0`$, one can simply modify the input as
      - $`\mathbf{x}_i \leftarrow \mathbf{x}_i - \bar{\mathbf{x}}_i`$
    - And this assumption does not impact the dimensionality reduction.
      - Why?)
        - Suppose we achieved the encoder function $`f`$ with the zero mean treatment on $`\mathbf{x}\in\mathbb{R}^D`$.
        - And, $`g:\mathbb{R}^d \rightarrow \mathbb{R}^D`$ is the reconstruction function.
        - Then, we can achieve the real reconstruction function as
          - $`g^+(\mathbf{x}) = g(\mathbf{x}) + \bar{\mathbf{x}}`$
- Two views of PCA
  - [View 1)](#view-1-pca-as-finding-maximum-variance-detection) PCA finds the directions $`Q`$ of **maximum variance** when the data $`\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n`$ are projected onto $`Q`$.
  - [View 2)](#view-2-pca-as-minimizing-reconstruction-error) PCA finds the directions $`Q`$ so that the reconstruction error $`\Vert g(f(\mathbf{x})) - \mathbf{x}\Vert_2^2 = \Vert Q^\top Q\mathbf{x} - \mathbf{x}\Vert_2^2`$ is **minimized** across the data points.

<br>

### View 1) PCA as Finding Maximum Variance Detection
- Settings)
  - $`\mathbf{X} = \begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \\ \vdots \\ \mathbf{x}_n \end{bmatrix} \in \mathbb{R}^{n\times D}`$ : $`n`$ number of $`D`$-dimensional training dataset
  - $`\displaystyle S = \frac{1}{n-1}\mathbf{X}^\top\mathbf{X}`$ : Sample covariance matrix
    - Recall that $`\bar{\mathbf{x}} = 0`$!
  - $`\mathbf{q}_j \in\mathbb{R}^D`$ : the $`j`$-th unit vector
    - i.e. $`\Vert \mathbf{q}_j \Vert = 1, \forall j`$
  - $`\mathbf{Q} = \begin{bmatrix} \mathbf{q}_1^\top \\ \mathbf{q}_2^\top \\ \vdots \\ \mathbf{q}_d^\top \end{bmatrix} \in \mathbb{R}^{d\times D}`$
- Model)
  - $`\mathbf{q}_j`$s are $`d`$ number of eigenvectors of $`S`$ with $`d`$ largest eigenvalues.
    - And, $`\mathbf{q}_j`$ is called the [principal component](#concept-principal-component-analysis-pca).
  - Thus, we can calculate $`\mathbf{Q}^\top`$ with...
    - Eigenvalue Decomposition)
      - Procedure)
        - Compute the eigenvalue decomposition $`S = V\Lambda V^\top`$
          - where
            - $`V\in\mathbb{R}^{D\times D}`$ : the orthogonal matrix whose columns are the eigenvectors of $`S`$.
            - $`\Lambda\in\mathbb{R}^{D\times D}`$ : the diagonal matrix containing the eigenvalues.
        - Then the first $`d`$ columns of $`V`$ is $`Q^\top = \begin{bmatrix} \mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_d \end{bmatrix}\in\mathbb{R}^{D\times d}`$
      - Runtime Analysis)
        - $`O(nD^2 + D^3)`$
          - $`O(nD^2)`$ for computing $`\mathbf{X^\top X}`$
          - $`O(D^3)`$ for computing the eigendecomposition.
    - Singular Value Decomposition)
      - Procedure)
        - Compute the singular value decomposition $`\mathbf{X} = U\Sigma V^\top`$
          - where 
            - $`\Sigma\in\mathbb{R}^{D\times D}`$ : the diagonal matrix
            - $`U,V\in\mathbb{R}^{D\times D}`$ : the orthogonal matrices s.t. $`U^\top U = V^\top V = I`$
        - The columns of $`V`$ (which are the right singular vectors of $`\mathbf{X}`$) are the eigenvectors of $`\mathbf{X^\top X}`$
        - Left $`d`$ columns of $`V`$ are the principal components.
      - Runtime Analysis)
        - $`O(nDd)`$ using the **truncated singular value decomposition**.
  - Then we have
    - $`\mathbf{z} = f(\mathbf{x}) = \mathbf{Qx} = \begin{bmatrix} \mathbf{x}_i^\top\mathbf{q}_1 & \mathbf{x}_i^\top\mathbf{q}_2 & \cdots & \mathbf{x}_i^\top\mathbf{q}_d \end{bmatrix}^\top \in \mathbb{R}^d`$
    - $`\hat{\mathbf{x}} = g(\mathbf{z}) = \mathbf{Q^\top z} \in \mathbb{R}^D`$.
- Derivation)
  - This will show why the **eigenvalue decomposition** works.
  - Consider the dimensionality reduction where...
    - the encoder $`f`$ goes   
      - In vector form   
        $`\begin{aligned}
          \mathbf{z}_i = \mathbf{Qx}_i 
          &= \begin{bmatrix} \mathbf{q}_1^\top \\ \mathbf{q}_2^\top \\ \vdots \\ \mathbf{q}_d^\top \end{bmatrix} \mathbf{x}_i = \begin{bmatrix} \mathbf{q}_1^\top\mathbf{x}_i \\ \mathbf{q}_2^\top\mathbf{x}_i \\ \vdots \\ \mathbf{q}_d^\top\mathbf{x}_i \end{bmatrix} = \begin{bmatrix} \mathbf{x}_i^\top\mathbf{q}_1 & \mathbf{x}_i^\top\mathbf{q}_2 & \cdots & \mathbf{x}_i^\top\mathbf{q}_d \end{bmatrix}^\top \in\mathbb{R}^d
        \end{aligned}`$
      - In matrix form   
        $`\begin{aligned}
          \mathbf{XQ}^\top 
          &= \begin{bmatrix} \mathbf{x}_1^\top \\ \mathbf{x}_2^\top \\ \vdots \\ \mathbf{x}_n^\top \end{bmatrix} \begin{bmatrix} \mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_d \end{bmatrix} \\
          &= \begin{bmatrix} \mathbf{x}_1^\top\mathbf{q}_1 & \mathbf{x}_1^\top\mathbf{q}_2 & \cdots & \mathbf{x}_1^\top\mathbf{q}_j &\cdots & \mathbf{x}_1^\top\mathbf{q}_d \\ \mathbf{x}_2^\top\mathbf{q}_1 & \mathbf{x}_2^\top\mathbf{q}_2 & \cdots & \mathbf{x}_2^\top\mathbf{q}_j  & \cdots & \mathbf{x}_2^\top\mathbf{q}_d \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ \mathbf{x}_n^\top\mathbf{q}_1 & \mathbf{x}_n^\top\mathbf{q}_2 & \cdots & \mathbf{x}_n^\top\mathbf{q}_j & \cdots & \mathbf{x}_n^\top\mathbf{q}_d \\ \end{bmatrix} = \begin{bmatrix} \mathbf{z}_1^\top \\ \mathbf{z}_2^\top \\ \vdots \\ \mathbf{z}_n^\top \\ \end{bmatrix} \in\mathbb{R}^{n\times d} \\
        \end{aligned}`$
    - the decoder $`g`$ goes   
      - In vector form   
        $`\begin{aligned}
          \hat{\mathbf{x}}_i &= \mathbf{Q}^\top (\mathbf{Q}\mathbf{x}_i) = \begin{bmatrix} \mathbf{q}_{1} & \mathbf{q}_{2} & \cdots & \mathbf{q}_{d} \end{bmatrix} \begin{bmatrix} \mathbf{q}_1^\top\mathbf{x}_i \\ \mathbf{q}_2^\top\mathbf{x}_i \\ \vdots \\ \mathbf{q}_d^\top\mathbf{x}_i \end{bmatrix} \\
          &= \begin{bmatrix} \mathbf{q}_{11} & \mathbf{q}_{21} & \cdots & \mathbf{q}_{d1} \\ \mathbf{q}_{12} & \mathbf{q}_{22} & \cdots & \mathbf{q}_{d2} \\ \vdots & \vdots & \ddots & \vdots \\ \mathbf{q}_{1D} & \mathbf{q}_{2D} & \cdots & \mathbf{q}_{dD} \\ \end{bmatrix} \begin{bmatrix} \mathbf{q}_1^\top\mathbf{x}_i \\ \mathbf{q}_2^\top\mathbf{x}_i \\ \vdots \\ \mathbf{q}_d^\top\mathbf{x}_i \end{bmatrix} \\
          &= \begin{bmatrix} (\mathbf{q}_1^\top\mathbf{x}_i)\mathbf{q}_{11} + (\mathbf{q}_2^\top\mathbf{x}_i)\mathbf{q}_{21} + \cdots + (\mathbf{q}_d^\top\mathbf{x}_i)\mathbf{q}_{d1} \\ (\mathbf{q}_1^\top\mathbf{x}_i)\mathbf{q}_{12} + (\mathbf{q}_2^\top\mathbf{x}_i)\mathbf{q}_{22} + \cdots + (\mathbf{q}_d^\top\mathbf{x}_i)\mathbf{q}_{d2} \\ \vdots \\ (\mathbf{q}_1^\top\mathbf{x}_i)\mathbf{q}_{1D} + (\mathbf{q}_2^\top\mathbf{x}_i)\mathbf{q}_{2D} + \cdots + (\mathbf{q}_d^\top\mathbf{x}_i)\mathbf{q}_{dD} \end{bmatrix} \\ 
        \end{aligned}`$   
        $`\begin{aligned}
          \quad &= \begin{bmatrix} (\mathbf{q}_1^\top\mathbf{x}_i)\mathbf{q}_1 + (\mathbf{q}_2^\top\mathbf{x}_i)\mathbf{q}_2 + \cdots + (\mathbf{q}_d^\top\mathbf{x}_i)\mathbf{q}_d \end{bmatrix} \\
          &= \sum_{j=1}^d \underbrace{(\mathbf{q}_j^\top\mathbf{x}_i)}_{\mathbb{R}} \underbrace{\mathbf{q}_j}_{\mathbb{R}^D} = \sum_{j=1}^d \underbrace{(\mathbf{x}_i^\top\mathbf{q}_j)}_{\mathbb{R}} \underbrace{\mathbf{q}_j}_{\mathbb{R}^D} \in\mathbb{R}^D
        \end{aligned}`$
      - In matrix form   
        $`\begin{aligned}
          \mathbf{XQ^\top}\mathbf{Q} 
          &= \begin{bmatrix} \mathbf{x}_1^\top\mathbf{q}_1 & \mathbf{x}_1^\top\mathbf{q}_2 & \cdots & \mathbf{x}_1^\top\mathbf{q}_j &\cdots & \mathbf{x}_1^\top\mathbf{q}_d \\ \mathbf{x}_2^\top\mathbf{q}_1 & \mathbf{x}_2^\top\mathbf{q}_2 & \cdots & \mathbf{x}_2^\top\mathbf{q}_j  & \cdots & \mathbf{x}_2^\top\mathbf{q}_d \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ \mathbf{x}_n^\top\mathbf{q}_1 & \mathbf{x}_n^\top\mathbf{q}_2 & \cdots & \mathbf{x}_n^\top\mathbf{q}_j & \cdots & \mathbf{x}_n^\top\mathbf{q}_d \\ \end{bmatrix} \begin{bmatrix} \mathbf{q}_1^\top \\ \mathbf{q}_2^\top \\ \vdots \\ \mathbf{q}_d^\top \end{bmatrix} \\
          &= \begin{bmatrix} \mathbf{x}_1^\top\mathbf{q}_1 & \mathbf{x}_1^\top\mathbf{q}_2 & \cdots & \mathbf{x}_1^\top\mathbf{q}_j &\cdots & \mathbf{x}_1^\top\mathbf{q}_d \\ \mathbf{x}_2^\top\mathbf{q}_1 & \mathbf{x}_2^\top\mathbf{q}_2 & \cdots & \mathbf{x}_2^\top\mathbf{q}_j  & \cdots & \mathbf{x}_2^\top\mathbf{q}_d \\ \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\ \mathbf{x}_n^\top\mathbf{q}_1 & \mathbf{x}_n^\top\mathbf{q}_2 & \cdots & \mathbf{x}_n^\top\mathbf{q}_j & \cdots & \mathbf{x}_n^\top\mathbf{q}_d \\ \end{bmatrix} \begin{bmatrix} \mathbf{q}_{11} & \mathbf{q}_{12} & \cdots & \mathbf{q}_{1D} \\ \mathbf{q}_{21} & \mathbf{q}_{22} & \cdots & \mathbf{q}_{2D} \\ \vdots & \vdots & \ddots & \vdots \\\mathbf{q}_{d1} & \mathbf{q}_{d2} & \cdots & \mathbf{q}_{dD} \\ \end{bmatrix} \\
        \end{aligned}`$
        $`\begin{aligned}
          \quad\quad
          &= \begin{bmatrix} \sum_{j=1}^d(\mathbf{x}_1^\top\mathbf{q}_j)\mathbf{q}_{j1} & \sum_{j=1}^d(\mathbf{x}_1^\top\mathbf{q}_j)\mathbf{q}_{j2} & \cdots & \sum_{j=1}^d(\mathbf{x}_1^\top\mathbf{q}_j)\mathbf{q}_{jD} \\ \sum_{j=1}^d(\mathbf{x}_2^\top\mathbf{q}_j)\mathbf{q}_{j1} & \sum_{j=1}^d(\mathbf{x}_2^\top\mathbf{q}_j)\mathbf{q}_{j2} & \cdots & \sum_{j=1}^d(\mathbf{x}_2^\top\mathbf{q}_j)\mathbf{q}_{jD} \\ \vdots & \vdots & \ddots & \vdots \\ \sum_{j=1}^d(\mathbf{x}_n^\top\mathbf{q}_j)\mathbf{q}_{j1} & \sum_{j=1}^d(\mathbf{x}_n^\top\mathbf{q}_j)\mathbf{q}_{j2} & \cdots & \sum_{j=1}^d(\mathbf{x}_n^\top\mathbf{q}_j)\mathbf{q}_{jD} \end{bmatrix} \\
          &= \begin{bmatrix} \sum_{j=1}^d(\mathbf{x}_1^\top\mathbf{q}_j) \left[ \mathbf{q}_{j1} \quad \mathbf{q}_{j2} \quad \cdots \quad \mathbf{q}_{jD} \right] \\ \sum_{j=1}^d(\mathbf{x}_2^\top\mathbf{q}_j) \left[ \mathbf{q}_{j1} \quad \mathbf{q}_{j2} \quad \cdots \quad \mathbf{q}_{jD} \right] \\ \vdots \\ \sum_{j=1}^d(\mathbf{x}_n^\top\mathbf{q}_j) \left[ \mathbf{q}_{j1} \quad \mathbf{q}_{j2} \quad \cdots \quad \mathbf{q}_{jD} \right] \end{bmatrix} \\
          &= \begin{bmatrix} \sum_{j=1}^d (\mathbf{x}_1^\top\mathbf{q}_j)\mathbf{q}_j^\top \\ 
          \sum_{j=1}^d (\mathbf{x}_2^\top\mathbf{q}_j)\mathbf{q}_j^\top \\ \vdots \\  
          \sum_{j=1}^d (\mathbf{x}_n^\top\mathbf{q}_j)\mathbf{q}_j^\top \end{bmatrix} \in\mathbb{R}^{n\times D}
        \end{aligned}`$
  - Also, consider the projection of $`\mathbf{x}_i`$ onto $`\mathbf{q}_j`$ is given by
    - $`(\mathbf{x}_i^\top \mathbf{q}_j) \mathbf{q}_j`$
      - cf.) $`\mathbf{x}_i^\top \mathbf{q}_j`$ can be interpreted as the shadow that $`\mathbf{x}_i`$ casts on $`\mathbf{q}_j`$ assuming $`\mathbf{q}_j`$ is a unit vector.
    - This is identical to the $`j`$-th element of the decoder $`\hat{\mathbf{x}}_i`$ where   
      $`\begin{aligned}
        \hat{\mathbf{x}}_i &= \sum_{j=1}^d (\mathbf{x}_i^\top\mathbf{q}_j)\mathbf{q}_j \\ &= (\mathbf{x}_i^\top\mathbf{q}_1)\mathbf{q}_1 + \cdots + (\mathbf{x}_i^\top\mathbf{q}_j)\mathbf{q}_j + \cdots + (\mathbf{x}_i^\top\mathbf{q}_d)\mathbf{q}_d
      \end{aligned}`$
  - Then what is the best $`\mathbf{q}_j`$?
    - One idea is to choose the vectors $`\mathbf{q}_j`$s that maximize the variance of values $`\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_n`$ where
      - $`\mathbf{z}_i = \begin{bmatrix} \mathbf{q}_1^\top\mathbf{x}_i \\ \mathbf{q}_2^\top\mathbf{x}_i \\ \vdots \\ \mathbf{q}_d^\top\mathbf{x}_i \end{bmatrix}`$
        - Why?)
          - Suppose not, i.e., $`\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_n`$ have very small variance.
          - This means that all of the original data $`\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n`$ got mapped to nearly the same value.
          - In contrast, having high variance means that it's relatively easy to distinguish the data points $`\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n`$ from their projection onto $`\mathbf{Q}`$.
  - How to get the maximum variance of $`\mathbf{z}_i`$
    - The sample variance for $`\mathbf{z}_i`$ is given by   
      $`\begin{aligned}
        \frac{1}{n-1}\sum_{i=1}^n \text{Var}(\mathbf{z}_i) &= \frac{1}{n-1}\sum_{i=1}^n \text{Var}(\mathbf{Qx}_i) \\
        &= \frac{1}{n-1}\Vert\mathbf{XQ^\top}\Vert_F^2 : \text{the Frobenius Norm} \quad (\because \mathbb{E}[\mathbf{x}_i] = 0, \forall i) \\
      \end{aligned}`$  
    - The variance related to the $`j`$-th row of $`\mathbf{Q}`$ can be written as   
      $`\begin{aligned}
        \frac{1}{n-1} \mathbf{q}_j^\top \mathbf{X}^\top \mathbf{X} \mathbf{q}_j  = \frac{1}{n-1} \mathbf{q}_j^\top S \mathbf{q}_j  \quad (S \text{ is the sample covariance matrix of } \mathbf{X})
      \end{aligned}`$
    - Thus, the optimization problem goes 
      - $`\displaystyle \arg\max_{\mathbf{q}_j} \mathbf{q}_j^\top S \mathbf{q}_j`$ where $`\Vert \mathbf{q}_j \Vert_2^2 = 1`$
        - Recall that $`\mathbf{q}_j`$ was a unit vector.
    - We can change this maximization problem into minimization problem of   
      - $`\displaystyle \arg\min_{\mathbf{q}_j} -\mathbf{q}_j^\top S \mathbf{q}_j`$ where $`\mathbf{q}_j^\top \mathbf{q}_j = 1`$
    - Setting up the Lagrangian we can solve the optimization problem as   
      $`\mathcal{L} = -\mathbf{q}_j^\top S \mathbf{q}_j + \lambda(\mathbf{q}_j^\top \mathbf{q}_j - 1)`$
    - Then the solution goes   
      $`\begin{aligned}
        \frac{\partial \mathcal{L}}{\partial \mathbf{q}_j} = 0 
        &\Leftrightarrow -2S\mathbf{q}_j + 2\lambda \mathbf{q}_j = 0 \\
        &\Rightarrow S\mathbf{q}_j = \lambda \mathbf{q}_j \\
      \end{aligned}`$
      - Thus, the optimal $`\mathbf{q}_j`$ is the eigenvector of $`S`$
      - Also, the maximum variance is the largest eigenvalue $`\lambda`$.
        - Why?)
          - Recall that the sample variance of $`\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_n`$ goes   
            $`\begin{aligned}
              \mathbf{q}_j^\top S \mathbf{q}_j &= \mathbf{q}_j^\top(\lambda \mathbf{q}_j) \\
              &= \lambda \Vert \mathbf{q}_j \Vert_2^2 \\
              &= \lambda & (\because \Vert \mathbf{q}_j \Vert_2^2=1)
            \end{aligned}`$
          - Thus, the maximum variance is the maximum eigenvalue.
          - And, $`\mathbf{q}_j`$ is the eigenvector with the maximum eigenvalue.
      - $`\mathbf{q}_j`$ is called the **principal component** (PC)



<br>

### View 2) PCA as Minimizing Reconstruction Error
- Settings)
  - $`\mathbf{q}_1, \cdots, \mathbf{q}_d`$ : the principal components of $`X`$
  - $`\displaystyle \hat{\mathbf{x}}_i = \mathbf{Q^\top Q}\mathbf{x}_i = \sum_{j=1}^d(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j`$ : the reconstruction
- Loss Derivation)
  - Recall that the loss of the dimensionality reduction was given by   
    $`\displaystyle\Vert \hat{\mathbf{x}}_i = \mathbf{x}_i \Vert_2^2 = \left\Vert \sum_{j=1}^d(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j - \mathbf{x}_i \right\Vert_2^2 \quad\cdots (1)`$
  - This loss (error) takes place because $`\mathbf{x}_i\in\mathbb{R}^D`$ but we reduced the dimension using $`\mathbf{Q}\in\mathbb{R}^{d\times D}`$.
    - $`\displaystyle \mathbf{x}_i \approx \sum_{j=1}^d(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j`$
  - Recall that during the eigenvalue decomposition in [view 1](#view-1-pca-as-finding-maximum-variance-detection), we used only $`d`$ number of eigenvectors $`\mathbf{q}_1, \cdots, \mathbf{q}_d`$ out of $`D`$ number of eigenvectors.
    - Adding $`\mathbf{q}_{d+1}, \cdots, \mathbf{q}_D`$ to $`\mathbf{q}_1, \cdots, \mathbf{q}_d`$ , we can get $`\mathbf{q}_1, \cdots, \mathbf{q}_D`$.
    - Since $`\mathbf{q}_1, \cdots, \mathbf{q}_D`$ are linearly independent, they form a basis of $`\mathbb{R}^D`$.
    - Thus, $`\displaystyle \mathbf{x}_i = \sum_{j=1}^D(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j \quad\cdots (2)`$
  - Plugging (2) into (1), we can get   
    $`\begin{aligned}
      \Vert \hat{\mathbf{x}}_i = \mathbf{x}_i \Vert_2^2 
      &= \left\Vert \sum_{j=1}^d(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j - \mathbf{x}_i \right\Vert_2^2 \\
      &= \left\Vert \sum_{j=1}^d(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j - \sum_{j=1}^D(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j \right\Vert_2^2 \\
      &= \left\Vert - \sum_{j={d+1}}^D(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j \right\Vert_2^2 = \left\Vert \sum_{j={d+1}}^D(\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j \right\Vert_2^2 \\
      &= \sum_{j={d+1}}^D ((\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j)^\top ((\mathbf{x}_i^\top \mathbf{q}_j)\mathbf{q}_j) \\
      &= \sum_{j={d+1}}^D (\mathbf{x}_i^\top \mathbf{q}_j)^2 \mathbf{q}_j^\top \mathbf{q}_j & (\because \mathbf{x}_i^\top \mathbf{q}_j \in \mathbb{R}) \\
      &= \sum_{j={d+1}}^D (\mathbf{x}_i^\top \mathbf{q}_j)^2 & (\because \Vert \mathbf{q}_j \Vert_2^2 = 1) \\
    \end{aligned}`$
  - Then the empirical risk of the model will be   
    $`\begin{aligned}
      \sum_{i=1}^n \Vert \hat{\mathbf{x}}_i = \mathbf{x}_i \Vert_2^2
      &= \sum_{i=1}^n \sum_{j={d+1}}^D (\mathbf{x}_i^\top \mathbf{q}_j)^2 \\
      &= \sum_{j={d+1}}^D \mathbf{q}_j^\top \mathbf{X^\top X} \mathbf{q}_j \\
      &= \sum_{j={d+1}}^D \mathbf{q}_j^\top (n-1)S \mathbf{q}_j & \left(\because S = \frac{1}{n-1} \mathbf{X^\top X}\right) \\
      &= (n-1)\sum_{j={d+1}}^D \lambda_j & (\because S\mathbf{q}_j = \lambda \mathbf{q}_j) \\
    \end{aligned}`$
    - where $`\lambda_j`$ is the eigenvalue of $`\mathbf{q}_j`$.
- Optimization)
  - We should minimize the loss of this model.
  - Then the minimization problem can be set up as   
    $`\displaystyle \arg\min_{\mathbf{q}_j} \sum_{j={d+1}}^D \mathbf{q}_j^\top \mathbf{X^\top X} \mathbf{q}_j`$
- Interpretation)
  - Recall that $`\mathbf{q}_j^\top \mathbf{X^\top X} \mathbf{q}_j`$ was the sample variance along the direction $`\mathbf{q}_j`$.
  - And, $`\{\mathbf{q}_j\}_{j=d+1, \cdots, D}`$ were the eigenvectors with the smallest eigenvalues.
  - Considering that the eigenvalues $`\lambda_j`$s were the variances, these eigenvalues were the ones that had the small variances.

<br>

### Concept) Fractional Reconstruction Accuracy
- Def.)
  - $`\displaystyle\frac{\sum_{j=1}^d \lambda_j}{\sum_{j=1}^D \lambda_j}`$
- Meaning)
  - Fraction of variance in data explained by keeping $`d`$ dimensions out of $`D`$ dimensions.
- Prop.)
  - Bigger the better
    - e.g.)
      - If the value is 0.9, it means 90% are explained by the dimensionality reduction.
      - The error $`\Vert \hat{x} - x \Vert_2^2`$ is 10%.

<br><br>

[Back to Main](../main.md)