[Back to Main](../main.md)

# 3. Linear Models and Perceptron

# 3.1 Linear Predictors
- Main Concept)
  - Hypothesis class of linear predictors
    - Advantages)
      - Intuitive to understand
      - Computationally efficient to train and evaluate

<br>

# 3.2 Binary Classification with Linear Predictors
### Settings)
- $`\mathcal{X} = \mathbb{R}^d`$ : the input space
- $`\mathcal{Y} = \{-1, 1\}`$ : the label space
- $`\mathcal{H} := \{x \mapsto \text{sign}(w^\top x + b) | w\in\mathbb{R}^d, b\in\mathbb{R}\}`$ : the hypothesis class
  - where 
    - $`w`$ : the weight vector
    - $`b`$ : the bias
    - $`\text{sign}(a) = \begin{cases} 1 & \text{if } a\ge 0 \\ -1 & \text{otherwise.} \end{cases}`$
- $`\ell(\hat{y}, y) = \begin{cases} 0 & \text{if } \hat{y}=y \\ 1 & \text{otherwise.} \end{cases}`$ : the loss function
- $`\gamma = \min_i{||w^\top x_i||}`$ : the distance from $`w`$ and to the closest example

#### Remark 1) Drop the bias term.
- Why doing this?)
  - For the simplicity.
- Justification)
  - Consider the halfspaces with bias.
    - $`w^\top x + b \gt 0`$
    - $`w^\top x + b \lt 0`$
  - These halfspaces can be transformed using the following mapping.
    - $`x\mapsto \begin{bmatrix} x\\1 \end{bmatrix}, w\mapsto \begin{bmatrix} w\\ b \end{bmatrix}`$
      - Then
        - $`w^\top x \gt 0`$
        - $`w^\top x \lt 0`$
  - This increases the dimension of the weight vector by 1 and this extra dimension absorbs the bias.

#### Remark 2) The weight vector has norm 1.
- i.e.)
  - $`||w||=1`$
- Why doing this?)
  - For the simplicity
- Justification)
  - Recall that our classifier's returns the sign.
    - i.e.) $`\text{sign}(w^\top x)`$
  - Even if we multiply $`\alpha = \frac{1}{||w||}`$, we can get the equivalent value.
    - i.e.) $`\text{sign}(\alpha w^\top x) = \text{sign}(w^\top x)`$
      - Why?)
        - Scalar multiplication does not affect the shape of the hyperplane $`w^\top x`$

#### Remark 3) The data points have norm at most 1.
- i.e.)
  - $`||x_i|| \le 1, \forall i`$
- Why doing this?)
  - For the simplicity
- Justification)
  - Putting $`\beta = \max_i ||x_i||`$ and dividing $`x_i`$ with $`\beta`$ we can get the desired result $`\frac{1}{\beta}x_i \le 1, \forall i`$.
  - Simultaneously multiplying and dividing with $`\beta`$ does not affect the classifier.
    - i.e.) $`\text{sign}(w^\top x) = \text{sign}\left((\beta w)^\top \left(\frac{1}{\beta}x\right)\right)`$
  - Considering [Remark 2](#remark-2-the-data-points-have-norm-at-most-1), we can modify as follows:
    - $`\displaystyle\text{sign}\left((\beta w)^\top \left(\frac{1}{\beta ||w||}x\right)\right)`$

#### Remark 4) Separability
- We assume that $`\exists w_* \in \mathbb{R}^d \text{ s.t. } \underbrace{y_i}_{\text{actual label value}} = \underbrace{\text{sign}(w_*^\top x_i)}_{\text{prediction made by our classifier}}`$.
  - i.e.)
    - There is exists $`w_*`$ that perfectly separates examples into two classes.

<br>

#### Interpretation) The Sign of the Inner Product of Weight and Example
- Our classifier returns $`\text{sign}(w^\top x)`$.
  - Then, what does this mean?
- Recall that $`w^\top x = \langle w, x \rangle = ||w||\cdot||x||\cos\theta`$
  - where $`\theta`$ is the angle between $`w`$ and $`x`$.
- Here, $`||w||, ||x|| \gt 0`$.
- Thus, $`\cos(\theta)`$ determines the **sign**!
  - Geometrical meaning
    |$`\theta`$|$`\cos(\theta)`$|$`w`$ and $`x`$|Halfspace|$`\text{sign}(w^\top x)`$|
    |:-|:-:|:-:|:-:|:-:|
    |$`\theta=0`$|$`\cos(\theta)=1`$|Aligned in the same direction|Same halfspace|+1|
    |$`\theta\in(0, \frac{\pi}{2})`$|$`\cos(\theta)\gt0`$||Same halfspace|+1|
    |$`\theta=\frac{\pi}{2}`$|$`\cos(\theta)=0`$|Perpendicular|$`x`$ is on the line!|0|
    |$`\theta\in(\frac{\pi}{2}, \frac{3\pi}{2})`$|$`\cos(\theta)\lt0`$||Different halfspace|-1|
    |$`\theta=\frac{3\pi}{2}`$|$`\cos(\theta)=0`$|Perpendicular|$`x`$ is on the line!|0|
    |$`\theta\in(\frac{3\pi}{2},2\pi)`$|$`\cos(\theta)\gt0`$||Same halfspace|+1|
- Recall that 
  - $`w`$ determines the line(plane) that separates the space.
  - $`w^\top x_i`$ classifies each $`x_i`$ by the sign.
    - If $`w`$ and $`x_i`$ are in the same halfspace, $`w^\top x_i = +1`$
    - If $`w`$ and $`x_i`$ are in the different halfspace, $`w^\top x_i = -1`$
  - For each $`x_i`$, the label is given as $`y_i\in \{+1,-1\}`$.
- Therefore, we are looking for a classifier $`w`$ that 
  - geometrically separate $`x_i`$
  - logically classify $`x_i`$
    - corresponding to the label $`y_i`$


<br>

# 3. Perceptron
## 3.1 Algorithm)
- Goal)
  - Solve the ERM problem in the [linearly separable case](#remark-4-separability).
- Algorithm)
  - Initialize $`w_1 = 0 \in \mathbb{R}^d`$
  - `for` $`t=1,2,\cdots`$ `do`
    - `if` $`\exists i\in \{1, \cdots, m\}`$ s.t. $`y_i \ne \text{sign}(w_t^\top x_i)`$ `then` update $`w_{t+1} = w_t + y_ix_i`$
    - `else` output $`w_t`$
  - `end for`





[Back to Main](../main.md)