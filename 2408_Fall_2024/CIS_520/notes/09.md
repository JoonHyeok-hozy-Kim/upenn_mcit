[Back to Main](../main.md)

# 9. Kernels Part 1
### Concept) Feature Map
- Def.)
  - $`\phi(\mathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}^D`$ where $`D\gt d`$
- Props.)
  - In the previous linear model cases, we saw that models with many features better explain the model.
    - We will use this property to learning.
      - i.e.) Increase the number of features using the feature map.
- Application)
  - Suppose
    - $`\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \in\mathbb{R}^2`$
  - We can set a linear model as follows:
    - $`h(\mathbf{x}) = \mathbf{w^\top x}`$
  - However, this cannot properly classify the following data.
    ||
    |:-|
    |<img src="../images/09/001.png" width="400px">|
  - If we use $`\phi(\mathbf{x}) = \begin{bmatrix} x_1 \\ x_2 \\ x_1^2+x_2^2 \end{bmatrix}`$ and change our model into
    - $`h(\phi(\mathbf{x})) = \mathbf{w}^\top \phi(\mathbf{x}), \mathbf{w}\in\mathbb{R}^3`$
  - $`\mathbf{w}^* = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}`$ can perfectly classify the model.
  - Here, $`h(\phi(\mathbf{x}))`$ is a linear model, but is a nonlinear model for $`\mathbf{x}`$.

<br>

#### Concept) Degree p Polynomial Feature Map
- Prop.)
  - For $`\mathbf{x}\in\mathbb{R}^d`$, the degree $`p`$ polynomial feature map $`\phi(\mathbf{x})`$ has $`\begin{pmatrix} p+d \\ d \end{pmatrix}`$ elements.
    - Why?)
      - Consider the following counting problem.
      - Let $`x_1^{a_1} x_2^{a_2} \cdots x_d^{a_d}`$ be an element in $`\phi(\mathbf{x})`$.
      - Then we can count the possible cases with the stars and bars problem:   
        - $`a_1 + a_2 + \cdots + a_d \le p`$
      - Using a slack variable $`0 \le A \le p`$, we can rewrite the problem as  
        - $`\underbrace{a_1 + a_2 + \cdots + a_d + A}_{d+1\text{ features}} = p`$
      - Thus, the number of cases can be counted as
        - $`\begin{pmatrix} p+(d+1)-1 \\ (d+1)-1 \end{pmatrix} = \begin{pmatrix} p+d \\ d \end{pmatrix}`$
- e.g.) Degree 2 Polynomial Feature Map for 2 features.
  - Let $`\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}`$. Then $`\phi(\mathbf{x}) = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ x_1x_2 \\ x_1^2 \\ x_2^2  \end{bmatrix}`$. 
  - Thus, $`\vert \phi(\mathbf{x}) \vert = 6 = \begin{pmatrix} 2+2 \\ 2 \end{pmatrix}`$
- Application)
  - Using the Degree p Polynomial Feature Map has pros and cons.
    - Pros
      - $`\begin{pmatrix} p+d \\ d \end{pmatrix}`$ number of features are enough to perfectly train for the data.
    - Cons
      - Since there are too many features, its even hard to write them on a computer.
        - Sol.) Use [kernel trick](#concept-kernel-trick)!

<br><br>

### Theorem) Representor Theorem
- Theorem)
  - Let $`\mathbf{x_1, \cdots x}_n`$ be a set of training examples equipped with labels $`y_1,\cdots y_n`$.
  - Then, any linear model $`\mathbf{w}^*`$ trained on this data can be written as a linear combination of the training examples:
    - $`\displaystyle \mathbf{w}^* = \sum_i \alpha_i \mathbf{x}_i`$
  - And, this is true regardless of the space that learning is done in.
    - e.g.) 
      - A linear model trained in the feature map space $`\phi(\mathbf{x})`$ can still be represented as:
        - $`\displaystyle \mathbf{w}^* = \sum_i \alpha_i \phi(\mathbf{x}_i)`$
- Informal Pf.)
  - Let $`\mathbf{w}`$ is the optimal w.r.t. the [ridge regression](06.md#concept-ridge-regression) with [L2 regularizer](06.md#concept-l2-regularization):
    - $`\displaystyle \min_{\mathbf{w}} \sum_{i=1}^n (\mathbf{w}^\top \mathbf{x}_i - y_i)^2 + \lambda\Vert \mathbf{w}\Vert_2^2 = \Vert\mathbf{Xw-y}\Vert_2^2 + \lambda \mathbf{w}^\top \mathbf{w}`$
  - Suppose not, i.e., $`\mathbf{w}`$ cannot be written as $`\displaystyle \mathbf{w}^* = \sum_i \alpha_i \mathbf{x}_i`$.
  - Then, $`\exists \mathbf{w}', \mathbf{v}`$ s.t.
    - $`\mathbf{w} = \mathbf{w}' + \mathbf{v}`$
    - $`\displaystyle \mathbf{w}' = \sum_i \alpha_i \mathbf{x}_i`$
    - $`\displaystyle \sum_i \alpha_i \mathbf{v}_i = 0`$
      - i.e.) Additional component $`\mathbf{v}`$ is orthogonal to each $`\mathbf{x}_i`$.
  - In that case   
    $`\begin{aligned}
      \mathbf{w}^\top \mathbf{x} &= (\mathbf{w}' + \mathbf{v})^\top \mathbf{x} \\
      &= \mathbf{w}'^\top \mathbf{x} + \mathbf{v}^\top \mathbf{x} \\
      &= \mathbf{w}'^\top \mathbf{x} & \because \mathbf{v}^\top \mathbf{x}= 0
    \end{aligned}`$
  - Thus, $`\Vert\mathbf{Xw-y}\Vert_2^2 = \Vert\mathbf{Xu-y}\Vert_2^2`$
  - Also, $`\Vert \mathbf{w}\Vert^2 = \Vert \mathbf{w}'\Vert^2 + \Vert \mathbf{v}\Vert^2 \Rightarrow \Vert \mathbf{w}'\Vert^2 \lt \Vert \mathbf{w}\Vert^2 \cdots \otimes`$
    - Why?)
      - Consider that our problem contained [L2 regularizer](06.md#concept-l2-regularization).
      - If $`\mathbf{w}`$ is optimal, $`\Vert \mathbf{w}\Vert^2 \le \Vert \mathbf{w}'\Vert, \forall \mathbf{w}'`$
   
<br>

### Concept) Kernel Trick
- Desc.)
  - Recall that even writing feature map $`\phi(\mathbf{x})`$ took a lot of resource.
  - Using the inner product between two feature map, we can drastically reduce the computational complexity.
    - In general, for a degree $`p`$ polynomial feature expansion, up to constants in some features we can write the inner product of the implied feature expansion as:
      - $`k_{\text{poly}}(\mathbf{x,z}) = (1+\mathbf{x^\top z})^p`$
    - e.g.) $`p=2`$
      - Let 
        - $`\mathbf{x,z}\in\mathbb{R}^d`$ : the features
        - $`\phi(\mathbf{x}),\phi(\mathbf{z})`$ : the feature maps
          - $`\phi(\mathbf{x}) = \begin{bmatrix} 1 & x_1 & \cdots & x_d & x_1x_2 & \cdots & x_{d-1}x_d & x_1^2 & \cdots & x_d^2 \end{bmatrix}^\top`$
          - $`\phi(\mathbf{z}) = \begin{bmatrix} 1 & z_1 & \cdots & z_d & z_1z_2 & \cdots & z_{d-1}z_d & z_1^2 & \cdots & z_d^2 \end{bmatrix}^\top`$
      - Then   
        $`\begin{aligned}
          \phi(\mathbf{x})^\top\phi(\mathbf{z}) 
          &= 1 + (x_1z_1 + \cdots + x_dz_d ) + (x_1x_2z_1z_2 + \cdots + x_{d-1}x_dz_{d-1}z_d) + (x_1^2z_1^2 + \cdots + x_d^2z_d^2) \\
          &= 1 + \sum_i x_iz_i + \sum_i \sum_j x_i x_j z_i z_j \\
          &= 1 + \sum_i x_iz_i + \left(\sum_i x_i z_i\right) \left(\sum_j x_j z_j\right) \\
          &= 1 + \mathbf{x}^\top\mathbf{z} + (\mathbf{x}^\top\mathbf{z})^2
        \end{aligned}`$
    - Then, just by getting $`\underbrace{\mathbf{x^\top z}}_{O(d)}`$, we can compute $`\underbrace{\phi(\mathbf{x})^\top\phi(\mathbf{z})}_{O\left(\begin{pmatrix} p+d \\ d \end{pmatrix}\right)}`$.
- Derivation)
  - Now, consider the model $`\displaystyle \mathbf{w^\top z}`$.
  - By the [representor theorem](#theorem-representor-theorem) we have
    - $`\displaystyle \mathbf{w^\top z} = \sum_i \alpha_i \mathbf{x}_i^\top \mathbf{z}`$.
  - Again, by the [representor theorem](#theorem-representor-theorem) we have   
    - $`\displaystyle \mathbf{w^\top z} \rightarrow \sum_i \alpha_i \phi(\mathbf{x}_i)^\top \phi(\mathbf{z}) = \sum_i \alpha_i k(\mathbf{x}_i, \mathbf{z})`$
- e.g.)
  - Recall the [ridge regression](06.md#concept-ridge-regression) with [L2 regularizer](06.md#concept-l2-regularization):    
    $`\begin{aligned}
      \min_{\mathbf{w}} \sum_{i=1}^n (\mathbf{w}^\top \mathbf{x}_i - y_i)^2 + \lambda\Vert \mathbf{w}\Vert_2^2 
      &= \min_{\mathbf{\alpha}} \sum_{i=1}^n \left(\left(\sum_j \alpha_j\mathbf{x}_j \right)^\top \mathbf{x}_i - y_i\right)^2 + \lambda\left(\sum_i \alpha_i\mathbf{x}_i \right)^\top\left(\sum_j \alpha_j\mathbf{x}_j \right) \\
      &= \min_{\mathbf{\alpha}} \sum_{i=1}^n \left(\sum_j \alpha_j\mathbf{x}_j^\top\mathbf{x}_i - y_i\right)^2 + \lambda\sum_i\sum_j  \alpha_i\alpha_j\mathbf{x}_i^\top \mathbf{x}_j \\
      &= \min_{\mathbf{\alpha}} \sum_{i=1}^n \left(\sum_j \alpha_j\phi(\mathbf{x}_j)^\top\phi(\mathbf{x}_i) - y_i\right)^2 + \lambda\sum_i\sum_j  \alpha_i\alpha_j\phi(\mathbf{x}_i)^\top\phi(\mathbf{x}_j) \\
      &= \min_{\mathbf{\alpha}} \sum_{i=1}^n \left(\sum_j \alpha_j k(\mathbf{x}_j, \mathbf{x}_i) - y_i\right)^2 + \lambda\sum_i\sum_j  \alpha_i\alpha_j k(\mathbf{x}_i, \mathbf{x}_j) \\
    \end{aligned}`$
  - Matrix Version)
    - We can rewrite $`\displaystyle \mathbf{w} = \sum_i \alpha_i\mathbf{x}_i = \mathbf{X}^\top\alpha`$
    - Then the optimization problem goes:    
      $`\begin{aligned}
        \min_\alpha\Vert\mathbf{Xw-y}\Vert_2^2 + \lambda\mathbf{w^\top w}
        &= \min_\alpha\Vert\mathbf{X}(\mathbf{X}^\top\alpha)-\mathbf{y}\Vert_2^2 + \lambda(\mathbf{X}^\top\alpha)^\top (\mathbf{X}^\top\alpha) \\
        &= \min_\alpha\Vert\mathbf{X}\mathbf{X}^\top\alpha-\mathbf{y}\Vert_2^2 + \lambda \alpha^\top\mathbf{X}\mathbf{X}^\top\alpha \\
      \end{aligned}`$
    - Consider that $`[\mathbf{X}\mathbf{X}^\top]_{ij} = \mathbf{x_i}^\top\mathbf{x_j}`$.
    - This time, using the kernel matrix $`\mathbf{K}`$ where $`\mathbf{K}_{ij} = k(\mathbf{x_i}, \mathbf{x_j}) = \phi(\mathbf{x}_j)^\top\phi(\mathbf{x}_i)`$, the optimization problem can be rewritten as
      - $`\min_\alpha\Vert\mathbf{X}\mathbf{X}^\top\alpha-\mathbf{y}\Vert_2^2 + \lambda \alpha^\top\mathbf{X}\mathbf{X}^\top\alpha = \min_\alpha\Vert\mathbf{K}\alpha-\mathbf{y}\Vert_2^2 + \lambda \alpha^\top\mathbf{K}\alpha`$
    - Then, the solution goes   
      $`\begin{aligned}
        & \frac{\partial}{\partial \alpha} \left(\Vert\mathbf{K}\alpha-\mathbf{y}\Vert_2^2 + \lambda \alpha^\top\mathbf{K}\alpha\right) = 0 \\
        \Rightarrow&  \frac{\partial}{\partial \alpha} \left((\mathbf{K}\alpha-\mathbf{y})^\top(\mathbf{K}\alpha-\mathbf{y}) + \lambda \alpha^\top\mathbf{K}\alpha\right) = 0 \\
        \Rightarrow&  \frac{\partial}{\partial \alpha} \left(\alpha^\top\mathbf{K}^\top\mathbf{K}\alpha-\mathbf{y}^\top\mathbf{K}\alpha - \alpha^\top\mathbf{K}^\top\mathbf{y} + \mathbf{y}^\top\mathbf{y} + \lambda \alpha^\top\mathbf{K}\alpha\right) = 0 \\
        \Rightarrow&  \frac{\partial}{\partial \alpha} \left(\alpha^\top\mathbf{K}^2\alpha-2\mathbf{y}^\top\mathbf{K}\alpha + \mathbf{y}^\top\mathbf{y} + \lambda \alpha^\top\mathbf{K}\alpha\right) = 0 & \because \mathbf{K}\text{ is symmetric.} \\
        \Rightarrow&  2\mathbf{K}^2\alpha-2\mathbf{y}^\top\mathbf{K} + 2\lambda \mathbf{K}\alpha = 0 \\
        \Rightarrow&  (\mathbf{K}^2 + \lambda \mathbf{K})\alpha = \mathbf{y}^\top\mathbf{K} \\
        \Rightarrow&  \mathbf{K}(\mathbf{K} + \lambda I)\alpha = \mathbf{K}\mathbf{y} \\
        \Rightarrow&  \alpha = (\mathbf{K} + \lambda I)^{-1}\mathbf{y} \\
      \end{aligned}`$




<br><br>

[Back to Main](../main.md)