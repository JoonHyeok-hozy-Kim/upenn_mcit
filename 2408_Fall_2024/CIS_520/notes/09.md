[Back to Main](../main.md)

# 9. Kernels Part 1
### Concept) Feature Map
- Def.)
  - $`\phi(\mathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}^D`$ where $`D\gt d`$
- Props.)
  - In the previous linear model cases, we saw that models with many features better explain the model.
    - We will use this property to learning.
      - i.e.) Increase the number of features using the feature map.
- Application)
  - Suppose
    - $`\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \in\mathbb{R}^2`$
  - We can set a linear model as follows:
    - $`h(\mathbf{x}) = \mathbf{w^\top x}`$
  - However, this cannot properly classify the following data.
    ||
    |:-|
    |<img src="../images/09/001.png" width="400px">|
  - If we use $`\phi(\mathbf{x}) = \begin{bmatrix} x_1 \\ x_2 \\ x_1^2+x_2^2 \end{bmatrix}`$ and change our model into
    - $`h(\phi(\mathbf{x})) = \mathbf{w}^\top \phi(\mathbf{x}), \mathbf{w}\in\mathbb{R}^3`$
  - $`\mathbf{w}^* = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}`$ can perfectly classify the model.
  - Here, $`h(\phi(\mathbf{x}))`$ is a linear model, but is a nonlinear model for $`\mathbf{x}`$.

#### Concept) Degree p Polynomial Feature Map
- Prop.)
  - For $`\mathbf{x}\in\mathbb{R}^d`$, the degree $`p`$ polynomial feature map $`\phi(\mathbf{x})`$ has $`\begin{pmatrix} p+d \\ d \end{pmatrix}`$ elements.
    - Why?)
      - Consider the following counting problem.
      - Let $`x_1^{a_1} x_2^{a_2} \cdots x_d^{a_d}`$ be an element in $`\phi(\mathbf{x})`$.
      - Then we can count the possible cases with the stars and bars problem:   
        - $`a_1 + a_2 + \cdots + a_d \le p`$
      - Using a slack variable $`0 \le A \le p`$, we can rewrite the problem as  
        - $`\underbrace{a_1 + a_2 + \cdots + a_d + A}_{d+1\text{ features}} = p`$
      - Thus, the number of cases can be counted as
        - $`\begin{pmatrix} p+(d+1)-1 \\ (d+1)-1 \end{pmatrix} = \begin{pmatrix} p+d \\ d \end{pmatrix}`$
- e.g.) Degree 2 Polynomial Feature Map for 2 features.
  - Let $`\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}`$. Then $`\phi(\mathbf{x}) = \begin{bmatrix} 1 \\ x_1 \\ x_2 \\ x_1x_2 \\ x_1^2 \\ x_2^2  \end{bmatrix}`$. 
  - Thus, $`\vert \phi(\mathbf{x}) \vert = 6 = \begin{pmatrix} 2+2 \\ 2 \end{pmatrix}`$
- Application)
  - Using the Degree p Polynomial Feature Map has pros and cons.
    - Pros
      - $`\begin{pmatrix} p+d \\ d \end{pmatrix}`$ number of features are enough to perfectly train for the data.
    - Cons
      - Since there are too many features, its even hard to write them on a computer.
        - Sol.) Use [kernel trick]()!

<br><br>

### Concept) Kernel Trick
- 







<br><br>

[Back to Main](../main.md)