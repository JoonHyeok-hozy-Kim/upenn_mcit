[Back to Main](../main.md)

# 5. Gradient Descent on Smooth and Strongly Convex Functions

### Concept) Strongly Convex Function
- Def.)
  - A function $`F:\mathbb{R}^d \rightarrow \mathbb{R}`$ is $`\mu`$-strongly convex
    - if $`\forall w,w'\in\mathbb{R}^d`$
      - $`\displaystyle F(w') \ge F(w) + \nabla F(w)^\top (w'-w) + \frac{\mu}{2}\Vert w' - w \Vert_2^2`$

<br>

### Theorem 8)
- Theorem)
  - Suppose we run [gradient descent](04.md#4-gradient-descent) on [L-smooth](04.md#concept-smoothness) and $`\mu`$-[strongly convex](#concept-strongly-convex-function) function $`F`$ with fixed constant learning rate $`\eta_t = \frac{1}{L}, \forall t\in [T]`$.
  - Then $`\forall\tau\in[T]`$, we have
    - $`\displaystyle\Vert w_{\tau+1} - w_* \Vert_2^2 \le \left(1-\frac{\mu}{L}\right)^\tau \Vert w_1 - w_* \Vert_2^2`$.
      - where $`w_*`$ is the global minimum.
- Analysis)
  - Suppose $`w_1 = 0`$ and $`\Vert w_*\Vert_2 = 1`$.
  - Then, after $`T`$ iterations
    - $`\displaystyle \Vert w_{T+1} - w_*\Vert_2^2 \le \left(1-\frac{\mu}{L}\right)^T`$
  - This implies that after $`\displaystyle T = \frac{L}{\mu} \log\frac{1}{\epsilon} = O(\log(1/\epsilon))`$ steps, we get $`\Vert w_{T+1}-w_*\Vert_2 \le \epsilon`$.
  - Therefore, this gradient descent has a convergence rate of $`O(\exp(-T))`$
    - which is exponentially faster than [L-smooth and convex function](04.md#theorem-6-convergence-of-gradient-descent)













[Back to Main](../main.md)