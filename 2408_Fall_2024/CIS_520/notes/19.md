[Back to Main](../main.md)

# 19. Performance Measures
### Settings)
- Binary Label
  - Training Examples : $`S = \{(x_1, y_1), \cdots, (x_m, y_m)\}`$
    - where
      - $`x_i`$ : the instance
      - $`y_i \in \{\pm1\}`$ : the binary labels.
  - Learning Targets)
    - [Classification Model](#binary-classification) : $`h:\mathcal{X}\rightarrow \{\pm1\}`$
    - Class Probability Estimation (CPE) model : $`\hat{\eta}:\mathcal{X}\rightarrow [0,1]`$
      - To predict the probability of a new instance having label $`+1`$.
    - Ranking or scoring model $`f:\mathcal{X}\rightarrow\mathbb{R}`$.
      - This assigns higher scores to positive instance than to negative ones.
  - Test Samples)
    - $`\{(x_1', y_1'), \cdots, (x_n', y_n')\}`$

<br>

## Binary Classification
### Concept) 0-1 Loss
- Rule)
  - Assign penalty of 1 if the predicted label $`\hat{y}`$ differs from the true label $`y`$.
    ||$`\hat{y} = -1`$|$`\hat{y} = +1`$|
    |:-:|:-:|:-:|
    |$`y = -1`$|0|1|
    |$`y = +1`$|1|0|
- Measure)   
  - Binary Classification Problem   
    $`\displaystyle \text{er}_{\text{test}}^{0-1}[h] = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(h(x_i') \ne y_i')`$
  - **Class Probability Estimation** (CPE)   
    - The problem setting)   
      $`h(x) = \text{sign}\left(\hat{\eta}(x) - 0.5\right) = \begin{cases}
        +1 & \text{if }\hat{\eta}(x) \gt 0.5 \\ -1 & \text{otherwise}
      \end{cases}`$
- Usages)
  - Naive Bayesian classifier
  - Logistic Regression
  - SVM
  - Neural Networks
  - Nearest Neighbors
  - Decision Trees
  - Boosting Algorithm

<br><br>

### Concept) Cost-Sensitive Loss(Asymmetric Classification Cost)
- Rule)
  - Assign different penalty for false positive and the false negative.
  - Put $`c\in[0,1]`$ s.t.
    ||$`\hat{y} = -1`$|$`\hat{y} = +1`$|
    |:-:|:-:|:-:|
    |$`y = -1`$|0|$`c`$|
    |$`y = +1`$|$`1-c`$|0|
- Measures)   
  - Binary Classification Problem   
    $`\displaystyle \text{er}_{\text{test}}^{c}[h] = \frac{1}{n}\sum_{i=1}^n \left( c\cdot \underbrace{\mathbf{1}(h(x_i')=+1 \wedge y_i'=-1)}_{\text{False Positive}} + (1-c)\cdot \underbrace{\mathbf{1}(h(x_i')=-1 \wedge y_i'=+1)}_{\text{False Negative}} \right)`$
  - **Class Probability Estimation** (CPE)   
    - The problem setting)   
      $`h(x) = \text{sign}\left(\hat{\eta}(x) - c\right) = \begin{cases}
        +1 & \text{if }\hat{\eta}(x) \gt c \\ -1 & \text{otherwise}
      \end{cases}`$
  - Weighted Surrogate Loss Minimization
    - Recall that [logistic regression](07.md#7-logistic-regression) and [SVM](08.md#8-support-vector-machine-svm) minimized surrogate loss over the training examples, which were [logistic loss](07.md#model-logistic-regression) and [hinge loss](08.md#concept-hinge-loss) respectively.
    - In these cases, replace the usual loss minimization problem of   
      $`\displaystyle\min_f \frac{1}{m}\sum_{i=1}^m \ell(y_i, f(x_i)) `$ with     
      $`\displaystyle\min_f \frac{1}{m}\sum_{i=1}^m\left( c\cdot \ell(y_i, f(x_i)) \cdot \mathbf{1}(y_i = -1) + (1-c) \ell(y_i, f(x_i))\cdot \mathbf{1}(y_i = +1) \right)`$
    - This has the effect of weighing the losses on positive and negative examples differently during training.
    - Finally, after obtaining $`f:\mathcal{X}\rightarrow\mathbb{R}`$ through the above minimization, the final classifier is obtained by thresholding $`f`$ at 0 as usual: $`h(x) = \text{sign}(f(x))`$.

<br><br>

### Concept) Confusion Matrix of a Binary Classifier (Contingency Table)
- The Table)
  ||$`h(x) = -1`$|$`h(x) = +1`$|
  |:-:|:-:|:-:|
  |$`y = -1`$|True Negative|False Positive|
  |$`y = +1`$|False Negative|True Positive|

<br><br>

### Concept) Complex Performance Measures
#### Concept) True Positive Rate

<br>

#### Concept) True Negative Rate

<br>

#### Concept) Arithmetic Mean / Geometric Mean

<br>

#### Concept) F1-Measure

<br>

#### Concept) Recall and Precision



<br><br>

##  Binary Class Probability Estimation
### Concept) Log Loss


<br><br>

##  Binary Ranking/Scoring
### Concept) ROC Curves (AUROC)

<br>

### Concept) Precision-Recall Curves


<br><br>

[Back to Main](../main.md)