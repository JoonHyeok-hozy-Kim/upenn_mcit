[Back to Main](../main.md)

# 4. Gradient Descent

### Ideation) Why Gradient Descent?
- Recall that our goal was to find a model $`h`$ that minimizes the empirical risk.
  - cf.) [Empirical Risk Minimization (ERM)](01.md#concept-empirical-risk-minimization)
    - $`\displaystyle h^* = \arg\min_{h\in\mathcal{H}} F(h)`$
      - where 
        - $`\mathcal{H}`$ : the family of models
        - $`\displaystyle F(h) = \frac{1}{n}\sum_{i=1}^n \ell(h(\mathbf{x}_i), y_i)`$ : the empirical risk
- Now, further assume that 
  - $`\mathcal{H}`$ is the family of linear models
  - $`w`$ and $`b`$ parameterize the entire set $`\mathcal{H}`$
    - i.e.) $`h(x) = w^\top x + b`$
- Then the above optimization problem over the function $`h`$ turns out to be the minimization problem over $`w`$ and $`b`$.
  - $`h^* = {w^*}^\top x + b^*`$   
    $`\begin{aligned}
        w^*, b^* &= \arg\min_{w,b} F(h) \\
        &= \arg\min_{w,b} \frac{1}{n}\sum_{i=1}^n \ell(h(\mathbf{x}_i), y_i) \\
    \end{aligned} `$
- Even if we don't assume the linear model, the optimization problem can be denoted as
  - $`\displaystyle \min_w F(w) \text{ s.t. } w\in\mathcal{C}`$
    - where
      - $`F:\mathbb{R}^d \rightarrow \mathbb{R}`$
      - $`\mathcal{C}\subseteq \mathbb{R}^d`$
        - cf.) Unconstrained Problem : $`\mathcal{C} = \mathbb{R}^d`$

<br>

## 4.1 Moving Downhill
- Ideation)
  - Our goal is to minimize $`F(w)`$ by choosing $`w`$.
  - How?)
    - Approximate $`F(w)`$ locally at some location $`w`$ with simpler functions.
      - e.g.) Taylor Approximation
    - These approximations provide the direction that $`w`$ should move toward to.
      - e.g.)
        |First Order|
        |:-:|
        |<img src="../images/04/001.png">|

<br>

#### Concept) Taylor Approximation
- First Order : $`F(w+v) \approx F(w) + \nabla F(w)^\top v`$
  - where $`\nabla F(w)`$ is the gradient
- Second Order : $`F(w+v) \approx F(w) + \nabla F(w)^\top v + \frac{1}{2}v^\top H v`$
  - where $`\displaystyle H = \frac{\partial^2 F}{\partial w_i \partial w_j}`$ is the Hessian

<br>

### Concept) Gradient Descent
- Idea)
  - Choose $`v`$ s.t. $`F(w+v) \lt F(w)`$
- Algorithm)
  - Initialize $`w_1 \in \mathbb{R}^d`$.
  - `while` $`t=1,2,\cdots, T`$ `do`
    - Update $`w_{t+1} = w_t - \eta_t\nabla F(w_t)`$.
  - `end`
- Why does it works?)
  - Intuition)
    - Consider that we are updating as 
      - $`w_{t+1} = w_t - \eta_t\nabla F(w_t)`$
    - Then 
      - $`F(w_{t+1}) = F(w_t - \eta_t\nabla F(w_t))`$
    - Recall the first order [Taylor approximation](#concept-taylor-approximation) : 
      - $`F(w+v) \approx F(w) + \nabla F(w)^\top v`$
    - Thus, we may approximate as   
      $`\begin{aligned}
        F(w_t - \eta_t\nabla F(w_t)) &\approx F(w_t) + \nabla F(w_t)^\top (-\eta_t \nabla F(w_t)) \\
        &= F(w_t) -\eta_t \nabla F(w_t)^\top \nabla F(w_t)
      \end{aligned}`$
    - Since $`\eta_t \gt 0`$ and $`\nabla F(w_t)^\top \nabla F(w_t) \ge 0`$,
      - $`F(w_{t+1}) \lt F(w_t)`$
        - i.e.) We are moving $`w`$ to the direction that decreases $`F(w)`$.

#### Concept) Learning Rate
- Props.)
  - If too large, then we could diverge
    - i.e.) Instead of making progress at each time step, we could instead make negative progress. 
  - If too small, then it could take us a very long time to converge to a good solution.

<br>

### Concept) ADAGRAD
- Desc.)    
  - A gradient descent algorithm that automatically adjusts the learning rate.
  - This is a conceptual explanation.
    - Refer to [this](https://optimization.cbe.cornell.edu/index.php?title=AdaGrad) for the actual definition of the algorithm.
- Algorithm)
  - Let
    - $`s_1 = 1 \in\mathbb{R}^d`$
    - $`w_1 \in\mathbb{R}^d`$
  - `while` $`t=1,2,\cdots, T`$ `do`
    - $`g \leftarrow \nabla F(w_t)`$
    - $`s \leftarrow s - (g \circ g)`$
      - where $`\odot`$ is Hadamard product (the element-wise multiplication).
        - Then $`g \odot g = \begin{bmatrix} g_1^2 \\ g_2^2 \\ \vdots \\ g_d^2 \end{bmatrix}`$
    - $`\displaystyle w_{t+1} \leftarrow w_t - \eta_t \frac{g}{\sqrt{s + \epsilon}}`$
      - where $`\epsilon \gt 0`$
- Desc.)
  - Step-sizes in directions with high gradient variance are lower than the step-sizes in directions with low gradient variance. 
    - Geometrically, the step-sizes tend to decrease proportionally to the curvature of the stochastic objective function.

<br>

### Concept) Newton's Method
- Desc.)
  - We use the second order [Taylor Expansion](#concept-taylor-approximation) of the function.
    - $`F(w+v) \approx F(w) + \nabla F(w)^\top v + \frac{1}{2} v^\top Hv`$
  - Recall that we are looking for $`v`$ on the approximation that can directly minimize the original function value $`F(w)`$.
  - To do this, take the derivative of the quadratic approximation w.r.t. $`v`$ and set it equal to zero.   
    $`\begin{aligned}
        & \nabla_v \left[ F(w) + \nabla F(w)^\top v + \frac{1}{2} v^\top Hv \right] = 0 \\
        \Leftrightarrow & \nabla F(w) + Hv = 0 \\
        \Leftrightarrow &  v^* = - H^{-1} \nabla F(w) \\
    \end{aligned}`$
  - Update $`w`$ as follows:
    - $`w_{t+1} = w_t - H^{-1} \nabla F(w)`$   
      |Geometrical Desc.|
      |:-:|
      |<img src="../images/04/002.png" width="500px">|
- Props.)
  - In settings where the quadratic approximation is accurate, this update rule converges extremely fast.
  - No divergence.
    - Why?) No step size used!
  - If there are directions of the function that are extremely shallow, the **inverse** Hessian will have extremely large eigenvalues, leading to very large steps in those directions.
    - Desc.)
      - Hessian's Eigenvalues
        - Positive Eigenvalues: 
          - If an eigenvalue is positive, it means that the function curves upwards in the corresponding direction (like a bowl). Moving in this direction makes the function increase. This corresponds to a local minimum.
        - Negative Eigenvalues: 
          - If an eigenvalue is negative, the function curves downwards in that direction (like an upside-down bowl). Moving in this direction makes the function decrease. This suggests a local maximum.
        - Zero Eigenvalue: 
          - If an eigenvalue is zero (or very close to zero), it indicates that the function is flat in that direction (no curvature). In this case, moving in that direction doesn’t change the function’s value much, which could mean the point is a saddle point or a region with very little curvature.
      - Shallow Directions and Large Eigenvalues
        - When the statement says "if there are directions of the function that are extremely shallow," it means that in those particular directions, the function changes very slowly. Mathematically, these are directions where the curvature is very small, meaning the second-order derivative (the curvature) is close to zero. In other words, the eigenvalues associated with those directions are very small.



[Back to Main](../main.md)