[Back to Main](../main.md)

# 14. Random Forest
### Concept) Bagging (Bootstrap Aggregating)
- How?)
  - Consider a supervised learning dataset : $`\mathcal{D} = \{(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_n, y_n), \}`$
  - Draw $`T`$ new data sets $`\mathcal{D}_1, \cdots, \mathcal{D}_T`$ of size $`n`$, with replacement.
    - e.g.)
      - Let $`\mathcal{D} = \underbrace{\{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), (\mathbf{x}_3, y_3), (\mathbf{x}_4, y_4), (\mathbf{x}_5, y_5)\}}_{\text{size of }n}`$
      - Draw $`T`$ new datasets as follows
        - $`\mathcal{D}_1 = \underbrace{\{(\mathbf{x}_2, y_2), (\mathbf{x}_5, y_5), (\mathbf{x}_2, y_2), (\mathbf{x}_4, y_4), (\mathbf{x}_4, y_4)\}}_{\text{size of }n}`$
        - $`\mathcal{D}_2 = \underbrace{\{(\mathbf{x}_1, y_1), (\mathbf{x}_3, y_3), (\mathbf{x}_2, y_2), (\mathbf{x}_3, y_3), (\mathbf{x}_5, y_5)\}}_{\text{size of }n}`$

<br>

### Concept) Bagged Model
- Let
  - $`\mathcal{A}`$ : the ML algorithm that takes as input a dataset $`\mathcal{D}`$ and returns a model $`h_\mathcal{D}`$.
  - $`\mathcal{D}_1, \cdots, \mathcal{D}_T`$ : the bagged datasets from $`\mathcal{D}`$
- Model)
  - $`\displaystyle H(\mathbf{x}) = \frac{1}{T}\sum_{t=1}^T h_{\mathcal{D}_t}(\mathbf{x})`$ for regression
  - $`\displaystyle H(\mathbf{x}) = \text{mode}\{ h_{\mathcal{D}_t}(\mathbf{x}) \}_{t=1,2,\cdots, T}`$ for classification
    - i.e.) Voting!
- Why doing this?)
  - To reduce overfitteing.
    - Recall that overfitting was fitting to the noise of the data.
    - By bag the dataset and average them out, we can average out the noise as well.
- Prop.)
  - The probability that a dataset $`\mathcal{D}_t`$ does not contain a specific training example is approximately 37%.
    - Why?)
      - Suppose there are $`\vert \mathcal{D} \vert = n`$
      - Then the probability that the $`i`$-th data is not picked is $`\frac{1}{n}`$.
      - Thus, the probability that the same  $`i`$-th data is not picked in a bag is $`\left(1-\frac{1}{n}\right)^n`$
        - Recall that $`\vert\mathcal{D}_t\vert = n`$
      - Then $`\displaystyle\lim_{n\rightarrow\infty}\left(1-\frac{1}{n}\right)^n = \frac{1}{e}\approx 0.3678`$

<br><br>

### Concept) Random Forest Model
- Desc.)
  - Apply [Bagging Model](#concept-bagged-model) methodology to the [Decision Tree](13.md#13-decision-trees) algorithm.
    - In this case $`\mathcal{A}`$ would be the decision tree.
  - Additionally, instead of [full depth](13.md#concept-regularization-for-decision-trees) decision tree, we will **subsample the features**.
    - How?)
      - Among $`d`$ features, choose $`k`$ features where $`k=\sqrt{d}`$ is desirable. (Scikit Learn)
      - The order of $`k`$ features doesn't matter.
    - Why doing this?)
      - We can more diversify the trees.
      - Bagging was choosing data.
      - This one diversifies the way of splitting!
- Model)
  - $`\displaystyle H(\mathbf{x}) = \frac{1}{T}\sum_{t=1}^T h_{\mathcal{D}_t}(\mathbf{x})`$ for regression
  - $`\displaystyle H(\mathbf{x}) = \text{mode}\{ h_{\mathcal{D}_t}(\mathbf{x}) \}_{t=1,2,\cdots, T}`$ for classification
    - where $`h_{\mathcal{D}_t}`$ is a tree with subsampled features.
- Advantages)
  - Fine grained control over model.
    - Recall that the [decision tree was sensitive to the depth](13.md#concept-regularization-for-decision-trees).
      - i.e.) Underfit -> 1 more depth -> Overfit
  - Scale insensitive.
    - No need to normalize the data.
  - Not much data preprocessing required.
    - Simple split for the categorical features.
      - "Is x type 1?" : Yes or No
      - No one-hat encoding required.
    - If no data is provided for a feature?
      - We can ignore that split by putting that example on both yes and no split.
  - Parallelism
    - e.g.) XGBoost library
  - Out of Bag (OOB) Estimation
    - Model)
      - Consider a classification problem.
      - Let $`H_{\neg i}(\mathbf{x}) = \text{mode}\{h_{\mathcal{D}_t}(\mathbf{x}) \vert (\mathbf{x}_i, y_i)\notin \mathcal{D}_t \}`$ be the prediction of random forest, excluding trees that were trained on $`(\mathbf{x}_i, y_i)`$.
      - Then the OOB is
        - $`\displaystyle \mathcal{E}_{\text{oob}} = \frac{1}{n} \sum_{i=1}^n \ell_{0/1} (H_{\neg i}(\mathbf{x}), y_i)`$
    - Derivation)
      - Recall that the probability an example $`(\mathbf{x}_i, y_i)`$ is not included in a bag was 37%.
      - In other words, 37% of the dataset are not used for training a tree.
      - Then, we may use those unused dataset as a validation set.
      - Make sure to exclude the trees that were trained on $`(\mathbf{x}_i, y_i)`$
    - Prop.)
      - It's a huge advantage on that these datasets are validation set for certain trees can be used as a training set for others.
  - Uncertainty quantification
    - Consider the individual tree predictions $`h_{\mathcal{D}_1}(\mathbf{x}), \cdots, h_{\mathcal{D}_T}(\mathbf{x})`$ as individual estimates of the label of $`\mathbf{x}`$.
    - Then, we can use this set of “sample predictions” to reason about our confidence in the prediction of $`\mathbf{x}`$.
      - e.g.)
        - Suppose the variance $`\text{Var}[h_{\mathcal{D}_1}(\mathbf{x}), \cdots, h_{\mathcal{D}_T}(\mathbf{x})]`$ is small across our predictions.
        - This means that all of our trees are making roughly the same prediction for $`\mathbf{x}`$.
          - i.e.) We are quite confident in that prediction.
        - On the other hand, high variance means great uncertainty or the low confidence level.
  - Out of bag pruning
    - How?)
      - Merge two leaf nodes into a single leaf node and delete the split that originally lead to being two leaf nodes.
        - This will change the training error.
          - Why?) Not a full depth tree anymore!
        - This does not change the out of bag error.
          - Why?) Pruning changes the training but OOB data are not used for training.
            - This is a great advantage.






<br><br>

[Back to Main](../main.md)