[Back to Main](../main.md)

# 12. Neural Networks
### Concept) Learning the Feature Map
- Ideation)
  - Recall that [kernel tricks](09.md#concept-kernel-trick) utilize the [feature maps](09.md#concept-feature-map) $`\phi(\mathbf{x})`$ that was powerful for explaining the complex dataset.
  - However, they had [limits](09.md#prop-limits-of-kernel-trick).
  - Then, instead of using kernel tricks to utilize feature maps, why don't we directly learn the feature maps?
- Settings)
  - Let
    - $`(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_n, y_n)`$ : training sets.
      - where $`\mathbf{x}_i \in \mathbb{R}^d`$
    - $`\phi(\mathbf{x}) \in \mathbb{R}^D`$ : the feature map
      - where $`D\gt d`$.
    - $`h(\mathbf{z}) = \mathbf{w^\top}\phi(\mathbf{z})`$ : the predictor using the feature map.
- Model)
  - Put $`\mathbf{V}\in\mathbf{R}^{D\times d}`$ s.t. $`\phi(\mathbf{x}) = \mathbf{Vx}`$.
  - Then, the predictor goes $`h(\mathbf{z}) = \mathbf{w^\top Vx}`$.
  - By learning $`\mathbf{w}\in\mathbb{R}^d`$ and $`\mathbf{V}\in\mathbb{R}^{D\times d}`$, we can optimize the model.
    - where learning $`\mathbf{V}`$ is learning the optimal feature map.
- Limit)
  - Consider that $`\mathbf{w^\top}\mathbf{(Vx)} = \mathbf{(w^\top V)}\mathbf{x}`$.
  - Since $`\mathbf{w^\top V} \in \mathbb{R}^D`$, the model is just the linear model.
    - Sol.) [Neural Network](#concept-neural-networks) : Adding non linearity to the model using the activation functions.

<br>

### Concept) Activation Function
- Desc.)
  - A function that outputs non linear result from the linear input.
- Types)
  - ReLU (Rectified Linear Unit) : $`g(s) = \max(0, s)`$
    - Best!
    - By adding multiple ReLUs and using multiple layers, we can mimic the high dimensional functions.
      |||
      |:-|:-|
      |<img src="../images/12/001.png" width="350px"> <br> $`\mathbf{w}=\begin{bmatrix} 1\\1 \end{bmatrix}, \mathbf{Vx}=\begin{bmatrix}\mathbf{v}_1\\\mathbf{v}_2\end{bmatrix}\begin{bmatrix}\mathbf{x_1}\end{bmatrix}`$ <br> Thus, $`\begin{cases} \text{ReLU}_1 = \max{(0, \mathbf{v}_1\mathbf{x_1})} \\ \text{ReLU}_2 = \max{(0, \mathbf{v}_2\mathbf{x_1})} \end{cases}`$ <br> $`\begin{aligned} \therefore  h(\mathbf{x}) &= w^\top g(\mathbf{Vx}) \\ &= \max{(0, \mathbf{v}_1\mathbf{x_1})} + \max{(0, \mathbf{v}_2\mathbf{x_1})} \end{aligned}`$ |<img src="../images/12/002.png" width="350px">|
  - Sigmoid : $`\displaystyle g(s) = \frac{1}{1+\exp(-s)}`$
  - Hyperbolic Tangent : $`g(s) = \tanh(s)`$
- Notation)
  - Activation Function is a scalar function.
  - If the input is vector, it means the element-wise operation.
    - e.g.)
      - $`g(\mathbf{Vz}) = \begin{bmatrix} g(\mathbf{V}_{11}\mathbf{z}_1 + \cdots + \mathbf{V}_{1d}\mathbf{z}_d) \\ g(\mathbf{V}_{21}\mathbf{z}_1 + \cdots + \mathbf{V}_{2d}\mathbf{z}_d) \\ \vdots \\ g(\mathbf{V}_{D1}\mathbf{z}_1 + \cdots + \mathbf{V}_{Dd}\mathbf{z}_d) \end{bmatrix}`$

<br>

### Concept) Neural Networks
- Model)
  - $`h(\mathbf{z}) = \mathbf{w}^\top \phi(\mathbf{z}) = \mathbf{w}^\top g(\mathbf{Vz})`$
    - where $`g(\cdot)`$ is an activation function












<br><br>

[Back to Main](../main.md)