# Diversity-Guided Multi-Objective Bayesian Optimization with Batch Evaluations

## Abstract
- Goal)
  - Enhancing the Multi-objective Bayesian Optimization (MOBO)
- Existing Findings)
  - MOBO can automate the process of discovering the set of optimal solutions.
    - i.e.) Pareto Front!
  - Testing several samples in **parallel** can reduce the time.
    - Batch is one example of the parallel approach.
- What this papaer suggests)
  - A novel way of choosing the **best batch of samples** to be evaluated in parallel.
- How?)
  - The suggested algorithm approximates and analyzes **a piecewise-continuous Pareto set representation**.
  - This representation allows a batch selection strategy that optimizes for both...
    - hypervolume **improvement**
    - **diversity** of selected samples in order to efficiently advance promising regions of the Pareto Front
      - By enforcing the exploration of diverse identified regions in approximated optimal space.

<br><br>

## 1. Introduction
### Key Contributions)
- DGEMO (Diversity-Guided Efficient Multi-objective Optimization)
  - It simultaneously...
    - solves the multi-objective optimization problems of black-box functions
    - minimizes the number of function evaluations.
- Batch Selections Strategy
  - Divides the space into diversity regions that provide additional information in terms of **shared properties** and **performance of optimal solutions**.

<br><br>

## 2. Related Work
### Bayesian Optimization (BO)
- Advantage)
  - BO achieves a minimal number of function evaluations by utilizing the surrogate model and sampling guided by carefully designed selection criteria.
    - Types of Selection Criteria)
      - **Sequential Selection**
        - Better performance
        - Slower convergence
      - **Batch Selection**
        - Sacrifice some performance
        - Faster thanks to parallel evaluation

### Multi-objective Optimization (MOO)
- Desc.)
  - MOO is applied to problems involving several conflicting objectives and optimizes for a set of Pareto-optimal solutions
- Drawback)
  - Requires substantial number of evaluations
  - Too costly to investigate the infinite number of points on Pareto Frontier.
- e.g.)
  - **MOEA** : Population-based multi-objective evolutionary algorithms.
  - **NSGA-II**
  - **MOEA/D**

### Multi-objective Bayesian Optimization (MOBO)
- Desc.)
  - [BO](#bayesian-optimization-bo) X [MOO](#multi-objective-bayesian-optimization-mobo)
- Two types)
  - Single-Point Method
    - Corresponds to **Sequential Selection** in [BO](#bayesian-optimization-bo)
    - e.g.)
      |Model|Desc.|
      |:-:|:-|
      |**ParEGO** | - Randomly scalarize the MOO problem to a single-objective problem <br> - Select a sample with maximum [EI(expected improvement)](../notes/05.md#concept-expected-improvement).|
      |**EHI**, **SUR** | - Extension of **ParEGO** with different acquisition functions.|
      |**PAL** | - Focus on uncertainty reduction on the surrogate model for better performance|
      |**PESMO**, **MESMO** | - Rely on entropy-based acquisition functions <br> - Select a sample that maximizes the information gain about the optimal Pareto set|
      |**USeMO** | - Use maximum uncertainty as a selection criterion based on the Pareto set generated by **NSGA-II** |
  - Batch Method
    - Corresponds to the **Batch Selection** in [BO](#bayesian-optimization-bo)
    - e.g.)
      |Model|Desc.|
      |:-:|:-|
      |**MOEA/D-DEGO**| - Generalize **ParEGO** by multiple scalarization weights <br> - Perform parallel optimization with **MOEA/D**|
      |**MOBO/D**| - An extension to **MOEA/D-EGO** by changing the acquisition function|
      |**TSEMO**| - Use **Thompson Sampling** (TS) on the GP posterior as an acquisition function <br> -  Optimize multiple objectives with **NSGA-II** <br> - Select the next batch of samples by maximizing the **hypervolume improvement**.|
      |**BS-MOBO**| - Applies MOBO on a different setting with large scale datasets using neural network as surrogate model|
    - But no model considers the **diversity** in both design and performance space in the batch selection.
      - Why **diversity** matters?)
        - For many real-world problems, the Pareto-optimal solutions are distributed in **diverse** regions of the design space.

<br><br>

## 3. Preliminaries
### 3.1 Multi-objective optimization
- Goal)
  - Simultaneously minimize $`m(\ge2)`$ number of objective functions $`\mathbf{f}`$
- Settings)
  - Design Space
    - $`\mathcal{X} \subset \mathbb{R}^d`$ :  A continuous set of input variables.
  - Multiple Objective Functions
    - Notations)
      - $`f_1,\cdots, f_m:\mathcal{X}\rightarrow\mathbb{R}`$
      - $`\mathbf{f(x)} = (f_1(\mathbf{x}), \cdots, f_m(\mathbf{x}))`$ : the vector of all objects
        - where $`\mathbf{x}\in\mathcal{X}`$
  - Performance Space
    - $`\mathbf{f}(\mathcal{X}) \subset \mathbb{R}^m`$ : the $`m`$-dimensional image
  - Conflicting Objectives
    - Assume that the objective functions are often conflicting.
    - Thus, the set of **optimal solutions** are available, not the single best solution.
    - These optimal solutions are denoted by the **Pareto Set** and the **Pareto Front**.
      - Pareto Set
        - $`\mathcal{P}_s \subseteq \mathcal{X}`$
      - Pareto Front
        - $`\mathcal{P}_f = \mathbf{f}(\mathcal{P}_s) \subseteq \mathbb{R}^m`$
      - Pareto Optimality
        - Def.) A point $`\mathbf{x}^*\in\mathcal{P}_s`$ is considered Pareto-Optimal if 
          - $`\nexists\mathbf{x}\in\mathcal{X} \text{ s.t. } \begin{cases} f_i(\mathbf{x}^*) \ge f_i(\mathbf{x}),\; \forall i \\ f_i(\mathbf{x}^*) \gt f_i(\mathbf{x}),\; \exists i \end{cases}`$
- How do we measure the utility of the set of points?)
  - MOO commonly uses **hypervolume indicator**
    - Concept) Hypervolume $`\mathcal{H}(\mathcal{P}_f)`$
      - Def.)
        - Let
          - $`\mathcal{P}_f`$ : a Pareto Front **approximation** in an $`m`$-dim'l performance sapce
          - $`r\in\mathbb{R}^m`$ : a reference point
            - i.e.) a fixed point deliberately chosen so that its performance is “inferior” to that of all candidate solutions (or Pareto-optimal solutions)
        - Then the **hypervolume** $`\mathcal{H}(\mathcal{P}_f)`$ is defined as   
          - $`\displaystyle \mathcal{H}(\mathcal{P}_f) = \int_{\mathbb{R}^m} \mathbb{1}_{H(\mathcal{P}_f)}(z)dz`$ 
            - where 
              - $`H(\mathcal{P}_f) = \{ z\in Z \;\vert\; \exists1\le i\le \vert\mathcal{P}_f \vert : r \preceq z \preceq \mathcal{P}_f(i) \}`$
              - $`\mathcal{P}_f(i)`$ : the $`i`$-th solution in $`\mathcal{P}_f`$
              - $`\preceq`$ : the relation operator of objective dominance
              - $`\mathbb{1}_{H(\mathcal{P}_f)} = \begin{cases} 1 & \text{ if } z\in H(\mathcal{P}_f) \\ 0 & \text{ otherwise} \end{cases}`$ : a Dirac Delta function
    - Concept) Hypervolume Improvement $`\text{HVI}(P, \mathcal{P}_f)`$
      - Def.)
        - $`\text{HVI}(P, \mathcal{P}_f) = \mathcal{H}(\mathcal{P}_f \cup P) - \mathcal{H}(\mathcal{P}_f)`$
      - Meaning)
        - How much the hypervolume would **increase** if a set of new points $`P(\mathbf{p}_1, \cdots, \mathbf{p}_n)\subset\mathbb{R}^m`$ is added to the current Pareto front approximation $`\mathcal{P}_f`$       
        ![](../images/paper_001.png)

<br>


### 3.2 Bayesian Optimization
- Goal)
  - Find a global optimum solution of a black-box function $`f:\mathcal{X}\subset\mathbb{R}^d\rightarrow\mathbb{R}`$ that is expensive to evaluate.
- Advantage of BO)
  - Powerful for such optimization problem
  - The selection strategy of the next points to evaluate that balances the trade-off between
    - the exploration of unknown regions
    - exploitation of the best performing ones
- Procedure)
  - Single Objective Optimizations)
    - Iterate the followings.
      1. Train a single surrogate model
         - e.g.) GP
      2. Optimize a single acquisition function to select the next point to evaluate
         - e.g) 
           - expected improvement (EI)
           - probability improvement (PI)
           - upper confidence bound (UCB)
  - Multi Objective Optimizations)
    - Iterate the followings.
      1. For each objective, train a surrogate model independently.
      2. The single acquisition function is adapted to overlook multiple models.
         - e.g.)
           - expected hypervolume improvement (EHI)
             - Emmerich, *The computation of the expected improvement in dominated hypervolume of pareto front approximations.*
           - predictive entropy search (PES)
             - Hernández-Lobato et al., *Predictive entropy search for efficient global optimization of black-box functions.*

<br><br>

## 4. Proposed Method

#### Concept) Latin Hypercube Sampling (LHS)
- Goal)
  - Drawing $`N`$ samples from the $`d`$-dimensional space
- Desc.)
  - For $`d`$-dimensional space, divide each dimension into $`N`$ equally sized intervals.
    - There will be $`N^d`$ cells total.
  - We want exactly one sample in each interval along each dimension.
    - i.e.) each of the $`N`$ intervals in a given dimension is used exactly once.   
      ![](../images/paper_002.png)




