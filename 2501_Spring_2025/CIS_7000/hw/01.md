# Gaussian Distribution Diagnostic Problem Set

## 1. The Univariate Linear Gaussian Identity
### Question 1-1)
Since $`N(a;\mu,\sigma^2)`$ is a density, we have that $`\displaystyle \int_\infty^\infty \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right)da = 1`$.   
Prove that $`\mathbb{E}[Y-\mu] = 0`$ and $`\mathbb{E}[(Y-\mu)^2] = \sigma^2`$ by differentiating both sides of the equation.

#### Q1-1 Sol.)
By differentiating the given equality with $`\mu`$, we may get   
$`\begin{aligned}
    0 &= \frac{d}{d\mu} \int_\infty^\infty \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right)da \\
    &= \int_\infty^\infty \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right) \cdot \left(-\frac{a-\mu}{\sigma^2}\right) da \\
    &= -\frac{1}{\sigma^2} \int_\infty^\infty (a-\mu) \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right) da \\
    &= -\frac{1}{\sigma^2} \int_\infty^\infty (a-\mu)  f(a;\mu,\sigma^2) da \\
\end{aligned}`$   
Thus, we have $`\displaystyle \mathbb{E}[Y-\mu] = \int_\infty^\infty (a-\mu)  f(a;\mu,\sigma^2) da = 0`$.
Again by differentiating the equality with $`\sigma^2`$, we may get   
$`\begin{aligned}
    0 &= \frac{d}{d\sigma^2} \int_\infty^\infty \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right)da \\
    &= \int_\infty^\infty \left[ \frac{1}{\sqrt{2\pi}} \left(-\frac{1}{2\sigma^2\sqrt{\sigma^2}}\right) \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right) + \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right) \left(\frac{(a-\mu)^2}{2(\sigma^2)^2}\right) \right] da \\
    &= \int_\infty^\infty \left[ \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right) \left\{ -\frac{1}{2\sigma^2} + \frac{(a-\mu)^2}{2(\sigma^2)^2} \right\} \right] da \\
    &= \int_\infty^\infty \left[ \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right) \left\{ \frac{(a-\mu)^2-\sigma^2}{2(\sigma^2)^2} \right\} \right] da \\
    &= \int_\infty^\infty \left[ \left\{ \frac{(a-\mu)^2-\sigma^2}{2(\sigma^2)^2} \right\} f(a;\mu,\sigma^2) \right] da \\
\end{aligned}`$   
Thus, we have $`\displaystyle \mathbb{E}[(Y-\mu)^2-\sigma^2] = \int_\infty^\infty \left\{ (a-\mu)^2 - \sigma^2 \right\} f(a;\mu,\sigma^2) da = 0`$.   
Hence, $`\mathbb{E}[(Y-\mu)^2] = \sigma^2`$.

<br>

### Question 1-2)
Let $`Y, Y'\sim N(0, 1)`$ be two i.i.d. standard Gaussian random variables. Write out the joint density $`p(Y=(b-a), Y'=a)`$ and simplify.

#### Q1-2 Sol.)
Since $`Y, Y'`$ are i.i.d., $`p(Y,Y') = p(Y)p(Y')`$. Also, $`f(a) = \displaystyle \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{a^2}{2}\right)`$.


Thus, we may get the joint pdf as below.   
$`\begin{aligned}
    p(Y=(b-a), Y'=a) 
    &= p(Y=(b-a)) \cdot p(Y'=a) \\
    &= \left( \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(b-a )^2}{2}\right) \right) \left( \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(a )^2}{2}\right) \right) \\
    &= \frac{1}{2\pi} \exp\left(-\frac{(b-a )^2 + (a )^2}{2}\right)
\end{aligned}`$

<br>

### Question 1-3)
Using your answer above, prove that $`\displaystyle \int_\infty^\infty p(Y=(b-a), Y'=a) da = \frac{1}{2\sqrt{\pi}}\exp\left(-\frac{1}{2^2} b^2\right)`$

#### Q1-3 Sol.)
Consider that   
  
$`\begin{aligned}
    p(Y=(b-a), Y'=a) 
    &= \frac{1}{2\pi} \exp\left(-\frac{(b-a )^2 + (a )^2}{2}\right) \\
    &= \frac{1}{2\pi} \exp\left(-\frac{2(a-\frac{b}{2})^2 + \frac{b^2}{2}}{2}\right) \\
    &= \frac{1}{2\pi} \exp\left(-(a-\frac{b}{2})^2 - \frac{b^2}{4} \right) \\
\end{aligned}`$

Thus,

$`\begin{aligned}
    \int_\infty^\infty p(Y=(b-a), Y'=a) da
    &= \int_\infty^\infty \frac{1}{2\pi} \exp\left(-(a-\frac{b}{2})^2 - \frac{b^2}{4} \right) da \\
    &= \frac{1}{2\pi\exp\left(\frac{b^2}{4}\right)} \int_\infty^\infty -(a-\frac{b}{2})^2da \\
    &= \frac{1}{2\pi\exp\left(\frac{b^2}{4}\right)} \sqrt{\pi} \\
    &= \frac{1}{2\sqrt{\pi}}\exp\left(-\frac{1}{2^2} b^2\right)
\end{aligned}`$

<br>

### Question 1-4)
Based on the previous result, what can you say about the distribution of the random variable $`Z = Y+Y'`$?

#### Q1-4 Sol.)
If $`Y,Y'`$ are i.i.d. and both Gaussian, then $`Z = Y+Y'`$ is also a Gaussian where $`Z\sim N(0, 2)`$

<br><br>

## 2. Multivariate Gaussian Random Variables
### Def.) Multivariate Gaussian
Let $`Y`$ be a $`d`$-dimensional vector valued random variable. $`Y`$ is Multivariate Gaussian iff. all linear combination of its entries are univariate Gaussian.
- i.e.) $`\forall c\in\mathbb{R}^d, c^\top Y \sim N(\mu, \sigma^2), \exists \mu,\sigma\in\mathbb{R}`$

<br>

### Question 2-1)
Let $`U = [U_1 \; \cdots \; U_d]`$ be a random $`d`$-dimensional vector, where $`U_i \stackrel{\text{i.i.d.}}{\sim} N(0,1), \forall i`$. Prove that $`U`$ meets the definition of a multivariate Gaussian random variable.

#### Q2-1 Sol.)
Put $`c = \begin{bmatrix} c_1\\ c_2\\\vdots\\c_d \end{bmatrix}\in\mathbb{R}^d`$. Then, $`\displaystyle c^\top U = \sum_{i=1}^d c_i U_i`$.   

In [Question 1-4](#question-1-4), we have shown that the sum of Gaussian is also Gaussian. Thus, if $`U_i \sim N(0, 1)`$, then $`c_i U_i \sim N(0, c_i^2)`$. Furthermore, $`\displaystyle\sum_{i=1}^d c_i U_i \sim N(0, \sum_{i=1}^d c_i^2)`$.

<br>

### Question 2-2)
Consider the random vector $`Y = LU + \mu`$, where $`\mu, L`$ are deterministic. Prove that $`Y`$ also meets the definition for a multivariate Gaussian random variable, and compute its mean and covariance.

#### Q2-2 Sol.)
Put $`l_{ij}`$ be the element of $`L`$ on its $`i`$-th row and its $`j`$-th column. Then Y can be rewritten as   
$`Y = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_d \end{bmatrix} = \begin{bmatrix} \sum_{k=1}^d l_{1k}U_k + \mu_1 \\ \sum_{k=1}^d l_{2k}U_k + \mu_2 \\ \vdots \\ \sum_{k=1}^d l_{dk}U_k + \mu_d \end{bmatrix}`$.

In [Question 1.4](#question-1-4), we have shown that the linear combination of Gaussian is also Gaussian. Additionally, adding constant $`\mu_i`$ changes the mean but the random variable still remains to be the Gaussian. Thus, $`Y_i \sim N(\mu_i, \sum_{k=1}^d l_{ik}^2)`$.

Additionally, we may get the covariance of $`Y`$ as    
$`\begin{aligned}
    \text{Cov}(Y) &= \mathbb{E}[(Y-\mu)(Y-\mu)^\top] \\
    &= \mathbb{E}[(LU)(LU)^\top] \\
    &= \mathbb{E}[LUU^\top L^\top] \\
    &= \mathbb{E}[L I_d L^\top] \\
    &= \mathbb{E}[LL^\top] \\
    &= LL^\top
\end{aligned}`$

$`\therefore Y \sim N(\mu, LL^\top)`$

<br>

### Question 2-3)
Let $`Z`$ be a multivariate Gaussian random variable where $`\mathbb{E}[Z] = \mu`$ and $`\mathbb{E}\left[(Z-\mathbb{E}[Z])(Z-\mathbb{E}[Z])^\top\right] = LL^\top`$. Prove that $`\forall a\in\mathbb{R}, c\in\mathbb{R}^d`$, we have that $`p((c^\top Z) = a) = p((c^\top(L^\top U + \mu)) = a)`$.

#### Q2-3 Sol.)
In [Question 2-2](#question-2-2), we have shown that $`Z = LU+\mu \sim N(\mu, LL^\top)`$. Then, $`c^\top Z \sim N(c^\top\mu, (c^\top L)(c^\top L)^\top) = N(c^\top\mu, c^\top LL^\top c)`$

Likewise, putting $`Z' = L^\top U + \mu`$, we have $`Z'\sim N(\mu, L^\top L)`$ and $`c^\top Z' \sim N(c^\top \mu, (c^\top L^\top)(c^\top L^\top)^\top) = N(c^\top\mu, c^\top L^\top L c)`$

Thus, for $`p((c^\top Z) = a) = p((c^\top Z') = a)`$ to hold, $`c^\top LL^\top c = c^\top L^\top L c`$ should hold.

<br>

### Question 2-4)
The last fact, taken together with the Cram√©r-Wold theorem, implies that $`p(Z=a) = p((LU+\mu) = a), \forall a\in\mathbb{R}^d`$. 
- i.e.) **Two multivariate Gaussian random variables are equal in distribution if they share the same mean and covariance**. 
 
We will exploit this fact to derive a density for $`Z`$.

Write the joint density $`p(U=a)`$ as a matrix.

#### Q2-4 Sol.)
$`\begin{aligned}
    p(U=a) &= \prod_{i=1}^d p(U_i = a_i) \\
    &= \prod_{i=1}^d \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{a_i^2}{2}\right) \\
    &= \left(\frac{1}{\sqrt{2\pi}}\right)^d \exp\left(-\frac{\sum_{i=1}^d  a_i^2}{2}\right) \\
    &= \left(\frac{1}{\sqrt{2\pi}}\right)^d \exp\left(-\frac{a^\top a}{2}\right) \\
\end{aligned}`$

<br>

### Question 2-5)
Assume that $`L`$ is a square matrix, and define $`K = LL^\top`$. Using the change-of-variables formula, prove that the density of $`LU+\mu`$ is $`\displaystyle N(a;\mu,K) := \frac{1}{\vert 2\pi K \vert^\frac{1}{2}} \exp \left( -\frac{1}{2} (\alpha-\mu)^\top K^{-1} (\alpha-\mu) \right)`$

#### Q2-5 Sol.)
Put $`Z = LU+\mu`$. Then $`L^{-1}(Z-\mu) = U`$.   
Considering that $`U \sim N(0, I_d)`$, we have $`\displaystyle p_U(u) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} u^\top u\right)`$.   
Thus, $`X\sim N(\mu, K)`$ where $`K = LL^\top`$ and we have   
$`\begin{aligned}
    p_Z(a) &= p_U(L^{-1}(a-\mu)) \cdot \vert\det(L^{-1})\vert \\
    &= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} (L^{-1}(a-\mu))^\top (L^{-1}(a-\mu))\right)  \cdot \frac{1}{\vert\det(L)\vert} \\
    &= \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sqrt{\det(L^\top L)}} \exp\left(-\frac{1}{2} (a-\mu)^\top (L^{-1})^\top (L^{-1}(a-\mu))\right) \\
    &= \frac{1}{\sqrt{2\pi \det(K)}} \exp\left(-\frac{1}{2} (a-\mu)^\top K^{-1} (a-\mu)\right)
\end{aligned}`$

<br>

### Question 2-6)
Consider the following multivariate Gaussian, written in block matrix form:

$`\begin{bmatrix} Y \\ Y' \end{bmatrix} \sim N\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} K & K' \\ K'^\top & K'' \end{bmatrix} \right)`$ where $`Y\in\mathbb{R}^d, Y'\in\mathbb{R}^{d'}`$

Prove that if $`K' = 0`$ then $`Y, Y'`$ are independent Gaussian random variables.

#### Q2-6 Sol.)
Consider that $`K' = \text{Cov}(Y, Y') = 0`$.  
Thus, $`\text{Cov}(Y, Y') = \mathbb{E}\left[ (Y-0)^\top (Y'-0) \right] = \mathbb{E}\left[ Y^\top Y' \right] = 0`$.   
i.e. $`\mathbb{E}[Y_i \cdot Y_j] = 0, \forall i,j`$.   
Therefore, $`Y, Y'`$ are independent Gaussian random variables.

<br>

### Question 2-7)
Prove the following generalization of the linear Gaussian identity:

If $`Y\sim N(\mu, LL^\top)`$ and $`Y'\sim N(\mu', L'{L'}^\top)`$ are independent multivariate Gaussian random variables, then $`p(AY + BY' + c)\sim N(A\mu + B\mu' + c, ALL^\top A^\top + BL'{L'}^\top B^\top)`$

#### Q2-7 Sol.)
Consider two tensors:   

$`T = \begin{bmatrix} A & B \end{bmatrix}`$ and $`S = \begin{bmatrix} Y \\ Y' \end{bmatrix}`$.

Then $`AY + BY' = \begin{bmatrix} A & B \end{bmatrix}\begin{bmatrix} Y \\ Y' \end{bmatrix} \sim N\left( \begin{bmatrix} A & B \end{bmatrix}\begin{bmatrix} \mu \\ \mu' \end{bmatrix}, \begin{bmatrix} A & B \end{bmatrix}\begin{bmatrix} K & 0 \\ 0 & K'' \end{bmatrix}\begin{bmatrix} A & B \end{bmatrix}^\top \right)`$.

Consider that $`\begin{bmatrix} A & B \end{bmatrix}\begin{bmatrix} \mu \\ \mu' \end{bmatrix} = A\mu + B\mu'`$.

Also,   
$`\begin{aligned}
    \begin{bmatrix} A & B \end{bmatrix}\begin{bmatrix} K & 0 \\ 0 & K'' \end{bmatrix} \begin{bmatrix} A & B \end{bmatrix}^\top
    &= \begin{bmatrix} AK & 0 \\ 0 & BK'' \end{bmatrix}\begin{bmatrix} A^\top \\ B^\top \end{bmatrix} \\
    &= AKA^\top + BK''B^\top \\
    &= ALL^\top A^\top + BL'{L'}^\top B^\top
\end{aligned}`$

Thus, $`AY + BY' \sim N\left(A\mu + B\mu', ALL^\top A^\top + BL'{L'}^\top B^\top \right)`$.

By adding the constant $`c`$, we have   

$`AY + BY' + c \sim N\left(A\mu + B\mu' + c, ALL^\top A^\top + BL'{L'}^\top B^\top \right)`$.