[Back to Main](../main.md)

Jan. 21 Lecture Note

# Bayesian Statistics
### Concept) Product Rule
- Thm.)   
  $`\begin{aligned}
    P(A, B) &= P(A\vert B) P(B) \\
    &= P(B\vert A) P(A) \\
  \end{aligned}`$

<br>

### Concept) Sum Rule
- Thm.)   
  $`\displaystyle P(A) = \int_B P(A, B) dB`$

<br>

### Concept) Bayes Rule
- Thm.)   
  $`\displaystyle P(A \vert B) = \frac{P(A,B)}{P(B)} = \frac{P(B\vert A) P(A)}{P(B)}`$

<br>

### Concept) Estimation Using Bayes Rule
- Setting)
  - $`\theta`$ : the parameter that we want to estimate
  - $`\mathcal{D}`$ : the dataset
- The estimation on $`\theta`$ using the dataset $`\mathcal{D}`$ goes   
  $`\displaystyle P(\theta\vert\mathcal{D}) = \frac{P(\mathcal{D}\vert\theta) P(\theta)}{\displaystyle\int P(D\vert\theta) P(\theta) d\theta}`$
  - Desc.)
    - $`P(\theta\vert\mathcal{D})`$ : the posterior
    - $`P(\mathcal{D}\vert\theta)`$ : the likelihood
    - $`P(\theta)`$ : the prior
  - cf.) Integrals are mostly intractable. Thus, we will find more feasible solutions.

<br>

### E.g.) Binomial Experiment
- Settings)
  - The dataset $`\mathcal{D}`$ is composed of
    - $`n`$ number of trials  with
      - $`x`$ number of 1 
      - $`(n-x)`$ number of 0.
  - e.g.) Tossing a coin $`n`$ times and count $`x`$ the number of heads.
- [Bayes Rule](#concept-bayes-rule) applied   
  $`\displaystyle p(\theta\vert x) = \frac{p(x\vert n, \theta) \cdot p(\theta\vert\alpha,\beta)}{\displaystyle\int P(x\vert n,\theta) p(\theta\vert\alpha,\beta) d\theta}`$

#### Likelihood
- Def.)   
  $`P(x\vert n,\theta) = \begin{pmatrix} n\\x \end{pmatrix} \theta^x(1-\theta)^{n-x}`$ : Binomial Distribution

<br>

#### Prior
- Def.)   
  $`\begin{aligned}
    P(\theta \vert \alpha, \beta) &= \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}
  \end{aligned}`$
  - where $`B(\alpha, \beta)`$ is a beta function s.t. $`\displaystyle B(\alpha, \beta) = \int \theta^{\alpha-1}(1-\theta)^{\beta-1} = 1`$ for the given $`\alpha, \beta`$

<br>

#### Normalization Constant
- Def.)
  - $`\displaystyle \begin{pmatrix} n\\x \end{pmatrix}\frac{B(\alpha+x, \beta+n-x)}{B(\alpha, \beta)}`$
- Derivation)
  - Recall [the Binomial Experiment's the Bayes Rule](#eg-binomial-experiment) s.t. $`\displaystyle p(\theta\vert x) = \frac{p(x\vert n, \theta) \cdot p(\theta\vert\alpha,\beta)}{\displaystyle\int P(x\vert n,\theta) p(\theta\vert\alpha,\beta) d\theta}`$.
  - Then, its denominator can be rewritten as   
    $`\begin{aligned}
        \int \underbrace{P(x\vert n,\theta)}_{\text{likelihood}} \; \underbrace{p(\theta\vert\alpha,\beta)}_{\text{prior}} d\theta 
        &= \int \underbrace{\begin{pmatrix} n\\x \end{pmatrix}\theta^x (1-\theta)^{n-x}}_{\text{likelihood}} \; \underbrace{\frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}}_{\text{prior}} d\theta \\
        &= \begin{pmatrix} n\\x \end{pmatrix} \frac{1}{B(\alpha, \beta)} \int \theta^{x+\alpha-1}(1-\theta)^{n-x+\beta-1} \\
        &= \begin{pmatrix} n\\x \end{pmatrix} \frac{B(\alpha+x, \beta+n-x)}{B(\alpha, \beta)}
    \end{aligned}`$

<br>

#### Posterior
- Def.)
  - $`\displaystyle p(\theta\vert x) = \frac{1}{B(\alpha+x, \beta+n-x)} \theta^{\alpha+x-1} (1-\theta)^{\beta+n-x-1}`$
- Derivation)
  - From the [the Binomial Experiment's the Bayes Rule](#eg-binomial-experiment) we may get,   
    $`\begin{aligned}
        p(\theta\vert x) 
        &= \frac{p(x\vert n, \theta) \cdot p(\theta\vert\alpha,\beta)}{\displaystyle\int P(x\vert n,\theta) p(\theta\vert\alpha,\beta) d\theta} \\
        &= \frac{\begin{pmatrix} n\\x \end{pmatrix} \theta^x(1-\theta)^{n-x} \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}}{\begin{pmatrix} n\\x \end{pmatrix} \frac{B(\alpha+x, \beta+n-x)}{B(\alpha, \beta)}} \\
        &= \frac{1}{B(\alpha+x, \beta+n-x)} \theta^{\alpha+x-1} (1-\theta)^{\beta+n-x-1}
    \end{aligned}`$

<br>

### Concept) Beta Distribution
- Def.)
  - $`\theta \sim B(\alpha, \beta)`$
- Why using this?)
  - It's property is just right for prior belief and the update of the posterior after the experiment.
  - Suppose we have the prior of $`B(3, 2)`$.
  - This can be interpreted as
    - $`\theta = 1`$ is counted to be 3.
    - $`\theta = 0`$ is counted to be 2.
  - Then the beta distribution will be a bit skewed toward $`\theta = 1`$.
  - Suppose we did the experiment $`n=95`$ times and got $`x=57`$.
    - According to the Posterior above, the beta function will be updated into $`B(60, 40)`$.
    - In this case, the ratio between two are the same : $`3:2`$.
    - However, $`B(60, 40)`$ has more data that support the ratio of $`3:2`$.
    - Thus, the skewness of the beta distribution will be the same but the peak will be more high and the trunk will be much narrower.
      - i.e.) Higher chance of having $`\theta = 0.6`$
- cf.) How to choose prior
  - Uniform prior : $`B(1,1)`$

<br>

### Concept) Hypothesis Testing in Bayesian Statistics : Bayesian Decision Making
- e.g.)
  - $`\text{Pr}(\theta\gt 0.6) = \displaystyle\int_{0.6}^1 p(\theta\vert\mathcal{D}) d\theta`$
- Problem)
  - There are infinitely many intervals that has the same probability (the area under $`p`$).
  - What interval should we choose?
    - This is called the Bayesian Decision Making
      - Suppose $`\theta^*`$ is the true value and we want a point $`\hat{\theta}`$.
      - Get the loss, which is defined as the difference between $`\theta^*`$ and $`\hat{\theta}`$.
        - $`\ell(\theta^*, \hat{\theta})`$
      - Get the following.   
        $`\begin{aligned}
            L(\hat{\theta}) 
            &= \int \ell(\theta^*, \hat{\theta}) p(\theta^* \vert \mathcal{D}) d(\theta^*) \\
            &= \mathbb{E}_{p(\theta^* \vert D)} [\ell(\theta^*, \hat{\theta})]
        \end{aligned}`$
      - Solve $`\displaystyle \hat{\theta} = \arg\min_{\hat{\theta}} L(\hat{\theta})`$

<br><br>

[Back to Main](../main.md)