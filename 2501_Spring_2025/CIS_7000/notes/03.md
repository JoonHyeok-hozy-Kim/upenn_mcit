[Back to Main](../main.md)

# 3. Regression In Bayesian Optimization Problem
- Problem Settings
  - $`\mathcal{D} = \{(X_1, y_1), \cdots, (X_n, y_n)\}`$
  - We assume that the actual label value $`y`$ is Gaussian by
    - $`\mathbf{y} = X\mathbf{w} + \epsilon`$ 
      - where 
        - $`\mathbf{y}, \epsilon \in\mathbb{R}^n, \epsilon \sim N(0, \sigma_n^2 I)`$
        - $`X\in\mathbb{R}^{n\times d}`$
        - $`\mathbf{w}\in\mathbb{R}^d`$
    - Assuming $`X\mathbf{w}`$ is constant, $`\mathbf{y}`$ is also Gaussian.
  - We want to estimate $`\mathbf{w}`$.
- Solution)
  - Suppose we know the Gaussian Prior on $`\mathbf{w}`$ as $`\mathbf{w}\sim N(\mu, \Sigma)`$
    - Why Gaussian?)
      - We will show that the ridge regression results in the Gaussian Distribution of $`\mathbf{w}`$
  - Then we may set up our problem as
    - Likelihood : $`p(\mathbf{y}\vert \mathbf{w}, X) = N(X\mathbf{w}, \sigma_n^2 I)`$
      - Why?) $`\mathbf{y}`$ is just an [affine transformation](02.md#thm1-affine-transformations-of-gaussians-are-gaussian) of $`\epsilon`$.
    - Prior : $`\mathbf{w}\sim N(\mu, \Sigma)`$
  - We can estimate $`\mathbf{w}`$ by calculating the posterior as below.
    - Posterior : $`\displaystyle p(\mathbf{w}\vert X,\mathbf{y}) = \frac{p(\mathbf{y}\vert \mathbf{w}, X) p(\mathbf{w})}{p(\mathbf{y})}`$ : the one that we've been looking for!
      - i.e.) We are estimating $`\mathbf{w}`$ given the data $`\mathcal{D} = \{(X_1, y_1), \cdots, (X_n, y_n)\}`$.
        - Thus, the posterior can also be written as $`p(\mathbf{w}\vert X,\mathbf{y}) = p(\mathbf{w}\vert \mathcal{D})`$
  - Recall that the main reason that we estimate $`\mathbf{w}`$ is to make predictions on $`\mathbf{y}`$ given the data $`X`$.
    - Let $`X^*`$ be the new test data and $`\mathbf{y}^*`$ be the prediction on it, given $`\mathbf{w}`$.
    - In this case, our goal is to get the **predictive posterior** $`p(\mathbf{y}^*\vert X^*, \mathcal{D})`$.
    - If we know the joint distribution between $`\mathbf{y}^*`$ and $`\mathbf{w}`$ given $`X^*`$ and $`\mathcal{D}`$, we may get the prediction posterior as below.
      - $`\displaystyle p(\mathbf{y}^*\vert X^*, \mathcal{D}) = \int_\mathbf{w} p(\mathbf{y}^*, \mathbf{w}\vert X^*)d\mathbf{w}`$
        - i.e.) Averaging out all possible $`\mathbf{w}`$
    - Luckily, we can denote the joint distribution $`p(\mathbf{y}^*, \mathbf{w}\vert X^*)`$ as
      - $`p(\mathbf{y}^*, \mathbf{w}\vert\mathcal{D}) = p(\mathbf{y}^*\vert X^*, \mathbf{w})p(\mathbf{w}\vert\mathcal{D})`$
    - Thus, $`\displaystyle p(\mathbf{y}^*\vert X^*, \mathcal{D}) = \int_\mathbf{w} p(\mathbf{y}^*, \mathbf{w}\vert X^*)d\mathbf{w} = \int_\mathbf{w} p(\mathbf{y}^*\vert X^*, \mathbf{w}) p(\mathbf{w}\vert \mathcal{D})d\mathbf{w}`$
      - Here,
        - $`p(\mathbf{y}^*\vert X^*, \mathbf{w})`$ is the likelihood given $`\mathbf{w}`$
        - $`p(\mathbf{w}\vert \mathcal{D})`$ is the posterior $`p(\mathbf{w}\vert X, \mathbf{y})`$ that we derived above.
- Analysis)
  - Let's look at $`\mathbf{w}`$ estimation in the Linear Algebraic point of view.
  - We may set up the problem as   
    $`\begin{bmatrix} \mathbf{w}\\\mathbf{y} \end{bmatrix} \sim N\left(\begin{bmatrix} \mu\\X\mu \end{bmatrix} , \begin{bmatrix} \Sigma & \Sigma X^\top \\ X\Sigma & X\Sigma X^\top + \sigma_n^2 I \end{bmatrix} \right)`$
    - where
      - $`\mathbf{y} = X\mathbf{w} + \epsilon`$ where $`\epsilon\sim N(0, \sigma_n^2 I)`$
      - $`\mathbf{w}\sim N(\mu, \Sigma)`$
    - cf.)
      - Calculating covariances   
        $`\begin{aligned}
            \text{Cov}[\mathbf{y}, \mathbf{w}] 
            &= \text{Cov}[X\mathbf{w}+\epsilon, \mathbf{w}] \\
            &= \text{Cov}[X\mathbf{w}, \mathbf{w}] & (\because \text{Cov}[\epsilon, \mathbf{w}] = 0) \\
            &= X\Sigma
        \end{aligned}`$   
        $`\begin{aligned}
            \text{Cov}[\mathbf{y}, \mathbf{y}] 
            &= \text{Cov}[X\mathbf{w}+\epsilon, X\mathbf{w}+\epsilon] \\
            &= \text{Cov}[X\mathbf{w}, X\mathbf{w}] + \text{Cov}[\epsilon, \epsilon] & (\because \text{Cov}[\epsilon, \mathbf{w}] = 0) \\
            &= X\Sigma X^\top + \sigma_n^2 I
        \end{aligned}`$
  - We are estimating $`\mathbf{w}`$ using the data $`\mathcal{D}`$.
    - i.e.) Our target is $`\mathbf{w} \vert \mathcal{D}`$
  - Thus, we are estimating $`p(\mathbf{w}\vert \mathcal{D}) = p(\mathbf{w}\vert \mathbf{y}, X)`$ 
  - [Recall](01.md#concept-estimation-using-bayes-rule) that $`p(\mathbf{w}\vert \mathbf{y}, X)`$ can be derived as   
    $`\begin{aligned}
        p(\mathbf{w}\vert \mathbf{y}, X) &= \frac{p(\mathbf{w}, \mathbf{y}\vert X)}{p(\mathbf{y}\vert X)} \\
        &= \frac{p(\mathbf{y}\vert \mathbf{w}, X) p(\mathbf{w})}{\int p(\mathbf{y}\vert \mathbf{w}, X) p(\mathbf{w}) d\mathbf{w}}
    \end{aligned}`$
    - where
      - $`p(\mathbf{y}\vert \mathbf{w}, X)`$ is the likelihood
      - $`p(\mathbf{w})`$ is the prior
      - $`\int p(\mathbf{y}\vert \mathbf{w}, X) p(\mathbf{w}) d\mathbf{w}`$ is the 
  - By the Bayes Rule we have the likelihood time the prior denoted as the joint distribution.
    - $`p(\mathbf{w}, \mathbf{y}\vert X) = p(\mathbf{y}\vert \mathbf{w}, X)p(\mathbf{w}) \quad (\because \mathbf{w}\perp X)`$
  - We denoted the joint distribution between $`\mathbf{w}`$ and $`\mathbf{y}`$ as   
    $`\begin{bmatrix} \mathbf{w}\\\mathbf{y} \end{bmatrix} \sim N\left(\begin{bmatrix} \mu\\X\mu \end{bmatrix} , \begin{bmatrix} \Sigma & \Sigma X^\top \\ X\Sigma & X\Sigma X^\top + \sigma_n^2 I \end{bmatrix} \right)`$
  - Thus, the joint distribution we derived above can be used to calculate the [conditional mean](02.md#concept-conditional-mean) and the [conditional variance](02.md#concept-conditional-variance) of $`\mathbf{w}`$ as below.
    - Conditional Mean
      - Formula) 
        - $`\mathbb{E}[\mathbf{x}_1 \vert \mathbf{x}_2] = \mu_1 + (\Sigma_{12}\Sigma_{22}^{-1})(\mathbf{x}_2 - \mu_2)`$
      - Derivation)   
        - $`\mu_{\mathbf{w}\vert\mathcal{D}} = \mu + (\Sigma X^\top)(X\Sigma X^\top + \sigma_n^2 I)^{-1}(\mathbf{y}-X\mu)`$
      - Further assuming that $`\mathbf{w}\sim N(0, I_d)`$, we may get
        - $`\mu_{\mathbf{w}\vert\mathcal{D}}  = X^\top(XX^\top + \sigma_n^2 I)^{-1}\mathbf{y}`$
          - which is in the Ridge Regression Form!
    - Conditional Variance
      - Formula) 
        - $`\mathbb{Var}[\mathbf{x}_1 \vert \mathbf{x}_2] = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}`$
      - Derivation)   
        - $`\Sigma_{\mathbf{w}\vert\mathcal{D}} = \Sigma - (\Sigma X^\top)(X\Sigma X^\top + \sigma_n^2 I)^{-1} X\Sigma`$
  - Hence, we may have the distribution of $`\mathbf{w}`$ as $`\mathbf{w} \sim N(\mu_{\mathbf{w}\vert\mathcal{D}}, \Sigma_{\mathbf{w}\vert\mathcal{D}})`$.
  - And the prediction that we make with this $`\mathbf{w}`$ on the test data $`X^*`$ will also follow the Gaussian distribution as
    - $`p(\mathbf{y}^* \vert X^*) \sim N(X^* \mu_{\mathbf{w}\vert\mathcal{D}}, X^* \Sigma_{\mathbf{w}\vert\mathcal{D}}{X^*}^\top)`$















<br><br>

[Back to Main](../main.md)