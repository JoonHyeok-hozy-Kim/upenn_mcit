[Back to Main](../main.md)

# Bayesian Optimization
### Concept) Iterative Optimization Algorithm
- Initial State)
  - $`\mathcal{D} = \{(x_1, y_1), \cdots, (x_n, y_n)\}`$ : given data
- Process)
  1. Determine $`x^* = \text{policy}(\mathcal{D})`$.
     - Meaning)
       - Determine what to do next based on the previous data.
     - e.g.)
       - Gradient Descent
         - Find the maximum gradient and proceed with that direction.
  2. Get $`y^* = \text{observe}(x^*)`$
     - Meaning)
       - Way to observe new data.
     - e.g.)
       - Bayesian Optimization 
         - $`y^*`$ is a Gaussian distribution, which is from the marginal of all other data points.
         - Recall that there is always the noise.
  3. Update $`\mathcal{D} \leftarrow \mathcal{D} \cup \{(x^*, y^*)\}`$
- Training)
  - Train a model on $`\mathcal{D}`$.
  - The objective is to get $`p(y^* \vert x^*, D)`$
    - Recall that $`y^*`$ is also a probability distribution.

<br>

### Concept) Acquisition Function
- Desc.)
  - A function that assigns a score to potential observation locations commensurate with their perceived ability to benefit the optimization process.
  - It tends to be cheap to evaluate with analytically tractable gradients.
  - Various acquisition functions are used in BO.
    - They all address the classic tension between [exploitation](#concept-exploitation) and [exploration](#concept-exploration).
  - The acquisition function vanishes at the location of the current observations
    - i.e.) No information gain.

#### Concept) Exploitation
- Sampling where the objective function is expected to be high

#### Concept) Exploration
- Sampling where we are uncertain about the objective function to inform future decisions

#### Concept) Trade-off between Exploitation and Exploration
- Acquisition function attains relatively large values both near... 
  - ([exploitation](#concept-exploitation)) : local maxima of the posterior mean
  - ([exploration](#concept-exploration)) : in regions with significant marginal uncertainty

<br>

### Bayesian Optimization Theory
- Settings)
  - $`\mathcal{A}`$ : an action space
  - $`\mathcal{X}`$ : an input space
    - i.e.) the set of all design variables
  - $`\mathcal{D} = \{(x_1, y_1), \cdots, (x_n, y_n)\}`$ : dataset
    - $`\forall i, y_i`$ is called the simple reward / simple regret.
      - Depending on the optimization problem, if it is the maximization (minimization) problem, the known label value is called the simple reward (regret).
  - $`u : \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}`$ : an utility function determining how good $`\mathcal{D}`$ is.
    - $`u(x,y\vert\mathcal{D}) = \begin{cases} y-y^+ & \text{if } y \gt y^+ \\ 0 & \text{otherwise} \end{cases}`$
      - where $`y^+`$ is my best observation so far.
      - Thus, we can gain utility only if the new observation $`y`$ has new information

#### Concept) Acquisition Function
- Derivation)
  - Let 
    - $`x^*`$ : the next promising point  
    - $`y^*`$ : the possible label value at $`x^*`$
  - Then, the optimal acquisition function will be
    - $`a(x^*, y^* \vert \mathcal{D}) = u(\mathcal{D} \cup (x^*, y^*)) - u(D)`$
  - But, the problem is that we do not know $`y^*`$.
  - Instead, we use
    - $`a(x^*\vert\mathcal{D}) = \mathbb{E}_{p(y^* \vert \mathcal{D}, x^*)} [a(x^*, y^* \vert\mathcal{D})]`$
  - How do we get $`x^*`$?
    - $`\displaystyle x^* = \arg\max_{x} \alpha(x^*\vert \mathcal{D})`$
- Types)
  - Knowledge Gradient
  - Mutual Information
  - Thomson Parallel Sampling

<br>

### Concept) Expected Improvement
- Derivation)
  - Suppose we got $`x^*`$ using the acquisition function.
  - What is the expected improvement that we get from that point?
    - $`\displaystyle \mathbb{E}_{p(y^* \vert \mathcal{D}, x^*)} [a(x^*, y^* \vert\mathcal{D})] = \int_{y^+}^\infty \underbrace{(y^*-y^+)}_{\text{liner}} \underbrace{p(y^* \vert x^* \mathcal{D})}_{\text{Gaussian!}}dy^*`$
      - This is cheap to calculate!








<br><br>

[Back to Main](../main.md)