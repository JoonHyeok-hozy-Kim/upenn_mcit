[Back to Main](../main.md)

Jan 23 

# Multivariate Gaussian Distribution
### Notation)
- $`\displaystyle N(a;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(a-\mu)^2}{2\sigma^2}\right)`$

### General Settings)
#### Gaussian Random Variables
- $`\mathbf{x}\sim N(\mu_\mathbf{x}, \Sigma_\mathbf{x})`$
- where $`\mathbf{x}\in\mathbb{R}^n`$
- $`\mathbf{y}\sim N(\mu_\mathbf{y}, \Sigma_\mathbf{y})`$
- where $`\mathbf{y}\in\mathbb{R}^n`$

#### Moment Generating Function (MGF)
- Def.)
  - $`M_\mathbf{x}(t) = \mathbb{E}[\exp(t^\top \mathbf{x})]`$
    - where $`t\in\mathbb{R}^n`$
- Prop.)
  - If two random variables have the same MGF, they have the same distribution.

<br>

### Thm.1) Affine transformations of Gaussians are Gaussian
- Thm.)
  - Suppose $`\exists A\in\mathbb{R}^{n\times n}, b\in\mathbb{R}^{n}`$ s.t. $`\mathbf{y} \triangleq A\mathbf{x} + b`$.
  - If $`\mathbf{x}\sim N(\mu_\mathbf{x}, \Sigma_\mathbf{x})`$, 
    - then $`\mathbf{y}\sim N(A\mu_\mathbf{x} + b, A \Sigma_\mathbf{x} A^\top)`$
- Pf.)
  - Then the first moment of the MGT of $`\mathbf{y}`$ goes   
    $`\begin{aligned}
        M_\mathbf{y}(t) 
        &= \mathbb{E}\left[ \exp(t^\top \mathbf{y}) \right] \\
        &= \mathbb{E}\left[ \exp(t^\top(A\mathbf{x} + b)) \right] \\
        &= \mathbb{E}\left[ \exp(t^\top A\mathbf{x} + t^\top b) \right] \\\
        &= \mathbb{E}\left[ \exp(t^\top A\mathbf{x}) \exp(t^\top b) \right] \\
        &= \mathbb{E}\left[ t^\top A\mathbf{x} \right] \cdot \exp(t^\top b) \\
        &= M_\mathbf{x}(A^\top t) \cdot \exp(t^\top b) & \cdots (A) \\
    \end{aligned}`$
  - Now, consider the second moment of $`M_\mathbf{x}(t)`$ that goes
    - $`\displaystyle M_\mathbf{x}(t) = \exp\left[ t^\top \mu_\mathbf{x} + \frac{1}{2}t^\top \Sigma_\mathbf{x} t \right]`$
  - Plugging this back into $`(A)`$, we may get   
    $`\begin{aligned}
        M_\mathbf{y}(t) 
        &= M_\mathbf{x}(A^\top t) \cdot \exp(t^\top b) \\
        &= \exp\left[ (A^\top t)^\top \mu_\mathbf{x} + \frac{1}{2}(A^\top t)^\top \Sigma_\mathbf{x} (A^\top t) \right] \cdot \exp(t^\top b) \\
        &= \exp\left[ (t^\top A) \mu_\mathbf{x} + \frac{1}{2}(t^\top A) \Sigma_\mathbf{x} (A^\top t) + t^\top b \right] \\
        &= \exp\left[ t^\top(A\mu_\mathbf{x} + b)  + \frac{1}{2}t^\top (A \Sigma_\mathbf{x} A^\top) t \right] \\
    \end{aligned}`$
  - $`\therefore \mathbf{y}\sim N(A\mu_\mathbf{x} + b, A \Sigma_\mathbf{x} A^\top)`$

<br>

### Thm.2) All multivariate normals are affine transformations of independent Gaussians
- Derivation)
  - Let $`z_1, \cdots, z_n \stackrel{\text{i.i.d.}}{\sim}N(0,1)`$.
  - Then $`\mathbf{z} = \begin{bmatrix} z_1 \\ \vdots \\ z_n \end{bmatrix} \sim N(\mathbf{0}, I_n)`$
  - Now, consider $`\mathbf{x}\sim N(\mu_\mathbf{x}, \Sigma_\mathbf{x})`$.
  - Since $`\Sigma_\mathbf{x}`$ is PSD, $`\exists A`$ s.t. $`\Sigma_\mathbf{x} = AA^\top`$.
    - e.g.) Using the Cholesky Decomposition, we may get   
      $`\begin{aligned}
        \Sigma_\mathbf{x} &= Q\Lambda Q^\top \\
        &= Q \Lambda^\frac{1}{2} \Lambda^\frac{1}{2} Q^\top \\
        &= (Q \Lambda^\frac{1}{2})  (Q\Lambda^\frac{1}{2})^\top \\
      \end{aligned}`$.
      - Thus, use $`A = Q \Lambda^\frac{1}{2}`$
  - Recall from [Thm.1](#thm1-affine-transformations-of-gaussians-are-gaussian) that   
    $`(A\mathbf{z} + \mu) \sim N(A\mathbf{0}+\mu, AIA^\top) = N(\mu, AA^\top) = N(\mu, \Sigma_\mathbf{x}) = \mathbf{x}`$
- Application)
  - How to sample $`\mathbf{x}\sim N(\mu_\mathbf{x}, \Sigma_\mathbf{x})`$.
    1. Get $`A`$ s.t. $`\Sigma_\mathbf{x} = AA^\top`$ using the Cholesky Decomposition.
    2. Sample $`\mathbf{z} \sim N(0,1)`$
       - e.g.) `torch.rand()`
    3. Affine transform $`\mathbf{z} \rightarrow \mathbf{x} = A\mathbf{z}+\mu_\mathbf{x}`$

<br>

### Thm.3) Sums and differences of independent Gaussians
- Settings)
  - Stack $`\mathbf{x}`$ and $`\mathbf{y}`$ as follows.
    $`\begin{bmatrix} \mathbf{x} \\ \mathbf{y} \end{bmatrix} \sim N\left( \begin{bmatrix} \mu_\mathbf{x} \\ \mu_\mathbf{y} \end{bmatrix}, \begin{bmatrix} \Sigma_\mathbf{x} & 0 \\ 0 & \Sigma_\mathbf{y} \end{bmatrix} \right)`$
- Sum)
  - Put $`A = \begin{bmatrix} 1 & 1\end{bmatrix}`$.
  - Then
    - $`A \begin{bmatrix} \mathbf{x} \\ \mathbf{y} \end{bmatrix} = \mathbf{x} + \mathbf{y}`$
    - $`\mathbf{x} + \mathbf{y} \sim N\left( A\begin{bmatrix} \mu_\mathbf{x} \\ \mu_\mathbf{y} \end{bmatrix}, A\begin{bmatrix} \Sigma_\mathbf{x} & 0 \\ 0 & \Sigma_\mathbf{y} \end{bmatrix}A^\top \right) = N\left( \mu_\mathbf{x} + \mu_\mathbf{y}, \Sigma_\mathbf{x} + \Sigma_\mathbf{y} \right)`$
- Difference)
  - Put $`A = \begin{bmatrix} 1 & -1\end{bmatrix}`$.
  - Then
    - $`A \begin{bmatrix} \mathbf{x} \\ \mathbf{y} \end{bmatrix} = \mathbf{x} - \mathbf{y}`$
    - $`\mathbf{x} + \mathbf{y} \sim N\left( A\begin{bmatrix} \mu_\mathbf{x} \\ \mu_\mathbf{y} \end{bmatrix}, A\begin{bmatrix} \Sigma_\mathbf{x} & 0 \\ 0 & \Sigma_\mathbf{y} \end{bmatrix}A^\top \right) = N\left( \mu_\mathbf{x} - \mu_\mathbf{y}, \Sigma_\mathbf{x} + \Sigma_\mathbf{y} \right)`$

<br><br>

### Thm.4) All marginal distributions of Gaussians are Gaussian
- Settings)
  - $`\mathbf{x}\sim N(\mu_\mathbf{x}, \Sigma_\mathbf{x})`$ where $`\mathbf{x}, \mu \in\mathbb{R}^n`$.
    - $`\mathbf{x} = \begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \end{bmatrix}`$ where $`\begin{cases} \mathbf{x}_1\in\mathbb{R}^{m} \\ \mathbf{x}_2 \in\mathbb{R}^{n-m} \end{cases}`$
    - $`\mathbf{x} = \begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \end{bmatrix} \sim N\left( \begin{bmatrix} \mu_\mathbf{1} \\ \mu_\mathbf{2} \end{bmatrix}, \begin{bmatrix} \Sigma_\mathbf{11} & \Sigma_\mathbf{12} \\ \Sigma_\mathbf{21} & \Sigma_\mathbf{22} \end{bmatrix} \right)`$
- Derivation)
  - Put $`A = [\underbrace{1 \quad \cdots \quad 1}_{m} \quad \underbrace{0 \quad \cdots \quad 0}_{n-m}]`$.
  - Then   
    $`A\begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \end{bmatrix} = \mathbf{x}_1 \sim N \left( A\begin{bmatrix} \mu_\mathbf{1} \\ \mu_\mathbf{2} \end{bmatrix}, A\begin{bmatrix} \Sigma_\mathbf{11} & \Sigma_\mathbf{12} \\ \Sigma_\mathbf{21} & \Sigma_\mathbf{22} \end{bmatrix}A^\top \right) = N(\mu_1, \Sigma_{11})`$
- Why does this matter?)
  - If we were to get a marginal distribution, we must have calculated   
    $`\displaystyle p(x_1, x_2, \cdots, x_m) = \int \cdots \int\int p(x_1, x_2, \cdots, x_m, x_{m+1}, \cdots,  x_n)dx_{m+1}dx_{m+2}\cdots dx_{n} = N(\mu_1, \Sigma_1)`$

<br>

### Thm.5) Conditional distributions of Gaussians
- Settings)
  - $`\mathbf{x} = \begin{bmatrix} \mathbf{x}_1 \\ \mathbf{x}_2 \end{bmatrix} \sim N\left( \begin{bmatrix} \mu_\mathbf{1} \\ \mu_\mathbf{2} \end{bmatrix}, \begin{bmatrix} \Sigma_\mathbf{11} & \Sigma_\mathbf{12} \\ \Sigma_\mathbf{21} & \Sigma_\mathbf{22} \end{bmatrix} \right)`$
  - $`A = -\Sigma_{12}\Sigma_{22}^{-1}`$
  - $`\mathbf{z} = \mathbf{x}_1 + A\mathbf{x}_2`$
- Derivation)
  - First we want to show that $`\mathbf{z}`$ is independent of $`\mathbf{x}_2`$.   
    $`\begin{aligned}
        \text{Cov}[\mathbf{z}, \mathbf{x}_2] &= \text{Cov}[\mathbf{x}_1 + A\mathbf{x}_2, \mathbf{x}_2] \\
        &= \text{Cov}[\mathbf{x}_1, \mathbf{x}_2] + \text{Cov}[A\mathbf{x}_2, \mathbf{x}_2] \\
        &= \Sigma_{12} + A\text{Cov}[\mathbf{x}_2, \mathbf{x}_2] \\
        &= \Sigma_{12} + (-\Sigma_{12}\Sigma_{22}^{-1})(\Sigma_{22}) \\
        &= 0 \\
    \end{aligned}`$ 
  - Now, let's derive the **conditional mean** $`\mathbb{E}[\mathbf{x}_1 \vert \mathbf{x}_2]`$.   
    $`\begin{aligned}
        \mathbb{E}[\mathbf{x}_1 \vert \mathbf{x}_2] &= \mathbb{E}[ \mathbf{z} - A\mathbf{x}_2 \vert \mathbf{x}_2] & (\because \mathbf{z} = \mathbf{x}_1 + A\mathbf{x}_2) \\
        &= \mathbb{E}[ \mathbf{z} \vert \mathbf{x}_2] - \mathbb{E}[ A\mathbf{x}_2 \vert \mathbf{x}_2] \\
        &= \mathbb{E}[ \mathbf{z} ] - \mathbb{E}[ A\mathbf{x}_2 \vert \mathbf{x}_2] & (\because \mathbf{z} \text{ is indep. of } \mathbf{x}_2) \\
        &= \mathbb{E}[ \mathbf{x}_1 + A\mathbf{x}_2 ] - A\mathbf{x}_2 & (\because \mathbf{E}[\mathbf{x}\vert \mathbf{x}] = \mathbf{x}) \\
        &= \mathbb{E}[ \mathbf{x}_1 ] + A(\mathbb{E}[\mathbf{x}_2] - \mathbf{x}_2) \\
        &= \mu_1 + A(\mu_2 - \mathbf{x}_2) \\
        &= \mu_1 + (-\Sigma_{12}\Sigma_{22}^{-1})(\mu_2 - \mathbf{x}_2) \\
        &= \mu_1 + (\Sigma_{12}\Sigma_{22}^{-1})(\mathbf{x}_2 - \mu_2) \\
    \end{aligned}`$
  - We can derive the **conditional variance** as follows.    
    $`\begin{aligned}
        \text{Var}[\mathbf{x}_1 \vert \mathbf{x}_2]
        &= \text{Var}[ \mathbf{z} - A\mathbf{x}_2 \vert \mathbf{x}_2] \\
        &= \text{Var}[ \mathbf{z} \vert \mathbf{x}_2] - \text{Var}[ A\mathbf{x}_2 \vert \mathbf{x}_2] \\
        &= \text{Var}[ \mathbf{z} \vert \mathbf{x}_2] & (\because \text{Var}[ A\mathbf{x}_2 \vert \mathbf{x}_2] = 0) \\
        &= \text{Var}[ \mathbf{z} ] & (\because \mathbf{z} \text{ is indep. of } \mathbf{x}_2) \\
        &= \text{Var}[ \mathbf{x}_1 + A\mathbf{x}_2 ] \\
        &= \text{Var}[ \mathbf{x}_1 ] + \text{Var}[ A\mathbf{x}_2 ] + A\text{Cov}[\mathbf{x}_1, \mathbf{x}_2] + \text{Cov}[\mathbf{x}_2, \mathbf{x}_1]A^\top \\
        &= \Sigma_{11} + A\Sigma_{22}A^\top + A\Sigma_{12} + \Sigma_{21}A^\top \\
        &= \Sigma_{11} + (-\Sigma_{12}\Sigma_{22}^{-1})\Sigma_{22}(-\Sigma_{12}\Sigma_{22}^{-1})^\top + (-\Sigma_{12}\Sigma_{22}^{-1})\Sigma_{12} + \Sigma_{21}(-\Sigma_{12}\Sigma_{22}^{-1})^\top \\
        &= \Sigma_{11} + \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12} - \Sigma_{21}\Sigma_{22}^{-1}\Sigma_{21}
    \end{aligned}`$


<br><br>

[Back to Main](../main.md)