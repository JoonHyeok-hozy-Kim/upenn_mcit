[Back to Main](../main.md)

 Jan. 21 Lecture Note

# Foundations of ML
- Topics
  1. [Distributions and Data]()
  2. [Outcome and Predictions]()
  3. [Models and Model Classes]()
  4. [Training and Testing]()
  5. [Learning = Generalization]()

<br>

## 1. Distributions and Data
### Definitions)
- Instance Space (Input Space) : $`X`$ 
- Outcome Space (Output Space) : $`Y`$ 
- Examples : $`\langle x,y \rangle \text{ where } x\in X, y\in Y`$
- Distributions : $`P`$ over $`X\times Y`$ where $`\langle x,y \rangle \stackrel{\text{i.i.d}}{\sim} P`$
- Sample : $`S = \{\langle x_1,y_1 \rangle, \cdots, \langle x_n,y_n \rangle\}`$ 
  - cf.) $`\vert S \vert = n`$

### Remarks)
- Instance Space $`X`$
  - Often, we will choose information/feature that goes into $`X`$.
  - Usefully choose features that are predictive/informative of $`y\in Y`$
  - Some features/information might favor (disfavor) certain individuals and groups.
    - e.g.) Zip code may disclose the race.
    - e.g.) Extracurricular activity may disclose the income level of a student.
  - Even obvious info/features may encode bias
- Outcome $`y\in Y`$
  - Sometimes the outcome y are objectively measurable and unbiased.
    - e.g.) stock ending price, temperature
  - Other times $`y`$ ("the ground truth") is more subjective.
    - Subjectivity on the answer.
      - e.g.) Are two face images are from a same person?
    - Sometimes $`y`$s are biased or filtered.
      - e.g.) Students at Penn with CGPA higher than 3.0 will be successful. 
        - People not applied to Penn and applied but rejected are all excluded. (Bias!)

<br>

## 2. Outcome and Predictions
- Our goal is to use sample $`S`$ to make predictions $`\hat{y}`$ for new $`x\notin S`$.
- Our prediction will be in the form of a model or a function $`h`$ s.t. $`\hat{y} = h(x)`$
- We allow/assume that $`P`$ might be **arbitrarily complex**.
  - cf.) Black box that we can not describe but can sample from this.
- But models $`h`$ will be simple by necessity.
  - cf.) Model should be able to compress the data!

<br>

## 3. Models and Model Classes
- Again, will generally choose "simple" models $`h(x)`$ based on $`\hat{\epsilon}(h)`$
- "Pre-identify" model class $`H`$
  - e.g.)
    - tabular data set : Logistic regression, Decision Tree
    - CV : CNN
- Hope that $`h\in H`$ with "small" $`\hat{\epsilon}(h)`$ will also have small $`\epsilon(h)`$.

<br>

## 4. Training and Testing
- Training Errors of $`h(x)`$ on $`S`$.
  - Binary Classification Model
    - Def.) $`\displaystyle \hat{\epsilon_S}(h) = \hat{\epsilon}(h)  \triangleq \frac{1}{n}\sum_{i=1}^n I[h(x_i)\ne y_i]`$
        - where $`I`$ is an indicator function s.t. $`I[\cdot] = \begin{cases} 1 & \text{if true} \\ 0 & \text{otherwise} \end{cases}`$
  - Regression Model
    - Def.) $`\displaystyle\hat{\epsilon}(h) = \frac{1}{n}\sum_{i=1}^n (h(x_i) - y_i)^2`$
- True Error (Test Error) of $`h(x)`$ on $`P`$
  - Def.) $`\epsilon_P(h) = \epsilon(h) \triangleq \mathbb{E}_{\langle x_1,y_1 \rangle\sim P} \left[ I[h(x_i)\ne y_i] \right] = \text{Pr}_{\langle x_1,y_1 \rangle\sim P}\left[ h(x_i)\ne y_i \right]`$

<br>

## 5. Learning = Generalization

<br>





<br><br>

[Back to Main](../main.md)