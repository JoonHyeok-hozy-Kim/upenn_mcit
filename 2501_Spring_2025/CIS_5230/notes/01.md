[Back to Main](../main.md)

 Jan. 21 Lecture Note

# Foundations of ML
- Topics
  1. [Distributions and Data](#1-distributions-and-data)
  2. [Outcome and Predictions](#2-outcome-and-predictions)
  3. [Models and Model Classes](#3-models-and-model-classes)
  4. [Training and Testing](#4-training-and-testing)
  5. [Learning = Generalization](#5-learning--generalization)

<br>

## 1. Distributions and Data
### Definitions)
- Instance Space (Input Space) : $`X`$ 
- Outcome Space (Output Space) : $`Y`$ 
- Examples : $`\langle x,y \rangle \text{ where } x\in X, y\in Y`$
- Distributions : $`P`$ over $`X\times Y`$ where $`\langle x,y \rangle \stackrel{\text{i.i.d}}{\sim} P`$
  - $`P`$ is possibly arbitrarily complex
- Sample : $`S = \{\langle x_1,y_1 \rangle, \langle x_2,y_2 \rangle, \cdots, \langle x_n,y_n \rangle\}`$ 
  - each $`\langle x_i,y_i \rangle \stackrel{\text{i.i.d.}}{\sim} P`$
  - cf.) $`\vert S \vert = n`$

### Remarks)
- Instance Space $`X`$
  - Often, we will choose information/feature that goes into $`X`$.
  - Usefully choose features that are predictive/informative of $`y\in Y`$
  - Some features/information might favor (disfavor) certain individuals and groups.
    - e.g.) Zip code may disclose the race.
    - e.g.) Extracurricular activity may disclose the income level of a student.
  - Even obvious info/features may encode bias
- Outcome $`y\in Y`$
  - Sometimes the outcome y are objectively measurable and unbiased.
    - e.g.) stock ending price, temperature
  - Other times $`y`$ ("the ground truth") is more subjective.
    - Subjectivity on the answer.
      - e.g.) Are two face images are from a same person?
    - Sometimes $`y`$s are biased or filtered.
      - e.g.) Students at Penn with CGPA higher than 3.0 will be successful. 
        - People not applied to Penn and applied but rejected are all excluded. (Bias!)

<br>

## 2. Outcome and Predictions
- Our goal is to use sample $`S`$ to make predictions $`\hat{y}`$ for new $`x\notin S`$.
- Our prediction will be in the form of a model or a function $`h`$ s.t. $`\hat{y} = h(x)`$
- We allow/assume that $`P`$ might be **arbitrarily complex**.
  - cf.) Black box that we can not describe but can sample from this.
- But models $`h`$ will be simple by necessity.
  - cf.) Model should be able to compress the data!
    - e.g.) Consider a model that keeps the data itself and return the label.
      - Terribly biased.

<br>

## 3. Models and Model Classes
- model : $`h(x)`$ based on $`\hat{\epsilon}(h)`$
  - We choose it.
  - Simple model!
- "Pre-identify" model class $`H`$
  - e.g.)
    - tabular data set : Logistic regression, Decision Tree
    - CV : CNN
- Hope that $`h\in H`$ with "small" $`\hat{\epsilon}(h)`$ will also have small $`\epsilon(h)`$.

<br>

## 4. Training and Testing
- Training Errors of $`h(x)`$ on $`S`$.
  - Binary Classification Model
    - Def.) $`\displaystyle \hat{\epsilon_S}(h) = \hat{\epsilon}(h)  \triangleq \frac{1}{n}\sum_{i=1}^n I[h(x_i)\ne y_i]`$
        - where $`I`$ is an indicator function s.t. $`I[\cdot] = \begin{cases} 1 & \text{if true} \\ 0 & \text{otherwise} \end{cases}`$
  - Regression Model
    - Def.) $`\displaystyle\hat{\epsilon}(h) = \frac{1}{n}\sum_{i=1}^n (h(x_i) - y_i)^2`$
- True Error (Test Error) of $`h(x)`$ on $`P`$
  - Def.) $`\epsilon_P(h) = \epsilon(h) \triangleq \mathbb{E}_{\langle x,y \rangle\sim P} \left[ I[h(x_i)\ne y_i] \right] = \text{Pr}_{\langle x,y \rangle\sim P}\left[ h(x_i)\ne y_i \right]`$

<br><br>

## 5. Learning = Generalization
#### Case Study 1: Decision Trees for Lending
- Goal)
  - Predict who will repay a loan.
- Settings)
  - $`y=\begin{cases}
      0 & \text{: default} \\ 1 & \text{: repaid}
  \end{cases}`$
  - $`x`$ might contain
    - credit history
    - current employment/salary
    - savings
    - Demographics : age, gender, race
    - social media usage
    - religion, politics
- Model)
  - Decision Tree
    - Prop.)
      - Depth = 3
    - e.g.) Nodes
      |||||||||||
      |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
      ||||||Income $`\ge`$ 50K|||||
      ||| y |||||| n ||
      |||Age $`\ge`$ 30||||||Employed?||
      ||   y   || n |||| y || n |
      ||$`\hat{y}=1`$||High school graduate?||||$`\hat{y}=1`$||$`\hat{y}=0`$|
      | |       | y |   | n |   |   |   |   |   |
      | |       |$`\hat{y}=1`$|   |$`\hat{y}=0`$||||||
      - cf.) Why not increasing depth to the maximum
        - Overfitting
          - Not available for the new data.
        - Computational complexity
          - Exponential number of nodes $`2^d`$ for the depth $`d`$.
      - cf.) How to choose good DT?
        - Calculate entropy of each test candidates.
          - $`H(p) = p\log_2\frac{1}{p} + (1-p)\log_2\frac{1}{1-p}`$
        - Choose the one with the maximum entropy.
        - This is a greedy method heuristic.
          - No guarantee that the result is the global optimum.

<br>

#### Case study 2: Neural Networks for Cat Detection
- Goal)
  - Determine whether there is a cat in an image.
- Settings)
  - $`y=\begin{cases}
      0 & \text{: no cat} \\ 1 & \text{: cat}
  \end{cases}`$
  - $`x`$ is a digital image in $`256\times 256`$ pixel format.
- Model) NN
  - Input : $`256\times 256`$ pixel values $`x_1, \cdots, x_{190K}`$
  - In each layer, some portion of inputs are connected.
    - e.g.) $`\displaystyle I\left(\sum_{i=1}^{100}w_ix_i \gt \theta \right)`$
      - In this case, we cannot differentiate. So, cannot apply gradient descent.
      - sigmoid function can be the substitute.

<br>

### Thm.) "Fundamental Theorem" of Machine Learning
- Thm)
  - No matter what $`P`$ looks like.
  - for any "reasonable" model class $`H`$
  - if we have "enough" data $`S`$
    - "enough" : $`n=\vert S\vert`$ is large compared to the "complexity" of $`H`$
      - "complexity" : may differ by model but the number of parameters can be a proxy for this.
  - then for every $`h\in H`$ we have 
    - $`\hat{\epsilon_S}(h) = \epsilon_P(h)`$ (a.k.a. Uniform Convergence)
- Special Case)
  - Case when the model class $`H`$ is finite (i.e. $`\vert H \vert`$ is finite)
    - Thm.)
      - If $`n = \frac{1}{\alpha^2}\log\vert H\vert`$, then $`\left\vert \hat{\epsilon_S}(h) - \epsilon_P(h) \right\vert \le \alpha, \; \forall h\in H`$
    - Prop.)
      - Consider that DT's nodes increase exponentially.
        - Due to $`\log\vert H \vert`$, the complexity becomes linear!
      - $`\log\vert H \vert`$ can be interpreted as the number of bits required to denote every possible models.
    - e.g.) Finite $`\vert H \vert`$
      - Binary Decision Tree : $`2^d-1`$ where $`d`$ is the depth of the DT.
    - Pf.)
      - Let $`\ell = \vert H \vert`$ where $`H = \{h_1, \cdots, h_\ell\}`$
        - Here, each $`h_i`$ forms every single possible models in $`H`$.
      - Consider the Bernoulli Experiment with $`n`$ trials $`\{b_1, \cdots, b_n\}`$, with the probability $`p`$.
        - Then the estimate of $`p`$ can be calculated as 
          - $`\displaystyle \hat{p} = \frac{1}{n}\sum_{i=1}^n b_i`$
        - Thm.) $`\forall\alpha \gt 0`$ s.t.
          - $`\text{Pr}[\vert\hat{p}-p\vert \gt \alpha] \le e^\frac{-\alpha^2 n}{3}`$
            - The simple version of the law of large numbers with the convergence rate!
            - Hoeffding's ~ (Chernoff Bound)
        - We will apply this to the accuracy of a model $`h\in H`$.
      - Fix a particular model $`h\in H`$.
      - Let $`p = \epsilon_P(h)`$
        - which is from $`\langle x, y \rangle \sim P`$
        - defining outcome : $`\begin{cases} 1 & \text{if } h(x)\ne y \\0 &\text{otherwise}\end{cases}`$
        - Then $`\text{Pr}_{\langle x_1, y_1 \rangle \cdots, \langle x_n, y_n \rangle \sim P}[\vert\hat{\epsilon_S}(h) - \epsilon_P(h)\vert \gt \alpha] \le e^\frac{-\alpha^2 n}{3}`$
      - Consider that for the event $`A,B`$.
        - Then $`P(A\vee B) \le P(A)+P(B)`$ : the union bound
      - Thus, by the union bound
        - $`\text{Pr}_S[\vert\hat{\epsilon_S}(h_1) - \epsilon_P(h_1)\vert \gt \alpha  \quad \vee \cdots \vee \quad \vert\hat{\epsilon_S}(h_\ell) - \epsilon_P(h_\ell)\vert \gt \alpha] \le \vert H\vert e^\frac{-\alpha^2 n}{3}`$ 
      - Consider the case that the number of samples is less than the number of features. Then, the sample may not represent the true $`P`$. Or, although $`n`$ is enough but the sample was not good enough by chance.
        - Set $`\delta \in [0,1]`$ s.t. $`\vert H\vert e^\frac{-\alpha^2 n}{3} \le \delta`$. Here, $`\delta`$ is the probability that the $`n`$-sized sample is not representative.
        - To get the enough size of $`n`$.
        - Putting log, we have
          - $`\ln\vert H\vert - \frac{\alpha^2 n}{3} \le \ln\delta`$
          - $`\Rightarrow \ln\vert H\vert + \ln\frac{1}{\delta} \le \frac{\alpha^2 n}{3}`$
          - $`\Rightarrow \frac{3}{\alpha^2}(\ln\vert H\vert + \ln\frac{1}{\delta}) \le n`$
        - In $`1-\delta`$ probability, the sample satisfies $`\left\vert \hat{\epsilon_S}(h) - \epsilon_P(h) \right\vert \le \alpha, \; \forall h\in H`$
    - Limit)
      - $`\vert H \vert`$ is not finite in reality.
        - Sol.)
          - VC Dimension
            - Complexity can be denoted as the features that can perfectly represent the labels.
            - Replace this with $`\ln\vert H \vert`$


<br>





<br><br>

[Back to Main](../main.md)