[Back to Main](../main.md)

# Ethical Algorithm Design and Generative AI
- Historically...
  - 90s, N-GRAM model

### Concept) Generative AI Concerns
- Bias / Stereotyping
- Privacy / Copyright / IP
- Toxicity
  - Generating offensive concepts
- Hallucination
  - Generating verifiably false
- Plagiarism / Cheating
- Disruption to work / labor / society
- Adversarial Attack
  - e.g.) Adding human unrecognizable but machine recognizable data to generate false inference. (Traffic light - red)
- Security

<br><br>

### Concept) Neural Network Background
- Single Neuron
  - Input : $`x\in\mathbb{R}^d`$
  - Output : $`f(w\cdot x) = f\left(\sum_{i=1}^d w_i x_i \right)`$ 
    - where 
      - $`w\in\mathbb{R}^d`$ is the vector of weights
      - $`f(\cdot)`$ is an activation function
        - e.g.) Sigmoid 
- Training Process
  - Loss Function : $`\displaystyle\ell(w) = \sum_{\langle x,y \rangle \in S} \left( f(w\cdot x) - y\right)^2`$
    - cf.) Differentiable -> Apply gradient descent to locally minimize the loss (error)
- Multiple Neurons / Layers
  - Multiple neurons.
  - The outputs of each neuron is again fed to new layer of neurons
  - e.g.) Pyramid Like Structure
    - $`w^1 \cdots w^k`$ : $`k`$ neurons where $`o_j = f(w^j x)`$
    - $`y = f(w^{k+1} o)`$ where $`o = [o_1, \cdots, o_k]`$
  - Local Minimization
    - Chain Rule -> Backpropagation
  - Key Idea)
    - Neuronal Selectivity
      - Intermediate neuron can learn new or better or higher level feature that re-represents the features in the previous layer.
- Auto-Encoder
  - Input and Output dimensionality are identical.
  - Smaller layers in the middle.
    - Dimensionality reduction
- RNN

<br><br>

### Concept) Biased / Stereotyped / Offensive / Toxic / Dangerous / Illegal Contents
- Problems)
  - Bias vs Correlation
    - Bias : stereotype 
      - e.g.) 
        - Nurse - Female 
        - Engineer - Male
    - Correlation : on average similar
  - Toxic/Offensive vs Quotation
    - What if a quotation from a well known source is toxic?
      - Should we prevent it?
- Mitigations
  - Curation of training
    - How?)
      - Remove problematic data at first.
    - Problem)
      - But current LLM big techs don't even know what's in their data.
  - Guardrail Model
    - How?)
      - Ask the prompt/output of the LLM and ask human to rate the problem.
      - Apply the result back to the prompt and output.
    - Problem)
      - Making annotation is difficult.
        - Ask subjective opinion?
        - How to get the general point of view??

<br><br>

### Concept) Privacy / Copyright / Intellectual Property (IP) Notation / Stylistic Inference (지프리 풍)
- Program)
  - Private Data
    - e.g.) Give "A"'s address
  - Copyright
    - New York Times article
  - How can you define the style?
    - What if a group of authors are sharing the same style?
- Mitigations)
  - Policy / Legal / Regulatory Approach
  - DP model training
  - Machine Unlearning
    - Intentionally delete certain data or prevent the inference to include certain data.
    - Technical Approach)
      - Suppose the initial model was trained containing the data.
      - Law enforced to get rid of that data?
      - How to train from the initial model to behave as if that data was not included?
        - Should be efficient than training from the scratch
  - Data Sharding
    - How?)
      - Partition the data and train each model on them.
      - Train the final model using the ensemble of those models.
    - Prop.)
      - If someone asks to delete the data, find a model and remove from the ensemble.
    - Problem)
      - How many partition? sharding?

<br><br>

### Concept) Adversarial Attacks
- e.g.)
  - Image manipulation (합성, 이미지 조작)
  - Data poisoning
    - Inject poisonous data to the training data
  - Spam filter attacks
    - Try sending all kinds of random string and test which one passes the filter
- Mitigation)
  - Robust adversarial training
    - Instead of e.g. minimizing the squared loss $`\sum_i (h(x_i) - y_i)^2`$
      - minimize $`\displaystyle\max_{\delta : \Vert \delta \Vert \le \alpha} \sum_i (h(x_i + \delta_i) - y_i)^2`$
        - i.e.) Perturbation : Manipulate the input a little.
          - The manipulation $`\delta`$ can be the adversarial attack.
    - Prop.)
      - The resulted model may be robust to the manipulation

<br><br>

### Concept) Plagiarism / Cheating
- e.g.)
  - College application essays
  - Writing samples in job application
- Problem)
  - Should we think of it as the tool or cheating?
- Mitigations)
  - Training model to distinguish between human generated and LLM generated content.
    - This is an arm race.
    - LLM will try to avoid this.
  - Watermarking
    - Images
      - Add machine readable data to the image.
    - NLP
      - Recall that LLMs draw tokens from the random distribution.
      - What if we force the model to choose from the odd number tokens.

<br><br>

### Concept) Hallucination
- Def.)
  - Plausible but verifiably false output
- e.g.)
  - scientific / academic citation
  - IMDb
- Mitigations)
  - RAG
  - Kalai & Vempala
    - Word Imbedding before the LLM
      - e.g.) co-occurrence of words
    - Schema : tuple of words that forms a fact
    - Huge space of "factoids"
      - very small subset of facts
    - Training data is a sample of actual facts.
    - Mitigation)
      - Only generate tuples from training data would be safe.
    - But we want to generalize...

<br><br>

### Question) How many facts are missing from our training data?
- One way)
  - Want to know the total population of trout
    - Catch and mark.
    - Later, the ones' that already have mark will be caught.
  - Good-Turing
    - Let $`\tau = \frac{\text{\# of facts occurring exactly once in training data}}{\text{size of data}}`$
    - Then $`\left\vert P(\text{next sample is new fact}) - \tau \right\vert \le \frac{1}{\sqrt{n}}`$
  - Kalai-Vempala
    - for any model
      - $`\text{(hallucination rate)} \ge \tau - (\text{model miscalibration})`$
        - miscalibration is one measure of overfitting.
        - 

<br>



<br><br>

[Back to Main](../main.md)