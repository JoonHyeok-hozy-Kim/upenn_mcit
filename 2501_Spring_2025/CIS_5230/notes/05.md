[Back to Main](../main.md)

# 5. Security and Privacy


- Brainstorming)
  - What is Privacy?
    - Right to be forgotten
      - cf.) What about a model trained on your data?
        - Membership Inference Attack
    - Control of access
    - My data not sold for profit
      - e.g.) Google, Facebook
        - Advertising : Business model that may sell data outside
      - e.g.) Apple
        - Consumer Electronics : Not selling data outside
      - e.g.) Amazon
        - Cloud
        - Consumer Service
    - Anonymity (Personally Identifiable Information, PII)
    - Encryption (Secrecy)
      - e.g.)
        - Privacy of existence of communication
    - Consent to minors
    - Transparency of data usage / exchange / dissemination
      - Opt in/out
    - Privacy jurisdiction / laws

<br>

## Analysis) Privacy vs Security
### Concept) Security
- Def.) Control of access to "raw" data
- Prop.)
  - Largely the domain of cryptography
    - e.g.) locks and keys

<br>

### Concept) Privacy
  - Def.) Allow certain uses of data but control inferences & exfiltrations 
  - Prop.)
    - Largely the domain of anonymization, differential privacy
    - Used to be highly concentrated in anonymization in the past...
      - But vulnerable...
        - whack a mole game : offense vs defense
- Toy Example)
  - Sensitive / private database (my data is a row.)
    - Security Concern) 
      - Control access (i.e. keep locked!)
  - Suppose we used this data to train a disease prediction model.
    - i.e.) Derivative byproduct of the data.
  - Then, suppose we share this model in public.
    - Privacy Concern) 
      - What might the model leak about the data?

<br>

### Concept) Cryptography
- Prop.)
  - Indistinguishable with the random
  - Only the legitimate party can decrypt.
- Models)
  - [One-time Pad](#model-one-time-pad-long-time-ago) (Long time ago...)
  - [Public Key Cryptography](#model-public-key-cryptography-late-1970s) (late 1970's)

#### Model) One-time Pad (Long time ago...)
- Desc.)
  - Suppose I want to send you a 1-bit message, $`a\in\{0,1\}`$
  - We meet beforehand and choose a random bit $`b\in\{0,1\}`$ with the prob. of 50%.
    - Here, $`b`$ is the randomly generated key that will be shared.
    - We will encrypt $`a`$ using $`b`$ as $`c= \underbrace{a\oplus b}_{\text{XOR}} = \begin{cases} 0 & a=b \\ 1 & a\ne b \end{cases} = (a+b) \text{ mod } 2`$
  - Later, I send you $`c = \underbrace{a\oplus b}_{\text{XOR}} = \begin{cases} 0 & a=b \\ 1 & a\ne b \end{cases} = (a+b) \text{ mod } 2`$.
    - This $`c`$ seems to be a perfectly random bit due to $`b`$.
  - You can decrypt $`a`$ with $`c\oplus b = (a\oplus b)\oplus b = a \oplus (b\oplus b) = a\oplus 0 = a`$
- Prop.)
- Drawbacks)
  - Keys must have same length as messages.
  - Cannot be reused.
    - Suppose we had already used $`a\oplus b`$.
    - We want to send a new message using $`a'\oplus b`$.
    - In adversary's perspective, they may get
      - $`(a\oplus b)\oplus(a'\oplus b) = (a\oplus a')\oplus(b\oplus b) = (a\oplus a')\oplus 0 = a\oplus a'`$
        - They know the relation between $`a, a'`$.
        - If $`a`$ already happened, the adversary can use this info and guess $`a'`$.
  - Every message requires fresh pad and key pair.
  - Every pair of communicating individual require a separate key.

<br>

#### Model) Public Key Cryptography (late 1970's)
- Idea.)
  - Pseudo random generator
    - short string input -> long string output
      - e.g.) $`2^{100}\rightarrow 2^{200}`$, the ratio goes $`\frac{1}{2^{100}}`$ : almost random due to the lack of the computational power, although it is deterministic.
    - Longer the key, more safe!
- Desc.)
  - Separate key into...
    - public encryption key $`(e)`$
      - Encryption Function $`f_e(x)`$ where $`x`$ is the message.
        - Prop.)
          - Here $`f_e(x)`$ is indistinguishable to the random string to the 3rd party with the bounded computational power!
          - Not a perfect security, but the computational complexity allows that.
    - private decryption key $`(d)`$
      - Decryption Function $`g_d(f_e(x)) = x`$ 
- e.g.)
  - Prime number multiplication
    - Easy one
      - Given prime numbers, return the multiplication of them.
    - Hard one
      - Given a long integer, get prime numbers that are the divisors.

<br><br>

## Concept) Three Notions of Privacy
1. [Anonymization Technologies]()
2. [Differential Privacy]()
3. [No Harm Whatsoever]()

<br>

#### Concept) Fingerprints
- Prop.)
  - Uniquely identifies an individual.
- e.g.)
  - CD's in 80s
    - Series of length of songs can work as the unique key for the index.
  - Fonts in Browsers
    - The sequence of fonts with the time that are downloaded
  - Geolocation data

<br>

#### Concept) Triangulations
- Desc.)
  - Combine other data to crash the anonymity of the given dataset.
  - Linkage attack
- e.g.)
  - Latanya Sweeny
    - Combined anonymized the MA medical data with other data and targeted the Governor William Well(?)
  - Netfilx Prize
    - Combining the anonymized playlist DB with the IMDb, researchers matched a row of a user with the IMDb id.

<br>

#### Concept) ? (Question Mark)
- e.g.)
  - Strava app
    - Clustered records in Afghanistan
      - Soldiers in the secret military base.
  - Golden State Killer
    - A serial killer in Sacramento, CA.
    - Had the DNA but no match.
    - Uploaded this DNA to the 23andMe and found cousins.
      - Although the killer's data was not on the DB, it shared props with others!
      - Not exclusive data of an individual is exposed but disclosed!



<br>

### Concept) Anonymization Technologies
- Problem Setting)
  - Not private enough, but "too" useful
    - Not enough privacy, too much utility case!
- Basic Idea)
  - $`\mathcal{D}`$ : some sensitive/private dataset
    - cf.) Assume to be a tabular data structure.
  - $`\mathcal{D}'`$ : the anonymized version of $`\mathcal{D}`$.
    - The transformation $`\mathcal{D}\rightarrow\mathcal{D}'`$ is achieved by two operations:
      - Reduction : remove/delete entire columns (e.g. [PII](#concept-privacy))
      - Coarsening : reducing the resolution/accuracy of a column
        - e.g.) Bucket information
          - 28 years old -> 20-29 range.
- Props.)
  - This technique is useful but cannot 100% trusted!
- Goal)
  - Suggested by multiple people
  - e.g.)
    - K-anonymity
      - Desc.)
        - Use reduction/coarsening until $`\mathcal{D}'`$ has property that any row of $`\mathcal{D}'`$ has $`\ge K`$ copies.
      - Limit)
        - If not enough reduction/coarsening is done, it is vulnerable to triangulation.
        - If too much is made, the data becomes useless.

<br>

### Concept) Differential Privacy
- Problem Setting)
  - Strong privacy, strong utility

<br>

### Concept) No Harm Whatsoever
- Problem Setting)
  - Too private enough, not so useful






<br><br>

[Back to Main](../main.md)