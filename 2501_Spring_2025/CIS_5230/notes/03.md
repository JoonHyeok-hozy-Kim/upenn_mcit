[Back to Main](../main.md)

# 3. Science of Fair ML: Models and Algorithms
Bias Mitigation

<br>

### Concept) Causes of Biases (Ways things can go wrong)
- Domain Specific Problems
  - Types)
    - Have much less data on some subgroup(s)
      - According to the [Fundamental Thm of ML](01.md#thm-fundamental-theorem-of-machine-learning), estimated error will not be close to the actual error from the true distribution.
      - Case when distribution between subgroups are different, and have plenty of data on certain subgroups, then the estimation cannot represent the whole population.
    - Features (missing features) favor/disfavor certain groups
      - e.g.) After school activity can reveal racial, financial status
      - Sol.) Add features
    - Some subgroups might be inherently harder to predict.
      - e.g.) 
        - First face recognition models were trained only with white male image data. 
        - Thus, it didn't work for black females.
    - Data might be biased in first place.
  - Prop)
    - Above problems can be exacerbated by the training process itself.
- Problem with model it self.
  - Model (training process) can somehow mitigate those domain specific problems.
- Feedback loop
  - e.g.) California used a system that send more police to area with high criminals. Then, due to the increased officers, more things are found.

<br>

### Concept) Fairness in ML
- Def.)
  - "Fairness" is typically a property of a **model**.
    - Here, the model refers to the result of the training process.
- Multiple types of fairness
  - [Group Fairness](#concept-group-fairness) (most common and covered in this class)
    - e.g.)
      - error in certain group
      - FPR, TPR in certain group
  - Individual Fairness (mainly philosophical)
    - e.g.) If similar input is given, similar output should come out.
      - $`\vert h(x_1) - h(x_2) \vert \le c\cdot d(x_1, x_2), \forall x_1,x_2\in\mathcal{D}`$
        - where $`c\in\mathbb{R}`$, $`d(x_1, x_2)`$ is a distance between two points.
          - Problems)
            - How to define $`d(x_1, x_2)`$ : philosophical and statistics cannot explain much
            - If $`d(x_1, x_2)`$ for $`\forall x_1, x_2`$ is added as the constraint to the optimization problem, complexity become $`O(n^2)`$. Too expensive.
  - Interpolations between groups and individuals
    - Although the model is fair by certain groups or individuals, it may not be fair for the mixture of those.
      - e.g.) Hispanic women earning 10K annually living in Philly...
  - Causal definition of Fairness
    - e.g.) Why certain group is underestimated?
    - Problem) Hard to prove the causal relationship. Cannot control!
  - Fair representation

<br>

### Concept) Group Fairness
- Desc.)
  - In this class we use **group fairness**!
- e.g.) Similar errors between the groups that we set up.
  - Problem with this)
    - To achieve group fairness, more constraint must be given to the optimization.
    - Then, some possible models that we could have achieved may be discarded due to the constraints.
    - We may discard possible models that are best for the global error.
      - e.g.) Consider the college admission among two groups.   
        ![](../images/notes/03_001.png)
        - Settings)
          - Two groups colored in red and green.
          - In each group O denotes the students who should get admitted, and X are the opposite.
        - Resulting Classifiers)
          - Global optimum classifier but biased against green group.
          - Another one accomplishes the Group fairness but more error than global optimum
        - Sol.)
          - Best way to solve this is add feature that differentiate two groups.
          - But in social justice, the disclosure of some background information is regulated.
            - e.g.) Race

#### Concept) Giving randomness
- How)
  - Suppose we are classifying with flip coin, 0.5 probability.
    - if $`y=0`$ then false negative is 0
    - if $`y=1`$ then 0.5 chance of false negative.
      - i.e.) $`P(\hat{y}=0\vert y=1) =0.5`$
- e.g.) Lottery
  - This is tolerable because people are indifferent about the result of the lottery.
- In CS, algorithms utilize randomness for reasons.
  - Faster speed or security algorithms.
  - i.e.) they are valuable.
    - But would people allow this?

<br>

### Tech.) Bolt-on Mitigation with Selective Randomness
- Desc.)
  - Assume the trained model is given.
  - [Give randomness](#concept-giving-randomness) to mitigate the bias in the model and promote fairness.
- Settings)
  - Given trained classifier $`\hat{y} = h(x)`$ 
    - "bolt-on" fairness, post-processing 
      - i.e.) This model didn't care about fairness at first, and fix the fairness issue afterwards.
  - $`A, B`$ : two subgroups
    - Mutual exclusiveness : $`A\cap B = \emptyset, A\cup B = \mathcal{X}`$
    - $`\epsilon_A, \epsilon_B`$ : Given classification error of the trained model $`h`$ on $`A,B`$.
      - i.e.) $`\epsilon_i(h) = \text{Pr}[h(x)\ne y \vert x\in i], i\in\{A,B\}`$
  - Unfairness in the existing model
    - $`\epsilon_A \lneq \epsilon_B \lneq 0.5`$
      - $`\lneq`$ means strictly less than
      - $`h`$ is unfair on the point that $`\epsilon_A \lneq \epsilon_B`$
- Proposal) Selective Randomization $`\tilde{h}(x)`$
  - Why doing this?)
    - The classifier was given!
    - Only thing we can do is improving it by adding some randomness.
  - New error rates after the randomization
    - Subgroup $`B`$
      - No randomness
        - i.e.) $`x\in B`$, use $`\hat{y} = h(x)`$
        - Thus, $`\widetilde{\epsilon_B} = \epsilon_B`$
    - Subgroup $`A`$
      - Apply randomness with the probability $`q_A`$.
        - How) If $`x\in A`$, 
          - $`\tilde{h}(x) = \begin{cases} 0.5 & \text{with prob. } q_A \\ h(x) & \text{with prob. } 1-q_A \end{cases}`$
            - i.e.) with the probability of $`q_A\in[0,1]`$, $`\hat{y} = \text{flip coin} = 0.5`$
      - Thus,   
        $`\begin{aligned}
          \widetilde{\epsilon_A} &= q_A \cdot (\text{flip coin}) + (1-q_A) \cdot \epsilon_A \\
          &= q_A \cdot 0.5 + (1-q_A) \cdot \epsilon_A \\
        \end{aligned}`$ 
  - Fairness Rules)
    - $`\widetilde{\epsilon_B} = \widetilde{\epsilon_A}`$ : equalizing the error rate among groups
      - Then, we can get the optimal probability of applying randomness to the subgroup $`A`$ given the existing model $`h`$.
        - $`\displaystyle q_A^* = \frac{\epsilon_B - \epsilon_A}{0.5-\epsilon_A}`$
          - e.g.)
            - $`\epsilon_A = 0.05, \epsilon_B = 0.2`$
              - $`q^* = 0.33`$
            - $`\epsilon_A = 0.1, \epsilon_B = 0.2`$
              - Bigger the gap $`(\gamma)`$ between two subgroups, bigger fraction $`(q_A)`$ is needed for the equalization.
    - $`\widetilde{\epsilon_A} = \widetilde{\epsilon_B} - \gamma, \; \exists\gamma\gt0`$ (Weakening the fairness demand)
      - $`\gamma = \widetilde{\epsilon_B} - \widetilde{\epsilon_A} \in [0, \epsilon_B-\epsilon_A]`$ denotes the unfairness, allowed difference between the groups
        - $`\gamma = 0`$ : absolute (maximum) fairness
        - $`\gamma = \epsilon_B - \epsilon_A`$ : no fairness ($`\widetilde{\epsilon_A} = \epsilon_A`$)
      - Then $`\displaystyle q^* = \frac{\epsilon_B - \gamma - \epsilon_A}{0.5-\epsilon_A}`$
        - Analysis) Flipping coin less compared to the equal error case.
        - $`\displaystyle \frac{\partial q^*}{\partial \gamma} \lt 0`$
- Calculating the Overall Error
  - Suppose the population of the subgroups are $`p_A, p_B`$.
    - $`p_i = P_x(x\in i), i\in\{A,B\}`$
    - $`p_A + p_B = 1`$ (normalized!)
  - Consider the above setting that uses $`\gamma`$ to denote the unfairness,
    - $`\widetilde{\epsilon_A} = \epsilon_B - \gamma`$
  - Then the overall error goes   
    $`\begin{aligned}
      \widetilde{\epsilon} &= p_A \widetilde{\epsilon_A}+p_B \widetilde{\epsilon_B} \\
      &= p_A (\epsilon_B - \gamma)+p_B \epsilon_B & (\because \widetilde{\epsilon_A} = \epsilon_B - \gamma, \widetilde{\epsilon_B} = \epsilon_B) \\
      &= -p_A \gamma + (p_A + p_B)\epsilon_B \\
      &= -p_A \gamma + \epsilon_B & (\because p_A + p_B=1) \\
    \end{aligned}`$
    - e.g.)
      - $`\epsilon_A = 0.05, \epsilon_B = 0.2, p_A = 0.7, \gamma = 0.03`$
        - $`q^* = \frac{0.2-0.05-0.03}{0.5-0.05} \approx 0.267`$
        - $`\tilde{\epsilon} = 0.2 - 0.7\cdot0.03 = 0.179`$
  - Plotting this on the $`\tilde{\epsilon}-\gamma`$ plane
    - Recall that $`\begin{cases} \tilde{\epsilon} = p_A \widetilde{\epsilon_A} + p_B \widetilde{\epsilon_B} \\ \gamma = \widetilde{\epsilon_B} - \widetilde{\epsilon_A} \end{cases}`$
    - Linear trade-off between $`\gamma`$ and overall error rate.
    - This line is called the "Pareto Frontier" between overall error and fairness 
      - Each model has different plots.
      - If a line of a model is closer to the origin, we can achieve the pareto improvement by changing into that model.
    - Each rule corresponds to each point on the frontier.
    - This is a bolt-on frontier.
      ![](../images/notes/03_002.png)
      - Slope : $`\displaystyle\frac{\epsilon_B - \epsilon_A}{\epsilon_B - \tilde{\epsilon}}`$ : given by the model $`h`$
    - Hypothetical Optimal models in a model class $`H`$
      - Convex curve the connects the vertices.
      - Not available in reality.
      - $`H`$-frontier
        ![](../images/notes/03_003.png)
        - Some possible selections that can be pareto improved.
- Applying to other statistics
  - False Positive
    - $`\epsilon_i^{FP} = \text{Pr}_{\langle x,y\rangle\sim P}\left[ h(x)=1 \vert x\in A, y=0 \right]`$
  - False Positive in this setting $`(FP_A)`$
    - $`FP_A(h) = P(h(x)=1\vert y=0, x\in A)`$
    - Suppose $`FP_{A} \lneq FP_B \leq 0.5`$
    - Same $`\tilde{h}`$ as above.
    - Then $`\widetilde{FP_A} = (1-q)FP_A + q\cdot 0.5`$
    - Rule : $`\widetilde{FP_A} = FP_B`$
      - Then $`\displaystyle q^* = \frac{FP_B - FP_A}{0.5 - FP_A}`$
      - Using this $`q^*`$, the overall error rate can be calculated as
        - $`\tilde{\epsilon} = p_A((1-q)\epsilon_A + q\cdot 0.5) + p_B \epsilon_B = p_A \epsilon_A + p_B \epsilon_B + p_A q (0.5-\epsilon_A)`$
        - Intuition)
          - Trying to equalize the FP resulted in rasing the overall error rate.
            - Recall that $`\epsilon = FP+FN`$
          - In case we do not know the subgroup and we try to equalize the false positive rate, other measures such as **false negative rate** or overall error rate will increase.
- [Hardt et al.](../readings/03/04.md) in this framework.
  |Existing Estimation / Group|A|B|
  |:-:|:-:|:-:|
  |$`h(x)=0`$|$`r`$|$`s`$||
  |$`h(x)=1`$|$`t`$|$`u`$||
  - Above is the amount of randomness we will take for each situation. (not a confusion matrix)
  - $`\widetilde{h}(x)`$ : given $`x`$, compute $`h(x)`$ for each $`x\in A, x\in B`$
    - With $`p\in\{r,s,t,u\}\subset[0,1]`$ (just like $`q_A`$ above)
      - use $`h(x)`$ with the probability $`1-p`$
      - flip coin with the probability $`p`$
    - For each rule, we may get $`r^*,s^*,t^*,u^*`$
<br>



<br><br>

[Back to Main](../main.md)