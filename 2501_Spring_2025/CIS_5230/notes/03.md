[Back to Main](../main.md)

# 3. Science of Fair ML: Models and Algorithms
Bias Mitigation

<br>

### Concept) Causes of Biases (Ways things can go wrong)
- Domain Specific Problems
  - Types)
    - Have much less data on some subgroup(s)
      - According to the [Fundamental Thm of ML](01.md#thm-fundamental-theorem-of-machine-learning), estimated error will not be close to the actual error from the true distribution.
      - Case when distribution between subgroups are different, and have plenty of data on certain subgroups, then the estimation cannot represent the whole population.
    - Features (missing features) favor/disfavor certain groups
      - e.g.) After school activity can reveal racial, financial status
      - Sol.) Add features
    - Some subgroups might be inherently harder to predict.
      - e.g.) 
        - First face recognition models were trained only with white male image data. 
        - Thus, it didn't work for black females.
    - Data might be biased in first place.
  - Prop)
    - Above problems can be exacerbated by the training process itself.
- Problem with model it self.
  - Model (training process) can somehow mitigate those domain specific problems.
- Feedback loop
  - e.g.) California used a system that send more police to area with high criminals. Then, due to the increased officers, more things are found.

<br>

### Concept) Fairness in ML
- Def.)
  - "Fairness" is typically a property of a **model**.
    - Here, the model refers to the result of the training process.
- Multiple types of fairness
  - [Group Fairness](#concept-group-fairness) (most common and covered in this class)
    - e.g.)
      - error in certain group
      - FPR, TPR in certain group
  - Individual Fairness (mainly philosophical)
    - e.g.) If similar input is given, similar output should come out.
      - $`\vert h(x_1) - h(x_2) \vert \le c\cdot d(x_1, x_2), \forall x_1,x_2\in\mathcal{D}`$
        - where $`c\in\mathbb{R}`$, $`d(x_1, x_2)`$ is a distance between two points.
          - Problems)
            - How to define $`d(x_1, x_2)`$ : philosophical and statistics cannot explain much
            - If $`d(x_1, x_2)`$ for $`\forall x_1, x_2`$ is added as the constraint to the optimization problem, complexity become $`O(n^2)`$. Too expensive.
  - Interpolations between groups and individuals
    - Although the model is fair by certain groups or individuals, it may not be fair for the mixture of those.
      - e.g.) Hispanic women earning 10K annually living in Philly...
  - Causal definition of Fairness
    - e.g.) Why certain group is underestimated?
    - Problem) Hard to prove the causal relationship. Cannot control!
  - Fair representation

<br>

### Concept) Group Fairness
- Desc.)
  - In this class we use **group fairness**!
- e.g.) Similar errors between the groups that we set up.
  - Problem with this)
    - To achieve group fairness, more constraint must be given to the optimization.
    - Then, some possible models that we could have achieved may be discarded due to the constraints.
    - We may discard possible models that are best for the global error.
      - e.g.) Consider the college admission among two groups.   
        ![](../images/notes/03_001.png)
        - Settings)
          - Two groups colored in red and green.
          - In each group O denotes the students who should get admitted, and X are the opposite.
        - Resulting Classifiers)
          - Global optimum classifier but biased against green group.
          - Another one accomplishes the Group fairness but more error than global optimum
        - Sol.)
          - Best way to solve this is add feature that differentiate two groups.
          - But in social justice, the disclosure of some background information is regulated.
            - e.g.) Race

### Sol.) How to mitigate the bias
#### Giving randomness
- Concept) Randomness
  - Suppose we are classifying with flip coin, 0.5 probability.
    - if $`y=0`$ then false negative is 0
    - if $`y=1`$ then 0.5 chance of false negative.
      - i.e.) $`P(\hat{y}=0\vert y=1) =0.5`$
  - e.g.) Lottery
    - This is tolerable because people are indifferent about the result of the lottery.
  - In CS, algorithms utilize randomness for reasons.
    - Faster speed or security algorithms.
    - i.e.) they are valuable.
      - But would people allow this?
- Application 
  - Bolt-on Mitigation
    - Settings)
      - Trained classifier $`\hat{y} = h(x)`$ is given!
        - "bolt-on" fairness, post-processing 
          - i.e.) This model didn't care about fairness at first, and fix the fairness issue afterwards.
      - $`A, B`$ : two groups
        - $`\epsilon_A, \epsilon_B`$ : Given classification error of the trained model $`h`$ on $`A,B`$.
          - e.g.) $`\epsilon_i(h) = \text{Pr}[h(x)\ne y \vert x\in i], i\in\{A,B\}`$
      - Suppose $`\epsilon_A \lneq \epsilon_B \lneq 0.5`$
        - $`\lneq`$ means strictly less than
        - $`h`$ is unfair on the point that it is worse than flipping coin
    - Proposal) Selective Randomization $`\tilde{h}(x)`$
      - Why doing this?)
        - The classifier was given!
        - Only thing we can do is improving it by adding some randomness.
      - Deriving $`\widetilde{\epsilon_A}`$ with some randomization of $`q_A`$
        - If $`x\in B`$, use $`\hat{y} = h(x)`$
        - If $`x\in A`$, 
          - with the probability of $`q_A`$, $`\hat{y} = \text{flip coin}`$
            - Here, flipping coin can achieve $`0.5`$
          - with the probability of $`1-q_A`$, $`\hat{y} = h(x)`$
        - Put $`\widetilde{\epsilon_A} = q_A \cdot (\text{flip coin}) + (1-q_A) \cdot h(x)`$ 
          - Then $`\widetilde{\epsilon_A}`$ is the new error rate of using the new model.
      - Rules)
        - $`\epsilon_B = \widetilde{\epsilon_A}`$
          - $`\displaystyle q_A^* = \frac{\epsilon_B - \widetilde{\epsilon_A}}{0.5-\widetilde{\epsilon_A}}`$
            - i.e.) We may calculate $`q_A^*`$ given the performance of the existing classifier.
        - $`\widetilde{\epsilon_A} = \epsilon_B - \gamma, \; \exists\gamma\gt0`$
          - $`\gamma`$ denotes the fairness, allowed difference between the groups
          - Then $`\displaystyle q_A = \frac{\epsilon_B - \gamma - \widetilde{\epsilon_A}}{0.5-\widetilde{\epsilon_A}}`$
            - Analysis) Flipping coin less compared to the equal error case.
    - Applying to other statistics
      - False Positive
        - $`\epsilon_i^{FP} = \text{Pr}_{\langle x,y\rangle\sim P}\left[ h(x)=1 \vert x\in A, y=0 \right]`$
    - [Hardt et al.](../readings/03/04.md) in this framework.
      |Existing Estimation / Group|A|B|
      |:-:|:-:|:-:|
      |$`h(x)=0`$|$`r`$|$`s`$||
      |$`h(x)=1`$|$`t`$|$`u`$||
      - Above is the amount of randomness we will take for each situation. (not a confusion matrix)
      - $`\widetilde{h}(x)`$ : given $`x`$, compute $`h(x)`$ for each $`x\in A, x\in B`$
        - With $`p\in\{r,s,t,u\}\subset[0,1]`$ (just like $`q_A`$ above)
          - use $`h(x)`$ with the probability $`1-p`$
          - flip coin with the probability $`p`$
        - For each rule, we may get $`r^*,s^*,t^*,u^*`$
    - Calculating the Overall Error
      - Suppose the population of the subgroups are $`p_A, p_B`$.
      - Using the $`\gamma`$ case,
        - $`\widetilde{\epsilon_A} = \epsilon_B - \gamma`$
      - Then the overall error goes   
        $`\begin{aligned}
          \widetilde{\epsilon} &= p_A \widetilde{\epsilon_A}+p_B \epsilon_B \\
          &= p_A (\epsilon_B - \gamma)+p_B \epsilon_B & (\because \widetilde{\epsilon_A} = \epsilon_B - \gamma) \\
          &= -p_A \gamma + (p_A + p_B)\epsilon_B \\
          &= -p_A \gamma + \epsilon_B & (\because p_A + p_B=1) \\
        \end{aligned}`$
      - Plotting this on the $`\tilde{\epsilon}-\gamma`$ plane
        - Linear trade-off between $`\gamma`$ and overall error rate.
        - This line is called the "Pareto Frontier" between overall error and fairness 
          - Each model has different plots.
          - If a line of a model is closer to the origin, we can achieve the pareto improvement by changing into that model.
        - Each rule corresponds to each point on the frontier.
        - This is a bolt-on frontier.
          ![](../images/notes/03_002.png)
      - Hypothetical Optimal models in a model class $`H`$
        - Convex curve the connects the vertices.
        - Not available in reality.
        - $`H`$-frontier
          ![](../images/notes/03_003.png)
          - Some possible selections that can be pareto improved.

<br><br>

[Back to Main](../main.md)