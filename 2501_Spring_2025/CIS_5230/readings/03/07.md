# 7. An Algorithmic Framework for Bias Bounties

## 1 Introduction
- ML has potential to train discriminatory models.
  - Partial remedies are suggested.
  - However, such practices are likely to remain insufficient.
    - Why?)
      - Impossible to anticipate all possible downstream use cases of the model.
      - Era of AI activism and no stringent regulation on AI and ML 
        - So, anyone can independently audit models with commercial or open source APIs, and publish their findings.
        - No external audit is guaranteed to them.
  - Sol.)
    - Bias Bounty Approach
      - Meaning)
        - External parties are invited to find biases in a model and reward doing so.
- What this paper suggests)
  - Propose and analyze a principled **algorithmic framework** for conducting bias bounties against an existing trained model $`f(x)`$
    - Props & Features of the Framework)
      - “Bias hunters” audit $`f`$ by submitting pairs of models $`(g, h)`$.
        - where 
          - $`g`$ : a model that identifies a subset of inputs to $`f`$ that $`f`$ performs poorly on.
          - $`h`$ : a proposed model which improves on this subset
      - The **proposed groups** do not need to be identified in advance, and the **improving models** do not need to be chosen from predetermined parametric classes.
        - Desc.)
          - Both groups and improving models can be arbitrarily complex.
          - In contrast to most fair ML frameworks.
            - Why?)
              - Usually, training is formulated as a constrained optimization problem over a fixed parametric family of models, with fairness constraints over fixed demographic subgroups
      - Validation
        - How?)
          - Given a proposed group and model pair $`(g,h)`$, our algorithm validates the proposed **improvement** on a holdout set, using techniques from adaptive data analysis to circumvent the potential for **overfitting** due to the potentially unlimited complexity of the submitted pair.
          - If the improvement is validated, our algorithm has a simple mechanism for automatically incorporating the improvement into an update of $`f`$ in a way that reduces both the overall error and the error on the proposed subgroup.
            - The algorithm guarantees that once a subgroup improvement has been accepted, the error on that subgroup will never increase (much) due to subsequent subgroup introductions.
            - Thus there is no tradeoff between overall and subgroup errors, or between different subgroups.
              - cf.) In contrast to the constrained optimization approaches
            - The algorithm achieves this through the use of a new model class called pointer decision lists.
      - The algorithm provably and monotonically converges quickly to one of two possible outcomes:
        - reach the Bayes optimal model
        - reach a model that cannot be distinguished from the Bayes optimal
          - Then the bias hunters can find no further improvements.
      - This framework can be viewed as an entirely algorithmic approach to training fair and accurate models by replacing the bias hunters with automated mechanisms for finding improving pairs.
        - Two such mechanisms)
          - one based on a reduction to cost-sensitive classification
          - an expectation-maximization style approach that alternates between finding the optimal model for a given subgroup and optimizing the subgroup for a given model
      - Limit of this framework)
        - It can only identify and correct **sub-optimal** performance on groups as measured on the data distribution for which we **have training data**.
        - This model appears to perform well on every group only because in gathering data, we have **failed to sample representative members** of that group.
        - The model that we have **cannot be improved on some group** only because we have **failed to gather the features** that are most predictive on that group.
          - Improvable only if better features are gathered.
        - Cannot be used to find and correct biases that come from having gathered the **wrong dataset**.
        - Obstacle in validation
          - Suppose we use a holdout set to check the improvement of $`h`$, which was built on examples from group $`g`$.
          - But there is on holdout set left to evaluate the next improved model.

<br><br>

## 2. Preliminaries
### Model)
- A supervised learning problem defined over...
  - $`(x,y)\in\mathcal{X}\times\mathcal{Y}`$ : labeled examples
  - $`\mathcal{D} \in \Delta(\mathcal{X}\times\mathcal{Y})`$ : a joint probability distribution over $`(x,y)`$
    - $`D\sim\mathcal{D}^n`$ : $`n`$ labeled i.i.d. sampled examples from $`\mathcal{D}`$
  - $`f:\mathcal{X}\rightarrow\mathcal{Y}$ : a model we want to learn
  - $`\ell : \mathcal{Y}\times\mathcal{Y} \rightarrow [0, 1]`$ : a loss function
    - $`\ell(\hat{y}, y)`$ : the cost of labeling $`y`$ with $`f(x) = \hat{y}`$

#### Def.1) Subgroups
- A subgroup of the feature space $`\mathcal{X}`$ will be represented as an indicator function $`g:\mathcal{X}\rightarrow\{0, 1\}`$.   
- $`x\in\mathcal{X}`$ is in group $`g`$ if $`g(x)=1`$. Otherwise, $`x`$ is not in $`g`$.   
- Given $`g`$, its empirical measure under the probability distribution $`\mathcal{D}`$ is 
  - $`\mu_g(\mathcal{D}) = P_\mathcal{D}\left[ g(x)=1 \right]`$
    - assuming $`\mathcal{D}`$ is uniformly distributed.


#### Def.2) Model Loss
- Given a model $`f:\mathcal{X}\rightarrow\mathcal{Y}`$
  - the average loss of $`f`$ on $`\mathcal{D}`$ is denoted by
    - $`L(\mathcal{D}, f) = \mathbb{E}_{(x,y)\sim\mathcal{D}} [\ell(f(x), y)]`$
  - the loss on $`f`$ conditional on membership $`g`$ is
    - $`L(\mathcal{D}, f, g) = \mathbb{E}_{(x,y)\sim\mathcal{D}} \left[\ell(f(x), y) \vert g(x)=1 \right]`$
- For $`D\sim\mathcal{D}^n`$
  - $`L(D,f)`$ : empirical loss on $`D`$
  - $`L(D,f,g)`$ : empirical loss on $`D`$ conditional on $`g`$


#### Def.3) Bayes Optimal Model
- A Bayes Optimal model $`f^*:\mathcal{X}\rightarrow\mathcal{Y}`$ w.r.t. $`\ell`$ and $`\mathcal{D}`$ is given by
  - $`\displaystyle f^*(x) \in \arg\min_{y\in\mathcal{Y}} \mathbb{E}_{(x',y')\sim\mathcal{D}} \left[ \ell(y, y') \vert x'=x \right]`$
    - where $`f^*(x)`$ can be defined arbitrarily for any $`x`$ that is not in the support of $`\mathcal{D}`$
- Prop.)
  - Point-wise optimal
  - Has the lowest of any possible model, simultaneously on every subgroup.


#### Observation 4)
- Fixing $`\ell`$ and $`\mathcal{D}`$, $`f^*`$ is a Bayes optimal model iff.
  - $`\forall g, h, \; L(\mathcal{D},f^*, g) \le L(\mathcal{D}, h, g)`$ 


#### Def.5) Approximated Bayes Optimality
- A model $`f:\mathcal{X}\rightarrow\mathcal{Y}`$ is $`(\epsilon, \mathcal{C})`$-Bayes optimal w.r.t. a collection of (group, model) pairs $`\mathcal{C}`$ if 
  - for each $`(g,h)\in\mathcal{C}`$, the performance of $`f`$ on $`g`$ is within $`\epsilon`$ of the performance of $`h`$ on $`g`$..
    - i.e.) $`\displaystyle\forall (g,h)\in\mathcal{C}, \; L(\mathcal{D}, f, g)\le L(\mathcal{D}, h, g) + \frac{\epsilon}{\mu_g(\mathcal{D})}`$
- When $`\mathcal{C} = \mathcal{G}\times\mathcal{H}`$, we call $`f`$ is $`(\epsilon, \mathcal{G}, \mathcal{H})`$-Bayes Optimal if the above inequality holds for $`\forall g\in\mathcal{G}, \forall  h\in\mathcal{H}`$


#### Remark 6) Multi-Group Fairness Definition
- $`\epsilon`$ scale proportionately to the inverse probability of the group $`g \;(\mu_g(\mathcal{D}))`$
  - similar to [Kearns el al., 2018](06.md)
- $`\epsilon = 0 \Rightarrow`$ identical to Bayes Optimality
- As $`\mathcal{G}`$ and $`\mathcal{H}`$ grow in expressivity, the [condition](#def5-approximated-bayes-optimality) becomes much stronger.