# The Frontiers of Fairness in Machine Learning

- Bolt-on methodology
  - 

## 2.1 Causes of Unfairness
### 2.1.1 Bias Encoded in Data
- Training data already including human bias
  - e.g.)
    - Recidivism Case)
      - Arrest data is skewed towards minority populations that are policed at a higher rate.
      - This data is used to estimate the recidivism score.

<br>

### 2.1.2 Minimizing Average Error Fits Majority Populations
- Suppose there are majority group and minority group.
- If we train a group-blind classifier to minimize overall error, if it cannot simultaneously fit both populations optimally, it will fit the majority population.
- This leads to a different (and higher) distribution of errors in the minority population.

<br>

### 2.1.3 The Need to Explore
- We cannot observe the prediction's performance until it happens.
  - e.g.) To check the recidivism, we should release the inmate and see if he recidivate.
- Thus, sometimes we should take actions we believe to be sub-optimal in order to gather more data.
  - Two Ethical questions on this)
    - When are the individual costs of exploration borne disproportionately by a certain sub-population?
    - For scenarios that taking the sub-optimal is immoral, how much does this slow learning, and does this lead to other sorts of unfairness?

<br><br>

## 2.2 Definition of Fairness
### 2.2.1 Statistical Definitions of Fairness
- Settings)
  - $`\mathcal{G}`$ : a small number of protected demographic groups
- Def.) Family of Fairness Definitions
  - For fixed $`\mathcal{G}`$,
    - we may use statistical measures such as
      - Raw Positive Classification Rate ([statistical parity](01.md#1-brief-overview))
      - False Positive Rate (FPR)
      - False Negative Rate (FNR)
      - Positive Predictive Value (Closely related to equalized calibration)
- Prop.)
  - Pros)
    - Can be achieved without making any assumptions on the data 
    - Can be easily verified
  - Do not on their own give meaningful guarantees to individuals or structured subgroups of the protected demographic groups.
  - Instead they give guarantees to “average” members of the protected groups.
  - Different statistical measures of fairness can be at odds with one another.
    - e.g.) Cannot simultaneously equalize FPR and FNR(TPR)

<br>

### 2.2.2 Individual Definition of Fairness
- Desc.)
  - This definition ask for constraints that bind on specific pairs of individuals, rather than on a quantity that is averaged over groups.
  - Check [Individual Fairness from the Lecture note](../../notes/03.md#concept-fairness-in-ml)
    - e.g.)
      - Similar individuals should be treated similarly.
      - Less qualified individuals should not be favored over more qualified individuals
- Prop.)
  - Pro)
    - The semantics of these kinds of definitions can be more meaningful than [statistical definitions](#221-statistical-definitions-of-fairness).
  - Con)
    - Require making significant assumptions compared to [statistical definitions](#221-statistical-definitions-of-fairness).

<br><br>

## 3. Questions at the Research Frontier
### 3.1 Between Statistical and Individual Fairness
- Mixing the pros of [statistical definitions](#221-statistical-definitions-of-fairness) and [individual definitions](#222-individual-definition-of-fairness).
  - i.e.) Satisfying both...
    - Practically implementable without the need for making strong assumptions on the data or the knowledge of the algorithm designer
    - Provide more meaningful guarantees to individuals
- Approaches)
  1. Asking for [statistical definitions](#221-statistical-definitions-of-fairness) definitions to hold not just on a small number of protected groups ($`\mathcal{G}`$), but on an exponential or infinite class of groups defined by some class of functions of bounded complexity.
     - Desc.)
       - It better address concerns about **intersectionality**.
         - Concept) Intersectionality
           - How different kinds of discrimination can compound and interact for individuals who fall at  the **intersection** of several protected classes
       - Recall that the [statistical definitions](#221-statistical-definitions-of-fairness) were weak on  that they ask for the constraints to hold on average over the entire group, not small groups.
         - This model can limit this weakness.
  2. Assume that the algorithm designer has perfect knowledge of a “fairness metric.”
     - e.g.)
       - Assume that the algorithm has access to an oracle which can return an unbiased estimator for the distance between two randomly drawn individuals according to an unknown fairness metric.
         - It is shown that when the metric is from a specific learnable family, this kind of feedback is sufficient to obtain an optimal regret bound to the best fair classifier while having only a bounded number of violations of the fairness metric.

<br>

### 3.2 Data Evolution and Dynamics of Fairness
- Desc.)
  - In real algorithmic systems, many different components that are combined together, and operate in complex environments that are dynamically changing.
    - Sometimes because of the actions of the learning algorithm itself.
- Case1)
  - How and when components that may **individually** satisfy notions of fairness compose into larger constructs that still satisfy fairness guarantees.
  - Similarly, the individual components of a “fair” system may appear to be unfair in isolation.
  - Experience from differential privacy suggests that graceful degradation under composition is key to designing complicated algorithms satisfying desirable statistical properties, because it allows algorithm design and analysis to be modular.
- Case2)
  - How algorithms dynamically effect their environment, and the incentives of human actors

<br>

### 3.3 Modeling and Correcting Bias in the Data
- Situation)
  - The available training data is already contaminated by bias
    - Why?)
      - The data itself is often a product of social and historical process that operated to the disadvantage of certain groups.
  - When trained in such data, machine learning techniques may reproduce, reinforce, and potentially exacerbate existing biases.
    - e.g.)
      - A model learns the tendency of jobs by gender and reproduce the result on the job application process.
      - Feedback loop
        - Deploy police using the arrest data by area.
        - More police deployed at the area with high criminal rates.
        - More arrests take place there.
- Sol.)
  - Can we design a model that can correct the bias in data?
    - Generally these approaches require 
      - knowledge of how the measurement process is biased
      - or judgments about properties the data would satisfy in an “unbiased” world
    - This can be described as the disconnection between the **observed space** and the unobservable **construct space** (desirable).
    - Within this framework, data correction efforts attempt to **undo** the effects of biasing mechanisms that drive discrepancies between these spaces.
    - Cost of corrections)
      - reductions in prediction accuracy

<br>

### 3.4 Fair Representation
- Def.) Fair representation learning
  - A data de-biasing process that produces transformations (**intermediate** representations) of the original data that **retain** as much of the task-relevant information as possible while **removing** information about **sensitive or protected attributes**. 
    - How?)
      - Transform the observational data that group membership may be inferred from other features to a construct space where protected attributes are statistically independent of other features.

<br>

### 3.5 Beyond Classification
- Learnings other than batch classification
  - e.g.)
    - online learning, bandit learning, and reinforcement learning
- These learning algorithms has dynamic settings in which the actions of the algorithm feed back into the data it observes.
- And, these dynamic settings capture many problems for which fairness is a concern.
  - e.g.) Bandit Learning Problems (불확실한 보상 구조 하에서 최적의 결정을 내리기 위해 탐색과 활용 사이의 균형을 유지하며 학습하는 방법) cannot observe **counterfactuals**
    - e.g.) Recidivism
      - If a candidate is predicted to recidivate and was not released. In this case, we do not know if he/she will recidivate or not.
    - Trade-off in exploration and exploitation
      - i.e.)  Rather than always making a myopically optimal decision, when counter-factuals cannot be observed, it is necessary for algorithms to sometimes take actions that appear to be sub-optimal so as to gather more data.
    - But in reality, this setting can be unethical if applied to individuals.


<br>